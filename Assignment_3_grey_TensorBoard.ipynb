{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assinment I have taken a Kaggle Image dataset to study and understadn how a Deep Learning CNN(Convolutional Neural Network) works, and can be operated on the images using TesorFlow and TensorBoard.\n",
    "The Kaggle image dataset being used in here is a Pokemon Image dataset taken from the below source for your humble reference.\n",
    "##### Pokemon Image Dataset - https://www.kaggle.com/aaronyin/oneshotpokemon#kaggle-one-shot-pokemon.zip\n",
    "\n",
    "<img src=\"Images/pokemon.png\" width=\"700\">\n",
    "\n",
    "We majorly use Keras which has been set over the platform of TesorFlow. Keras helps us play with the images by understanding it and manipulating it as per our requirement as we need to re-adjust the images which are our primary source of dataset to adjust with the training module of our TesorFlow to operate on it.\n",
    "\n",
    "#### Dataset description:\n",
    "There are three directories, \n",
    "\"pokemon-a\" and \"pokemon-b\" are standard pokemon images with no background. One can use them for training. \n",
    "\"pokemon-tcg-images\" are pokemon images cropped from pokemon tcg card. They are for testing. \n",
    "Filenames in all directories start with pokemon-id, aka class label.\n",
    "\n",
    "#### Where those images from:\n",
    "All images are collected from internet, and original authors owns the copyright (Nintendo, I guess). \n",
    "Check links below for more information:\n",
    "\n",
    "* <a href=\"https://www.kaggle.com/kvpratama/pokemon-images-dataset\">kaggle-pokemon-images</a>\n",
    "\n",
    "* <a href=\"https://veekun.com/dex/downloads\">veekun</a>\n",
    "\n",
    "* <a href=\"https://github.com/PokemonTCG\">pokemon-tcg</a>\n",
    "\n",
    "\n",
    "Keras is a high-level python API which can be used to quickly build and train neural networks using either Tensorflow or Theano as back-end.\n",
    "\n",
    "Before we start wit hCNN in Deep learning, lets discuss about RNN and its drawbacks.\n",
    "\n",
    "The Regular Neural Netowrks(NN) is not capable of dealing with images. Just imagine each pixel is connected to one neuron and there will thousands of neurons which will be computationally expensive. CNN handles images in different ways, but still it follows the general concept of NN.\n",
    "\n",
    "<img src=\"Images/image1.jpeg\">\n",
    "\n",
    "They are made up of neurons that have learnable weights and biases. Each neuron accepts the inputs, action a dot product operation and follows the non-linearity function. And they still have a loss function (e.g. SVM/Softmax) on the fully-connected layer and all the tips & tricks we developed for learning regular NN still apply.\n",
    "\n",
    "CNN is used as the default model for anything to deal with images. Nowadays there are papers that has mentioned about the use of Recurrent Neural Network(RNN) for the image recognition. Traditionally RNNs are being used for text and speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try the below scenarios for our models that has been created in the whole notebook:\n",
    "##### * Part A - Have used a Convolution for the CNN that has been established further\n",
    "\n",
    "##### * Part B - Activation function\n",
    "    * Model 1 - Created with ReLU and explained\n",
    "    * Model 2 - Created with TanH and explained\n",
    "    * Model 3 - Created with Softsign and explained\n",
    "\n",
    "##### * Part C - Cost function\n",
    "    * Model 4 - Created with Cross-Entropy as an activation function and explained by running it for 10 Epoch\n",
    "    * Model 5 - Created with Quadratic cost (mean-square error) activation function and implemented with Adam as an optimization and explained\n",
    "\n",
    "##### * Part D - Epochs\n",
    "    * Model 1 being the best till the run, is taken to test if the number of Epoch helps it improve the training with explanation\n",
    "\n",
    "##### * Part E - Gradient estimation   \n",
    "    * Model 6 - Created with Adadelta as an optimization technique and explained\n",
    "    * Model 7 - Created with Adagrad as an optimization technique and explained\n",
    "##### * Part F - Network Architecture\n",
    "    * Model 8 - Network is reduced from the usual one's taken in the previous models and checked in performance\n",
    "    * Model 9 - Network increased with 3 layers in total and checked with performance\n",
    "##### * Part G - Network initialization\n",
    "    * Model 10 - Xavier Uniform network initialization being applied and checked\n",
    "    * Model 11 - Zero network initialization being applied and checked\n",
    "\n",
    "At the end as a conclusion:\n",
    "#### Conclusion:\n",
    "We compare the performance of the models generated will now based on their Accuracy metric and try to check which one performs good. Also, we try to save the model, apply the same on a real world test data to check how it performs. We check the accurace of the same and come to conclusion with leraning how to perfrom a Deep Learning algortihm CNN during the process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Part A` - Deep Learning with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATADIR = \"kaggle-one-shot-pokemon/\"\n",
    "\n",
    "CATEGORIES = [\"pokemon-a\", \"pokemon-b\"]\n",
    "\n",
    "for category in CATEGORIES:  # do dogs and cats\n",
    "    path = os.path.join(DATADIR,category)  # create path to dogs and cats\n",
    "    for img in os.listdir(path):  # iterate over each image per dogs and cats\n",
    "        img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert to array\n",
    "        plt.imshow(img_array, cmap='gray')  # graph it\n",
    "        plt.show()  # display!\n",
    "\n",
    "        break  # we just want one for now so break\n",
    "    break  #...and one more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 300)\n"
     ]
    }
   ],
   "source": [
    "print(img_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "IMG_SIZE = 50\n",
    "\n",
    "new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "plt.imshow(new_array, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "plt.imshow(new_array, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 939/939 [00:02<00:00, 417.32it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 819/819 [00:13<00:00, 60.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1758\n"
     ]
    }
   ],
   "source": [
    "training_data = []\n",
    "\n",
    "def create_training_data():\n",
    "    for category in CATEGORIES:  # do dogs and cats\n",
    "\n",
    "        path = os.path.join(DATADIR,category)  # create path to dogs and cats\n",
    "        class_num = CATEGORIES.index(category)  # get the classification  (0 or a 1). 0=dog 1=cat\n",
    "\n",
    "        for img in tqdm(os.listdir(path)):  # iterate over each image per dogs and cats\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert to array\n",
    "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
    "                training_data.append([new_array, class_num])  # add this to our training_data\n",
    "            except Exception as e:  # in the interest in keeping the output clean...\n",
    "                pass\n",
    "\n",
    "create_training_data()\n",
    "\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for sample in training_data[:10]:\n",
    "    print(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]]]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for features,label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "\n",
    "print(X[0].reshape(-1, IMG_SIZE, IMG_SIZE, 1))\n",
    "\n",
    "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many input neurons do I have?\n",
    "\n",
    "What is the input layer for the Pokemon data? Each image is \n",
    "\n",
    "    $50x50 = 2500$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1758, 50, 50, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "50*50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We save the evaluation using pickle as a TEXT based out for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"X.pickle\",\"wb\")\n",
    "pickle.dump(X, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"y.pickle\",\"wb\")\n",
    "pickle.dump(y, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"X.pickle\",\"rb\")\n",
    "X = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"y.pickle\",\"rb\")\n",
    "y = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a mode generation function to cut the cookie cutter and use the for generating the models based on our requirements as and when it comes for a specific activation function, loss, optimization techniques.\n",
    "\n",
    "#### Function `function_model` - generates the model based on the parameters given to it\n",
    "* `activation` - the name of the activation function that needs to be used\n",
    "* `loss_name` - Loss to be used(Cost function)\n",
    "* `opti` - optimization technique that needs to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_model(activation, loss_name, opti):\n",
    "    global count\n",
    "    global tensorboard\n",
    "    NAME = \"Pokemon-model{}-{}\".format(count, int(time.time()))\n",
    "    tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Conv2D in Convelution for a 2D image\n",
    "    #3x3 in the window we are using\n",
    "    model.add(Conv2D(64, (3, 3), input_shape=X.shape[1:]))\n",
    "    model.add(Activation(activation))\n",
    "    #We do the pooling as size 2x2\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    #We do it again, but dont need shapping\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    #Convolution is 2D so we flatten the data to compress it to understandable\n",
    "    model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation(activation))\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.compile(loss=loss_name,\n",
    "                  optimizer=opti,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "#     model1.fit(X, y, batch_size=32, epochs=epoc_num, validation_split=0.3, callbacks=[tensorboard])\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_x=function_model('relu', 'binary_crossentropy', 'adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A trial to generate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1230 samples, validate on 528 samples\n",
      "Epoch 1/5\n",
      "1230/1230 [==============================] - ETA: 58s - loss: 0.6944 - acc: 0.53 - ETA: 31s - loss: 0.6529 - acc: 0.68 - ETA: 22s - loss: 0.5812 - acc: 0.75 - ETA: 18s - loss: 0.5156 - acc: 0.79 - ETA: 15s - loss: 0.4597 - acc: 0.83 - ETA: 13s - loss: 0.4121 - acc: 0.85 - ETA: 12s - loss: 0.4118 - acc: 0.86 - ETA: 11s - loss: 0.3803 - acc: 0.87 - ETA: 10s - loss: 0.3692 - acc: 0.88 - ETA: 9s - loss: 0.3711 - acc: 0.8750 - ETA: 8s - loss: 0.3531 - acc: 0.880 - ETA: 8s - loss: 0.3396 - acc: 0.882 - ETA: 7s - loss: 0.3256 - acc: 0.889 - ETA: 7s - loss: 0.3169 - acc: 0.890 - ETA: 6s - loss: 0.3110 - acc: 0.889 - ETA: 6s - loss: 0.3051 - acc: 0.890 - ETA: 6s - loss: 0.2953 - acc: 0.891 - ETA: 5s - loss: 0.3036 - acc: 0.888 - ETA: 5s - loss: 0.2944 - acc: 0.894 - ETA: 4s - loss: 0.3047 - acc: 0.884 - ETA: 4s - loss: 0.2985 - acc: 0.885 - ETA: 4s - loss: 0.2922 - acc: 0.889 - ETA: 4s - loss: 0.3019 - acc: 0.888 - ETA: 3s - loss: 0.2997 - acc: 0.890 - ETA: 3s - loss: 0.2906 - acc: 0.893 - ETA: 3s - loss: 0.2905 - acc: 0.894 - ETA: 2s - loss: 0.2902 - acc: 0.893 - ETA: 2s - loss: 0.2865 - acc: 0.895 - ETA: 2s - loss: 0.2935 - acc: 0.891 - ETA: 2s - loss: 0.2917 - acc: 0.890 - ETA: 1s - loss: 0.2932 - acc: 0.889 - ETA: 1s - loss: 0.2932 - acc: 0.888 - ETA: 1s - loss: 0.2881 - acc: 0.890 - ETA: 1s - loss: 0.2881 - acc: 0.890 - ETA: 0s - loss: 0.2807 - acc: 0.893 - ETA: 0s - loss: 0.2804 - acc: 0.894 - ETA: 0s - loss: 0.2760 - acc: 0.896 - ETA: 0s - loss: 0.2736 - acc: 0.897 - 11s 9ms/sample - loss: 0.2728 - acc: 0.8976 - val_loss: 0.1948 - val_acc: 0.9167\n",
      "Epoch 2/5\n",
      "1230/1230 [==============================] - ETA: 6s - loss: 0.2891 - acc: 0.906 - ETA: 6s - loss: 0.2497 - acc: 0.921 - ETA: 6s - loss: 0.2655 - acc: 0.916 - ETA: 6s - loss: 0.2364 - acc: 0.929 - ETA: 6s - loss: 0.2156 - acc: 0.931 - ETA: 6s - loss: 0.1946 - acc: 0.937 - ETA: 6s - loss: 0.1867 - acc: 0.937 - ETA: 6s - loss: 0.1784 - acc: 0.941 - ETA: 5s - loss: 0.1845 - acc: 0.941 - ETA: 5s - loss: 0.1768 - acc: 0.940 - ETA: 5s - loss: 0.1749 - acc: 0.943 - ETA: 5s - loss: 0.1733 - acc: 0.942 - ETA: 5s - loss: 0.1670 - acc: 0.944 - ETA: 4s - loss: 0.1727 - acc: 0.944 - ETA: 4s - loss: 0.1705 - acc: 0.945 - ETA: 4s - loss: 0.1720 - acc: 0.943 - ETA: 4s - loss: 0.1705 - acc: 0.944 - ETA: 4s - loss: 0.1706 - acc: 0.942 - ETA: 3s - loss: 0.1653 - acc: 0.944 - ETA: 3s - loss: 0.1589 - acc: 0.946 - ETA: 3s - loss: 0.1559 - acc: 0.946 - ETA: 3s - loss: 0.1563 - acc: 0.944 - ETA: 3s - loss: 0.1543 - acc: 0.945 - ETA: 2s - loss: 0.1600 - acc: 0.944 - ETA: 2s - loss: 0.1634 - acc: 0.943 - ETA: 2s - loss: 0.1616 - acc: 0.945 - ETA: 2s - loss: 0.1620 - acc: 0.944 - ETA: 2s - loss: 0.1612 - acc: 0.944 - ETA: 1s - loss: 0.1593 - acc: 0.944 - ETA: 1s - loss: 0.1560 - acc: 0.945 - ETA: 1s - loss: 0.1523 - acc: 0.947 - ETA: 1s - loss: 0.1528 - acc: 0.947 - ETA: 1s - loss: 0.1536 - acc: 0.946 - ETA: 0s - loss: 0.1546 - acc: 0.945 - ETA: 0s - loss: 0.1613 - acc: 0.943 - ETA: 0s - loss: 0.1627 - acc: 0.942 - ETA: 0s - loss: 0.1587 - acc: 0.944 - ETA: 0s - loss: 0.1606 - acc: 0.943 - 9s 7ms/sample - loss: 0.1617 - acc: 0.9423 - val_loss: 0.3448 - val_acc: 0.8977\n",
      "Epoch 3/5\n",
      "1230/1230 [==============================] - ETA: 7s - loss: 0.2475 - acc: 0.906 - ETA: 7s - loss: 0.1886 - acc: 0.937 - ETA: 7s - loss: 0.1411 - acc: 0.958 - ETA: 7s - loss: 0.1653 - acc: 0.953 - ETA: 6s - loss: 0.1970 - acc: 0.925 - ETA: 6s - loss: 0.1771 - acc: 0.932 - ETA: 6s - loss: 0.1605 - acc: 0.937 - ETA: 6s - loss: 0.1608 - acc: 0.941 - ETA: 5s - loss: 0.1817 - acc: 0.937 - ETA: 5s - loss: 0.2557 - acc: 0.921 - ETA: 5s - loss: 0.2713 - acc: 0.920 - ETA: 5s - loss: 0.2497 - acc: 0.927 - ETA: 5s - loss: 0.2487 - acc: 0.925 - ETA: 4s - loss: 0.2476 - acc: 0.924 - ETA: 4s - loss: 0.2442 - acc: 0.925 - ETA: 4s - loss: 0.2423 - acc: 0.925 - ETA: 4s - loss: 0.2399 - acc: 0.924 - ETA: 4s - loss: 0.2384 - acc: 0.925 - ETA: 3s - loss: 0.2286 - acc: 0.929 - ETA: 3s - loss: 0.2279 - acc: 0.929 - ETA: 3s - loss: 0.2227 - acc: 0.930 - ETA: 3s - loss: 0.2231 - acc: 0.929 - ETA: 3s - loss: 0.2207 - acc: 0.929 - ETA: 2s - loss: 0.2212 - acc: 0.927 - ETA: 2s - loss: 0.2202 - acc: 0.927 - ETA: 2s - loss: 0.2138 - acc: 0.930 - ETA: 2s - loss: 0.2150 - acc: 0.929 - ETA: 2s - loss: 0.2128 - acc: 0.929 - ETA: 1s - loss: 0.2108 - acc: 0.931 - ETA: 1s - loss: 0.2066 - acc: 0.933 - ETA: 1s - loss: 0.2061 - acc: 0.932 - ETA: 1s - loss: 0.2071 - acc: 0.931 - ETA: 1s - loss: 0.2030 - acc: 0.933 - ETA: 0s - loss: 0.1994 - acc: 0.935 - ETA: 0s - loss: 0.1982 - acc: 0.935 - ETA: 0s - loss: 0.2022 - acc: 0.934 - ETA: 0s - loss: 0.1998 - acc: 0.934 - ETA: 0s - loss: 0.1973 - acc: 0.935 - 9s 7ms/sample - loss: 0.1961 - acc: 0.9358 - val_loss: 0.2064 - val_acc: 0.9053\n",
      "Epoch 4/5\n",
      "1230/1230 [==============================] - ETA: 7s - loss: 0.1694 - acc: 0.906 - ETA: 7s - loss: 0.2115 - acc: 0.906 - ETA: 7s - loss: 0.1903 - acc: 0.916 - ETA: 6s - loss: 0.1722 - acc: 0.921 - ETA: 6s - loss: 0.1734 - acc: 0.925 - ETA: 6s - loss: 0.1866 - acc: 0.921 - ETA: 6s - loss: 0.1627 - acc: 0.933 - ETA: 6s - loss: 0.1697 - acc: 0.933 - ETA: 6s - loss: 0.1520 - acc: 0.941 - ETA: 5s - loss: 0.1505 - acc: 0.940 - ETA: 5s - loss: 0.1508 - acc: 0.940 - ETA: 5s - loss: 0.1442 - acc: 0.942 - ETA: 5s - loss: 0.1530 - acc: 0.935 - ETA: 5s - loss: 0.1467 - acc: 0.937 - ETA: 4s - loss: 0.1488 - acc: 0.937 - ETA: 4s - loss: 0.1435 - acc: 0.941 - ETA: 4s - loss: 0.1454 - acc: 0.939 - ETA: 4s - loss: 0.1436 - acc: 0.942 - ETA: 3s - loss: 0.1436 - acc: 0.944 - ETA: 3s - loss: 0.1497 - acc: 0.940 - ETA: 3s - loss: 0.1528 - acc: 0.939 - ETA: 3s - loss: 0.1506 - acc: 0.938 - ETA: 3s - loss: 0.1491 - acc: 0.940 - ETA: 2s - loss: 0.1513 - acc: 0.940 - ETA: 2s - loss: 0.1516 - acc: 0.940 - ETA: 2s - loss: 0.1474 - acc: 0.942 - ETA: 2s - loss: 0.1474 - acc: 0.943 - ETA: 2s - loss: 0.1491 - acc: 0.943 - ETA: 1s - loss: 0.1483 - acc: 0.942 - ETA: 1s - loss: 0.1487 - acc: 0.943 - ETA: 1s - loss: 0.1480 - acc: 0.944 - ETA: 1s - loss: 0.1475 - acc: 0.945 - ETA: 1s - loss: 0.1459 - acc: 0.947 - ETA: 0s - loss: 0.1453 - acc: 0.946 - ETA: 0s - loss: 0.1421 - acc: 0.948 - ETA: 0s - loss: 0.1435 - acc: 0.947 - ETA: 0s - loss: 0.1438 - acc: 0.946 - ETA: 0s - loss: 0.1416 - acc: 0.947 - 9s 7ms/sample - loss: 0.1423 - acc: 0.9463 - val_loss: 0.1998 - val_acc: 0.9261\n",
      "Epoch 5/5\n",
      "1230/1230 [==============================] - ETA: 7s - loss: 0.0337 - acc: 1.000 - ETA: 7s - loss: 0.0541 - acc: 0.984 - ETA: 6s - loss: 0.0822 - acc: 0.979 - ETA: 6s - loss: 0.0761 - acc: 0.984 - ETA: 6s - loss: 0.0761 - acc: 0.981 - ETA: 6s - loss: 0.0704 - acc: 0.984 - ETA: 6s - loss: 0.0844 - acc: 0.977 - ETA: 6s - loss: 0.0803 - acc: 0.976 - ETA: 5s - loss: 0.0837 - acc: 0.972 - ETA: 5s - loss: 0.0940 - acc: 0.965 - ETA: 5s - loss: 0.0907 - acc: 0.968 - ETA: 5s - loss: 0.1027 - acc: 0.963 - ETA: 5s - loss: 0.1027 - acc: 0.961 - ETA: 4s - loss: 0.0971 - acc: 0.964 - ETA: 4s - loss: 0.1032 - acc: 0.962 - ETA: 4s - loss: 0.1030 - acc: 0.962 - ETA: 4s - loss: 0.1069 - acc: 0.961 - ETA: 4s - loss: 0.1059 - acc: 0.963 - ETA: 3s - loss: 0.1065 - acc: 0.963 - ETA: 3s - loss: 0.1040 - acc: 0.965 - ETA: 3s - loss: 0.1080 - acc: 0.959 - ETA: 3s - loss: 0.1210 - acc: 0.957 - ETA: 3s - loss: 0.1168 - acc: 0.959 - ETA: 2s - loss: 0.1212 - acc: 0.955 - ETA: 2s - loss: 0.1223 - acc: 0.956 - ETA: 2s - loss: 0.1206 - acc: 0.957 - ETA: 2s - loss: 0.1228 - acc: 0.956 - ETA: 2s - loss: 0.1256 - acc: 0.953 - ETA: 1s - loss: 0.1244 - acc: 0.953 - ETA: 1s - loss: 0.1239 - acc: 0.955 - ETA: 1s - loss: 0.1271 - acc: 0.954 - ETA: 1s - loss: 0.1273 - acc: 0.953 - ETA: 1s - loss: 0.1248 - acc: 0.954 - ETA: 0s - loss: 0.1217 - acc: 0.955 - ETA: 0s - loss: 0.1238 - acc: 0.955 - ETA: 0s - loss: 0.1254 - acc: 0.955 - ETA: 0s - loss: 0.1259 - acc: 0.956 - ETA: 0s - loss: 0.1261 - acc: 0.956 - 9s 7ms/sample - loss: 0.1249 - acc: 0.9569 - val_loss: 0.2143 - val_acc: 0.9223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ce5c8bcda0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_x.fit(X, y, batch_size=32, epochs=5, validation_split=0.3, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start Convolution, lets explain about it a bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution\n",
    "Let’s dive into how convolution layer works. Convolution has got set learn-able filters which will be a matrix(width, height, and depth). We consider an image as a matrix and filter will be sliding through the image matrix as shown below to get the convoluted image which is the filtered image of the actual image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/gif_image.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depend upon the task, more than one filter is available in the model to cater the different features. Feature might be looking for a cat, looking for color etc. Filter matrix value is learned during the training phase of the model.\n",
    "\n",
    "<img src='Images/gif_2.gif'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Part B` - Activation functions\n",
    "In computational networks, the activation function of a node defines the output of that node given an input or set of inputs. A standard computer chip circuit can be seen as a digital network of activation functions that can be “ON” (1) or “OFF” (0), depending on input. This is similar to the behavior of the linear perceptron in neural networks. However, only nonlinear activation functions allow such networks to compute nontrivial problems using only a small number of nodes. In artificial neural networks this function is also called the transfer function.\n",
    "\n",
    "##### We use ReLU, TanH & Softsign to demonstrate it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Rectified linear unit (ReLU) transfer function\n",
    "\n",
    "Rectified linear unit (ReLU)\n",
    "\n",
    "Activation identity\n",
    "\n",
    "$f(x)=x$\n",
    "$f'(x)=1$\n",
    "$(-\\infty,\\infty)$\n",
    "$C^\\infty$\n",
    "\n",
    "$f(x) = \\begin{cases}\n",
    "    0 &amp; \\text{for } x &lt; 0\\\\\n",
    "    x &amp; \\text{for } x \\ge 0\\end{cases}$\n",
    "\n",
    "$f'(x) = \\begin{cases}\n",
    "    0 &amp; \\text{for } x &lt; 0\\\\\n",
    "    1 &amp; \\text{for } x \\ge 0\\end{cases}$\n",
    "\n",
    "$[0,\\infty)$\n",
    "$C^0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - CNN \n",
    "With below conditions\n",
    "* <span class=\"mark\">ReLU</span> \n",
    "* CrossEntropy \n",
    "* 5 Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\prabh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Train on 1230 samples, validate on 528 samples\n",
      "WARNING:tensorflow:From C:\\Users\\prabh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/5\n",
      "1230/1230 [==============================] - ETA: 16s - loss: 0.6923 - acc: 0.46 - ETA: 9s - loss: 0.7463 - acc: 0.4062 - ETA: 7s - loss: 0.6945 - acc: 0.479 - ETA: 5s - loss: 0.6592 - acc: 0.609 - ETA: 5s - loss: 0.6314 - acc: 0.662 - ETA: 4s - loss: 0.6070 - acc: 0.697 - ETA: 4s - loss: 0.5719 - acc: 0.732 - ETA: 3s - loss: 0.5532 - acc: 0.742 - ETA: 3s - loss: 0.5349 - acc: 0.753 - ETA: 3s - loss: 0.5200 - acc: 0.768 - ETA: 3s - loss: 0.4998 - acc: 0.778 - ETA: 3s - loss: 0.4774 - acc: 0.789 - ETA: 2s - loss: 0.4779 - acc: 0.790 - ETA: 2s - loss: 0.4622 - acc: 0.792 - ETA: 2s - loss: 0.4870 - acc: 0.772 - ETA: 2s - loss: 0.4698 - acc: 0.781 - ETA: 2s - loss: 0.4479 - acc: 0.790 - ETA: 2s - loss: 0.4340 - acc: 0.800 - ETA: 2s - loss: 0.4354 - acc: 0.805 - ETA: 1s - loss: 0.4515 - acc: 0.810 - ETA: 1s - loss: 0.4489 - acc: 0.817 - ETA: 1s - loss: 0.4467 - acc: 0.822 - ETA: 1s - loss: 0.4487 - acc: 0.824 - ETA: 1s - loss: 0.4477 - acc: 0.826 - ETA: 1s - loss: 0.4346 - acc: 0.832 - ETA: 1s - loss: 0.4255 - acc: 0.838 - ETA: 1s - loss: 0.4247 - acc: 0.836 - ETA: 1s - loss: 0.4220 - acc: 0.833 - ETA: 0s - loss: 0.4150 - acc: 0.837 - ETA: 0s - loss: 0.4078 - acc: 0.840 - ETA: 0s - loss: 0.4021 - acc: 0.841 - ETA: 0s - loss: 0.3957 - acc: 0.844 - ETA: 0s - loss: 0.3937 - acc: 0.845 - ETA: 0s - loss: 0.3860 - acc: 0.848 - ETA: 0s - loss: 0.3843 - acc: 0.849 - ETA: 0s - loss: 0.3790 - acc: 0.851 - ETA: 0s - loss: 0.3776 - acc: 0.852 - ETA: 0s - loss: 0.3741 - acc: 0.852 - 4s 4ms/sample - loss: 0.3713 - acc: 0.8545 - val_loss: 0.2255 - val_acc: 0.9091\n",
      "Epoch 2/5\n",
      "1230/1230 [==============================] - ETA: 3s - loss: 0.2004 - acc: 0.968 - ETA: 3s - loss: 0.2855 - acc: 0.890 - ETA: 2s - loss: 0.2305 - acc: 0.916 - ETA: 2s - loss: 0.2277 - acc: 0.929 - ETA: 2s - loss: 0.2170 - acc: 0.931 - ETA: 2s - loss: 0.2101 - acc: 0.927 - ETA: 2s - loss: 0.2154 - acc: 0.919 - ETA: 2s - loss: 0.1991 - acc: 0.925 - ETA: 2s - loss: 0.1992 - acc: 0.923 - ETA: 2s - loss: 0.1932 - acc: 0.925 - ETA: 2s - loss: 0.1913 - acc: 0.926 - ETA: 2s - loss: 0.1896 - acc: 0.927 - ETA: 2s - loss: 0.1851 - acc: 0.927 - ETA: 2s - loss: 0.1799 - acc: 0.930 - ETA: 2s - loss: 0.1834 - acc: 0.927 - ETA: 1s - loss: 0.1763 - acc: 0.929 - ETA: 1s - loss: 0.1860 - acc: 0.926 - ETA: 1s - loss: 0.1827 - acc: 0.927 - ETA: 1s - loss: 0.1831 - acc: 0.927 - ETA: 1s - loss: 0.1802 - acc: 0.929 - ETA: 1s - loss: 0.1783 - acc: 0.930 - ETA: 1s - loss: 0.1922 - acc: 0.924 - ETA: 1s - loss: 0.2132 - acc: 0.923 - ETA: 1s - loss: 0.2098 - acc: 0.925 - ETA: 1s - loss: 0.2182 - acc: 0.921 - ETA: 1s - loss: 0.2211 - acc: 0.920 - ETA: 1s - loss: 0.2171 - acc: 0.922 - ETA: 0s - loss: 0.2179 - acc: 0.920 - ETA: 0s - loss: 0.2193 - acc: 0.919 - ETA: 0s - loss: 0.2187 - acc: 0.918 - ETA: 0s - loss: 0.2205 - acc: 0.918 - ETA: 0s - loss: 0.2177 - acc: 0.920 - ETA: 0s - loss: 0.2204 - acc: 0.919 - ETA: 0s - loss: 0.2188 - acc: 0.921 - ETA: 0s - loss: 0.2173 - acc: 0.922 - ETA: 0s - loss: 0.2181 - acc: 0.921 - ETA: 0s - loss: 0.2145 - acc: 0.923 - ETA: 0s - loss: 0.2121 - acc: 0.924 - 4s 3ms/sample - loss: 0.2102 - acc: 0.9252 - val_loss: 0.2751 - val_acc: 0.8977\n",
      "Epoch 3/5\n",
      "1230/1230 [==============================] - ETA: 3s - loss: 0.1812 - acc: 0.937 - ETA: 3s - loss: 0.1842 - acc: 0.937 - ETA: 3s - loss: 0.1742 - acc: 0.937 - ETA: 3s - loss: 0.1837 - acc: 0.929 - ETA: 2s - loss: 0.1720 - acc: 0.937 - ETA: 2s - loss: 0.1653 - acc: 0.937 - ETA: 2s - loss: 0.1571 - acc: 0.942 - ETA: 2s - loss: 0.1510 - acc: 0.941 - ETA: 2s - loss: 0.1436 - acc: 0.947 - ETA: 2s - loss: 0.1580 - acc: 0.943 - ETA: 2s - loss: 0.1611 - acc: 0.943 - ETA: 2s - loss: 0.1514 - acc: 0.947 - ETA: 2s - loss: 0.1579 - acc: 0.947 - ETA: 2s - loss: 0.1580 - acc: 0.948 - ETA: 2s - loss: 0.1641 - acc: 0.945 - ETA: 2s - loss: 0.1574 - acc: 0.949 - ETA: 1s - loss: 0.1599 - acc: 0.948 - ETA: 1s - loss: 0.1666 - acc: 0.942 - ETA: 1s - loss: 0.1767 - acc: 0.937 - ETA: 1s - loss: 0.1713 - acc: 0.939 - ETA: 1s - loss: 0.1653 - acc: 0.942 - ETA: 1s - loss: 0.1646 - acc: 0.941 - ETA: 1s - loss: 0.1634 - acc: 0.942 - ETA: 1s - loss: 0.1695 - acc: 0.938 - ETA: 1s - loss: 0.1643 - acc: 0.941 - ETA: 1s - loss: 0.1621 - acc: 0.942 - ETA: 1s - loss: 0.1582 - acc: 0.944 - ETA: 0s - loss: 0.1601 - acc: 0.943 - ETA: 0s - loss: 0.1612 - acc: 0.942 - ETA: 0s - loss: 0.1623 - acc: 0.942 - ETA: 0s - loss: 0.1657 - acc: 0.940 - ETA: 0s - loss: 0.1695 - acc: 0.938 - ETA: 0s - loss: 0.1684 - acc: 0.939 - ETA: 0s - loss: 0.1670 - acc: 0.939 - ETA: 0s - loss: 0.1662 - acc: 0.940 - ETA: 0s - loss: 0.1653 - acc: 0.939 - ETA: 0s - loss: 0.1660 - acc: 0.938 - ETA: 0s - loss: 0.1666 - acc: 0.938 - 4s 3ms/sample - loss: 0.1685 - acc: 0.9366 - val_loss: 0.2094 - val_acc: 0.9034\n",
      "Epoch 4/5\n",
      "1230/1230 [==============================] - ETA: 3s - loss: 0.2249 - acc: 0.906 - ETA: 3s - loss: 0.1722 - acc: 0.937 - ETA: 3s - loss: 0.1655 - acc: 0.937 - ETA: 3s - loss: 0.1764 - acc: 0.937 - ETA: 2s - loss: 0.1785 - acc: 0.937 - ETA: 2s - loss: 0.1786 - acc: 0.937 - ETA: 2s - loss: 0.1618 - acc: 0.942 - ETA: 2s - loss: 0.1604 - acc: 0.941 - ETA: 2s - loss: 0.1546 - acc: 0.944 - ETA: 2s - loss: 0.1632 - acc: 0.940 - ETA: 2s - loss: 0.1589 - acc: 0.943 - ETA: 2s - loss: 0.1489 - acc: 0.947 - ETA: 2s - loss: 0.1462 - acc: 0.947 - ETA: 2s - loss: 0.1426 - acc: 0.946 - ETA: 2s - loss: 0.1383 - acc: 0.950 - ETA: 2s - loss: 0.1348 - acc: 0.953 - ETA: 1s - loss: 0.1377 - acc: 0.950 - ETA: 1s - loss: 0.1370 - acc: 0.951 - ETA: 1s - loss: 0.1353 - acc: 0.952 - ETA: 1s - loss: 0.1338 - acc: 0.951 - ETA: 1s - loss: 0.1350 - acc: 0.950 - ETA: 1s - loss: 0.1357 - acc: 0.951 - ETA: 1s - loss: 0.1353 - acc: 0.949 - ETA: 1s - loss: 0.1367 - acc: 0.947 - ETA: 1s - loss: 0.1363 - acc: 0.948 - ETA: 1s - loss: 0.1372 - acc: 0.948 - ETA: 1s - loss: 0.1341 - acc: 0.949 - ETA: 0s - loss: 0.1487 - acc: 0.944 - ETA: 0s - loss: 0.1531 - acc: 0.941 - ETA: 0s - loss: 0.1521 - acc: 0.941 - ETA: 0s - loss: 0.1520 - acc: 0.942 - ETA: 0s - loss: 0.1509 - acc: 0.942 - ETA: 0s - loss: 0.1513 - acc: 0.942 - ETA: 0s - loss: 0.1489 - acc: 0.943 - ETA: 0s - loss: 0.1513 - acc: 0.942 - ETA: 0s - loss: 0.1485 - acc: 0.943 - ETA: 0s - loss: 0.1496 - acc: 0.943 - ETA: 0s - loss: 0.1486 - acc: 0.944 - 4s 3ms/sample - loss: 0.1491 - acc: 0.9439 - val_loss: 0.1796 - val_acc: 0.9205\n",
      "Epoch 5/5\n",
      "1230/1230 [==============================] - ETA: 3s - loss: 0.1059 - acc: 0.968 - ETA: 3s - loss: 0.1006 - acc: 0.968 - ETA: 3s - loss: 0.0949 - acc: 0.979 - ETA: 3s - loss: 0.1200 - acc: 0.960 - ETA: 2s - loss: 0.1239 - acc: 0.968 - ETA: 2s - loss: 0.1350 - acc: 0.958 - ETA: 2s - loss: 0.1347 - acc: 0.955 - ETA: 2s - loss: 0.1216 - acc: 0.960 - ETA: 2s - loss: 0.1184 - acc: 0.961 - ETA: 2s - loss: 0.1246 - acc: 0.959 - ETA: 2s - loss: 0.1289 - acc: 0.957 - ETA: 2s - loss: 0.1242 - acc: 0.958 - ETA: 2s - loss: 0.1190 - acc: 0.961 - ETA: 2s - loss: 0.1161 - acc: 0.962 - ETA: 2s - loss: 0.1210 - acc: 0.960 - ETA: 2s - loss: 0.1173 - acc: 0.960 - ETA: 1s - loss: 0.1167 - acc: 0.961 - ETA: 1s - loss: 0.1125 - acc: 0.963 - ETA: 1s - loss: 0.1154 - acc: 0.962 - ETA: 1s - loss: 0.1192 - acc: 0.959 - ETA: 1s - loss: 0.1225 - acc: 0.956 - ETA: 1s - loss: 0.1190 - acc: 0.958 - ETA: 1s - loss: 0.1318 - acc: 0.956 - ETA: 1s - loss: 0.1299 - acc: 0.955 - ETA: 1s - loss: 0.1303 - acc: 0.957 - ETA: 1s - loss: 0.1266 - acc: 0.959 - ETA: 1s - loss: 0.1256 - acc: 0.959 - ETA: 0s - loss: 0.1219 - acc: 0.960 - ETA: 0s - loss: 0.1225 - acc: 0.959 - ETA: 0s - loss: 0.1244 - acc: 0.958 - ETA: 0s - loss: 0.1225 - acc: 0.959 - ETA: 0s - loss: 0.1241 - acc: 0.959 - ETA: 0s - loss: 0.1232 - acc: 0.960 - ETA: 0s - loss: 0.1280 - acc: 0.957 - ETA: 0s - loss: 0.1285 - acc: 0.957 - ETA: 0s - loss: 0.1306 - acc: 0.954 - ETA: 0s - loss: 0.1322 - acc: 0.954 - ETA: 0s - loss: 0.1311 - acc: 0.954 - 4s 3ms/sample - loss: 0.1301 - acc: 0.9553 - val_loss: 0.1785 - val_acc: 0.9242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ce71c9b438>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X/255.0\n",
    "\n",
    "NAME = \"Pokemon-model1-{}\".format(int(time.time()))\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))\n",
    "\n",
    "model1 = Sequential()\n",
    "\n",
    "# Conv2D in Convelution for a 2D image\n",
    "#3x3 in the window we are using\n",
    "model1.add(Conv2D(64, (3, 3), input_shape=X.shape[1:]))\n",
    "model1.add(Activation('relu'))\n",
    "#We do the pooling as size 2x2\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#We do it again, but dont need shapping\n",
    "model1.add(Conv2D(64, (3, 3)))\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#Convolution is 2D so we flatten the data to compress it to understandable\n",
    "model1.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "\n",
    "model1.add(Dense(64))\n",
    "model1.add(Activation('relu'))\n",
    "\n",
    "model1.add(Dense(1))\n",
    "model1.add(Activation('sigmoid'))\n",
    "\n",
    "model1.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model1.fit(X, y, batch_size=32, epochs=5, validation_split=0.3, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow out put for accuracy\n",
    "\n",
    "<img src=\"./Images/TensorBoard/epoch_acc_model1.svg?sanitize=true\" width=\"500\"/>\n",
    "\n",
    "As see, the `acuracy` increases slowly, even though it started from a good value of 0.85 it plateaus after 3. Mostly we can see how it performed with a higher Epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TanH\n",
    "\n",
    "$f(x)=\\tanh(x)=\\frac{2}{1+e^{-2x}}-1$\n",
    "$f'(x)=1-f(x)^2$\n",
    "$(-1,1)$\n",
    "$C^\\infty$\n",
    "\n",
    "##### The Hyperbolic Tangent Activation Function\n",
    "\n",
    "Though the logistic sigmoid has a nice biological interpretation, it turns out that the logistic sigmoid can cause a neural network to get “stuck” during training. This is due in part to the fact that if a strongly-negative input is provided to the logistic sigmoid, it outputs values very near zero. Since neural networks use the feed-forward activations to calculate parameter gradients (again, see this previous post for details), this can result in model parameters that are updated less regularly than we would like, and are thus “stuck” in their current state.\n",
    "\n",
    "An alternative to the logistic sigmoid is the hyperbolic tangent, or tanh function:\n",
    "\n",
    "<img src=\"Images/tanh.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross Entropy\n",
    "\n",
    "Cross-Entropy\n",
    "Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - CNN\n",
    "With below conditions\n",
    "* <span class=\"mark\">TanH</span>\n",
    "* CrossEntropy\n",
    "* 5 Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1230 samples, validate on 528 samples\n",
      "Epoch 1/5\n",
      "1230/1230 [==============================] - ETA: 41s - loss: 0.6978 - acc: 0.53 - ETA: 32s - loss: 1.3796 - acc: 0.53 - ETA: 29s - loss: 2.8218 - acc: 0.48 - ETA: 27s - loss: 3.2507 - acc: 0.50 - ETA: 25s - loss: 3.5030 - acc: 0.50 - ETA: 24s - loss: 3.6667 - acc: 0.51 - ETA: 23s - loss: 3.8220 - acc: 0.50 - ETA: 22s - loss: 3.9346 - acc: 0.50 - ETA: 21s - loss: 3.9535 - acc: 0.51 - ETA: 20s - loss: 3.9659 - acc: 0.51 - ETA: 20s - loss: 4.0261 - acc: 0.51 - ETA: 19s - loss: 4.0018 - acc: 0.52 - ETA: 18s - loss: 4.0889 - acc: 0.51 - ETA: 17s - loss: 4.1003 - acc: 0.51 - ETA: 16s - loss: 4.0894 - acc: 0.52 - ETA: 16s - loss: 4.1328 - acc: 0.51 - ETA: 15s - loss: 4.1016 - acc: 0.52 - ETA: 14s - loss: 4.0724 - acc: 0.52 - ETA: 13s - loss: 4.0880 - acc: 0.52 - ETA: 13s - loss: 4.0869 - acc: 0.52 - ETA: 12s - loss: 4.0844 - acc: 0.52 - ETA: 11s - loss: 4.0565 - acc: 0.52 - ETA: 10s - loss: 4.0875 - acc: 0.52 - ETA: 10s - loss: 4.1253 - acc: 0.51 - ETA: 9s - loss: 4.1272 - acc: 0.5175 - ETA: 8s - loss: 4.1177 - acc: 0.518 - ETA: 8s - loss: 4.0982 - acc: 0.519 - ETA: 7s - loss: 4.0792 - acc: 0.521 - ETA: 6s - loss: 4.0517 - acc: 0.523 - ETA: 5s - loss: 4.0502 - acc: 0.522 - ETA: 5s - loss: 4.0718 - acc: 0.519 - ETA: 4s - loss: 4.0524 - acc: 0.520 - ETA: 3s - loss: 4.0554 - acc: 0.518 - ETA: 3s - loss: 4.0287 - acc: 0.521 - ETA: 2s - loss: 4.0166 - acc: 0.521 - ETA: 1s - loss: 3.9976 - acc: 0.522 - ETA: 1s - loss: 3.9852 - acc: 0.522 - ETA: 0s - loss: 3.9728 - acc: 0.523 - 32s 26ms/sample - loss: 3.9637 - acc: 0.5236 - val_loss: 3.2336 - val_acc: 0.5606\n",
      "Epoch 2/5\n",
      "1230/1230 [==============================] - ETA: 26s - loss: 4.1394 - acc: 0.43 - ETA: 25s - loss: 4.2349 - acc: 0.42 - ETA: 24s - loss: 4.1029 - acc: 0.43 - ETA: 23s - loss: 3.8043 - acc: 0.47 - ETA: 23s - loss: 3.7084 - acc: 0.48 - ETA: 22s - loss: 3.5662 - acc: 0.50 - ETA: 21s - loss: 3.7093 - acc: 0.48 - ETA: 21s - loss: 3.7035 - acc: 0.48 - ETA: 20s - loss: 3.6476 - acc: 0.48 - ETA: 19s - loss: 3.5997 - acc: 0.49 - ETA: 19s - loss: 3.5958 - acc: 0.48 - ETA: 18s - loss: 3.5377 - acc: 0.49 - ETA: 17s - loss: 3.5179 - acc: 0.49 - ETA: 17s - loss: 3.4406 - acc: 0.50 - ETA: 16s - loss: 3.3987 - acc: 0.50 - ETA: 15s - loss: 3.3728 - acc: 0.50 - ETA: 14s - loss: 3.3481 - acc: 0.51 - ETA: 14s - loss: 3.3245 - acc: 0.51 - ETA: 13s - loss: 3.2916 - acc: 0.51 - ETA: 12s - loss: 3.2511 - acc: 0.51 - ETA: 12s - loss: 3.2132 - acc: 0.52 - ETA: 11s - loss: 3.1776 - acc: 0.52 - ETA: 10s - loss: 3.1601 - acc: 0.52 - ETA: 10s - loss: 3.1657 - acc: 0.52 - ETA: 9s - loss: 3.1477 - acc: 0.5225 - ETA: 8s - loss: 3.1094 - acc: 0.526 - ETA: 7s - loss: 3.0992 - acc: 0.525 - ETA: 7s - loss: 3.0948 - acc: 0.523 - ETA: 6s - loss: 3.0777 - acc: 0.523 - ETA: 5s - loss: 3.0493 - acc: 0.526 - ETA: 5s - loss: 3.0219 - acc: 0.528 - ETA: 4s - loss: 3.0320 - acc: 0.523 - ETA: 3s - loss: 3.0103 - acc: 0.524 - ETA: 3s - loss: 3.0178 - acc: 0.520 - ETA: 2s - loss: 2.9869 - acc: 0.523 - ETA: 1s - loss: 2.9791 - acc: 0.521 - ETA: 0s - loss: 2.9581 - acc: 0.522 - ETA: 0s - loss: 2.9375 - acc: 0.523 - 32s 26ms/sample - loss: 2.9359 - acc: 0.5228 - val_loss: 2.1279 - val_acc: 0.5606\n",
      "Epoch 3/5\n",
      "1230/1230 [==============================] - ETA: 26s - loss: 2.1187 - acc: 0.56 - ETA: 25s - loss: 2.4023 - acc: 0.50 - ETA: 25s - loss: 2.2891 - acc: 0.52 - ETA: 25s - loss: 2.3341 - acc: 0.50 - ETA: 24s - loss: 2.1830 - acc: 0.53 - ETA: 23s - loss: 2.1960 - acc: 0.53 - ETA: 22s - loss: 2.2403 - acc: 0.51 - ETA: 21s - loss: 2.2179 - acc: 0.51 - ETA: 21s - loss: 2.2270 - acc: 0.51 - ETA: 20s - loss: 2.2177 - acc: 0.51 - ETA: 20s - loss: 2.2071 - acc: 0.51 - ETA: 19s - loss: 2.2064 - acc: 0.50 - ETA: 19s - loss: 2.1740 - acc: 0.51 - ETA: 18s - loss: 2.1797 - acc: 0.50 - ETA: 17s - loss: 2.1497 - acc: 0.51 - ETA: 17s - loss: 2.1368 - acc: 0.50 - ETA: 16s - loss: 2.1166 - acc: 0.51 - ETA: 15s - loss: 2.0711 - acc: 0.51 - ETA: 14s - loss: 2.0184 - acc: 0.52 - ETA: 13s - loss: 2.0085 - acc: 0.52 - ETA: 13s - loss: 1.9877 - acc: 0.52 - ETA: 12s - loss: 1.9627 - acc: 0.53 - ETA: 11s - loss: 1.9389 - acc: 0.53 - ETA: 10s - loss: 1.9294 - acc: 0.53 - ETA: 9s - loss: 1.9194 - acc: 0.5325 - ETA: 9s - loss: 1.9092 - acc: 0.531 - ETA: 8s - loss: 1.8951 - acc: 0.531 - ETA: 7s - loss: 1.8984 - acc: 0.525 - ETA: 6s - loss: 1.8870 - acc: 0.524 - ETA: 6s - loss: 1.8848 - acc: 0.520 - ETA: 5s - loss: 1.8670 - acc: 0.522 - ETA: 4s - loss: 1.8578 - acc: 0.520 - ETA: 4s - loss: 1.8351 - acc: 0.523 - ETA: 3s - loss: 1.8106 - acc: 0.527 - ETA: 2s - loss: 1.8014 - acc: 0.525 - ETA: 1s - loss: 1.7783 - acc: 0.529 - ETA: 1s - loss: 1.7754 - acc: 0.525 - ETA: 0s - loss: 1.7657 - acc: 0.523 - 33s 27ms/sample - loss: 1.7624 - acc: 0.5228 - val_loss: 1.1328 - val_acc: 0.5606\n",
      "Epoch 4/5\n",
      "1230/1230 [==============================] - ETA: 27s - loss: 1.4253 - acc: 0.43 - ETA: 25s - loss: 1.2651 - acc: 0.50 - ETA: 24s - loss: 1.1557 - acc: 0.54 - ETA: 24s - loss: 1.1654 - acc: 0.53 - ETA: 23s - loss: 1.1403 - acc: 0.53 - ETA: 22s - loss: 1.1303 - acc: 0.54 - ETA: 21s - loss: 1.1571 - acc: 0.52 - ETA: 21s - loss: 1.1665 - acc: 0.51 - ETA: 20s - loss: 1.1644 - acc: 0.50 - ETA: 19s - loss: 1.1318 - acc: 0.51 - ETA: 19s - loss: 1.1193 - acc: 0.51 - ETA: 18s - loss: 1.1301 - acc: 0.50 - ETA: 17s - loss: 1.1210 - acc: 0.50 - ETA: 16s - loss: 1.1227 - acc: 0.50 - ETA: 16s - loss: 1.1063 - acc: 0.50 - ETA: 15s - loss: 1.0851 - acc: 0.51 - ETA: 14s - loss: 1.0712 - acc: 0.51 - ETA: 14s - loss: 1.0508 - acc: 0.52 - ETA: 13s - loss: 1.0390 - acc: 0.52 - ETA: 12s - loss: 1.0319 - acc: 0.52 - ETA: 12s - loss: 1.0210 - acc: 0.52 - ETA: 11s - loss: 1.0072 - acc: 0.52 - ETA: 10s - loss: 1.0008 - acc: 0.52 - ETA: 10s - loss: 0.9885 - acc: 0.53 - ETA: 9s - loss: 0.9797 - acc: 0.5337 - ETA: 8s - loss: 0.9739 - acc: 0.532 - ETA: 7s - loss: 0.9658 - acc: 0.533 - ETA: 7s - loss: 0.9570 - acc: 0.535 - ETA: 6s - loss: 0.9496 - acc: 0.536 - ETA: 5s - loss: 0.9464 - acc: 0.533 - ETA: 5s - loss: 0.9386 - acc: 0.535 - ETA: 4s - loss: 0.9336 - acc: 0.534 - ETA: 3s - loss: 0.9319 - acc: 0.529 - ETA: 3s - loss: 0.9292 - acc: 0.525 - ETA: 2s - loss: 0.9231 - acc: 0.526 - ETA: 1s - loss: 0.9178 - acc: 0.526 - ETA: 0s - loss: 0.9150 - acc: 0.523 - ETA: 0s - loss: 0.9115 - acc: 0.521 - 32s 26ms/sample - loss: 0.9085 - acc: 0.5228 - val_loss: 0.6978 - val_acc: 0.5606\n",
      "Epoch 5/5\n",
      "1230/1230 [==============================] - ETA: 26s - loss: 0.7530 - acc: 0.46 - ETA: 25s - loss: 0.6893 - acc: 0.57 - ETA: 24s - loss: 0.7128 - acc: 0.53 - ETA: 23s - loss: 0.7096 - acc: 0.53 - ETA: 22s - loss: 0.7162 - acc: 0.51 - ETA: 22s - loss: 0.7156 - acc: 0.51 - ETA: 21s - loss: 0.7124 - acc: 0.50 - ETA: 20s - loss: 0.7169 - acc: 0.49 - ETA: 20s - loss: 0.7112 - acc: 0.49 - ETA: 19s - loss: 0.7030 - acc: 0.50 - ETA: 18s - loss: 0.6970 - acc: 0.50 - ETA: 18s - loss: 0.6962 - acc: 0.50 - ETA: 17s - loss: 0.6844 - acc: 0.51 - ETA: 16s - loss: 0.6800 - acc: 0.51 - ETA: 16s - loss: 0.6748 - acc: 0.51 - ETA: 15s - loss: 0.6707 - acc: 0.51 - ETA: 14s - loss: 0.6655 - acc: 0.52 - ETA: 14s - loss: 0.6582 - acc: 0.52 - ETA: 13s - loss: 0.6492 - acc: 0.52 - ETA: 12s - loss: 0.6384 - acc: 0.52 - ETA: 11s - loss: 0.6315 - acc: 0.52 - ETA: 11s - loss: 0.6220 - acc: 0.53 - ETA: 10s - loss: 0.6172 - acc: 0.52 - ETA: 9s - loss: 0.6109 - acc: 0.5221 - ETA: 9s - loss: 0.6020 - acc: 0.525 - ETA: 8s - loss: 0.5946 - acc: 0.524 - ETA: 7s - loss: 0.5851 - acc: 0.539 - ETA: 7s - loss: 0.5805 - acc: 0.552 - ETA: 6s - loss: 0.5735 - acc: 0.565 - ETA: 5s - loss: 0.5670 - acc: 0.578 - ETA: 5s - loss: 0.5625 - acc: 0.585 - ETA: 4s - loss: 0.5572 - acc: 0.595 - ETA: 3s - loss: 0.5531 - acc: 0.603 - ETA: 3s - loss: 0.5478 - acc: 0.614 - ETA: 2s - loss: 0.5438 - acc: 0.621 - ETA: 1s - loss: 0.5364 - acc: 0.630 - ETA: 0s - loss: 0.5292 - acc: 0.637 - ETA: 0s - loss: 0.5207 - acc: 0.646 - 31s 25ms/sample - loss: 0.5177 - acc: 0.6496 - val_loss: 0.3105 - val_acc: 0.8845\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ce7569fef0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "NAME = \"Pokemon-model2-{}\".format(int(time.time()))\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))\n",
    "\n",
    "\n",
    "model2.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
    "model2.add(Activation('tanh'))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model2.add(Conv2D(256, (3, 3)))\n",
    "model2.add(Activation('tanh'))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model2.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "\n",
    "model2.add(Dense(64))\n",
    "model2.add(Activation('tanh'))\n",
    "\n",
    "model2.add(Dense(1))\n",
    "model2.add(Activation('sigmoid'))\n",
    "\n",
    "model2.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model2.fit(X, y, batch_size=32, epochs=5, validation_split=0.3, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow out put for accuracy\n",
    "\n",
    "<img src=\"Images/TensorBoard/epoch_acc_model2.svg?sanitize=true\" width=\"500\">\n",
    "\n",
    "Seems the model doesn't perfrom good using the tanh activation function. It starts from a very low `accuracy` of 0.54 and increases slowly after an Epoch of 3. That's the conclusion for this model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - CNN - <span class=\"mark\">Softsign</span> - CrossEntropy - 5 Epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Softsign function: \n",
    "\n",
    "$y = x / (1 + |x|)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1230 samples, validate on 528 samples\n",
      "Epoch 1/5\n",
      "1230/1230 [==============================] - ETA: 40s - loss: 0.6863 - acc: 0.62 - ETA: 32s - loss: 1.1585 - acc: 0.57 - ETA: 29s - loss: 2.0871 - acc: 0.55 - ETA: 27s - loss: 2.6814 - acc: 0.51 - ETA: 25s - loss: 2.2101 - acc: 0.58 - ETA: 24s - loss: 2.0334 - acc: 0.62 - ETA: 23s - loss: 1.7999 - acc: 0.66 - ETA: 22s - loss: 1.6745 - acc: 0.69 - ETA: 21s - loss: 1.5355 - acc: 0.72 - ETA: 20s - loss: 1.4182 - acc: 0.74 - ETA: 20s - loss: 1.3616 - acc: 0.74 - ETA: 19s - loss: 1.2905 - acc: 0.74 - ETA: 18s - loss: 1.2419 - acc: 0.73 - ETA: 17s - loss: 1.1709 - acc: 0.74 - ETA: 16s - loss: 1.1014 - acc: 0.76 - ETA: 16s - loss: 1.0603 - acc: 0.76 - ETA: 15s - loss: 1.0163 - acc: 0.77 - ETA: 14s - loss: 0.9801 - acc: 0.77 - ETA: 13s - loss: 0.9423 - acc: 0.78 - ETA: 13s - loss: 0.9101 - acc: 0.79 - ETA: 12s - loss: 0.8765 - acc: 0.79 - ETA: 11s - loss: 0.8478 - acc: 0.80 - ETA: 11s - loss: 0.8297 - acc: 0.80 - ETA: 10s - loss: 0.8132 - acc: 0.80 - ETA: 9s - loss: 0.7977 - acc: 0.8075 - ETA: 8s - loss: 0.7776 - acc: 0.811 - ETA: 8s - loss: 0.7621 - acc: 0.813 - ETA: 7s - loss: 0.7444 - acc: 0.817 - ETA: 6s - loss: 0.7281 - acc: 0.820 - ETA: 5s - loss: 0.7083 - acc: 0.825 - ETA: 5s - loss: 0.6963 - acc: 0.826 - ETA: 4s - loss: 0.6853 - acc: 0.828 - ETA: 3s - loss: 0.6803 - acc: 0.826 - ETA: 3s - loss: 0.6661 - acc: 0.830 - ETA: 2s - loss: 0.6532 - acc: 0.833 - ETA: 1s - loss: 0.6407 - acc: 0.835 - ETA: 1s - loss: 0.6250 - acc: 0.840 - ETA: 0s - loss: 0.6156 - acc: 0.842 - 32s 26ms/sample - loss: 0.6113 - acc: 0.8431 - val_loss: 0.3022 - val_acc: 0.8826\n",
      "Epoch 2/5\n",
      "1230/1230 [==============================] - ETA: 27s - loss: 0.1153 - acc: 0.96 - ETA: 26s - loss: 0.2613 - acc: 0.89 - ETA: 25s - loss: 0.2397 - acc: 0.90 - ETA: 25s - loss: 0.2139 - acc: 0.92 - ETA: 24s - loss: 0.2096 - acc: 0.92 - ETA: 23s - loss: 0.2175 - acc: 0.92 - ETA: 22s - loss: 0.2194 - acc: 0.91 - ETA: 21s - loss: 0.2284 - acc: 0.91 - ETA: 21s - loss: 0.2395 - acc: 0.90 - ETA: 20s - loss: 0.2375 - acc: 0.90 - ETA: 19s - loss: 0.2301 - acc: 0.91 - ETA: 18s - loss: 0.2368 - acc: 0.91 - ETA: 18s - loss: 0.2501 - acc: 0.90 - ETA: 17s - loss: 0.2537 - acc: 0.90 - ETA: 16s - loss: 0.2605 - acc: 0.89 - ETA: 15s - loss: 0.2600 - acc: 0.89 - ETA: 15s - loss: 0.2641 - acc: 0.89 - ETA: 14s - loss: 0.2632 - acc: 0.89 - ETA: 13s - loss: 0.2658 - acc: 0.89 - ETA: 12s - loss: 0.2650 - acc: 0.89 - ETA: 12s - loss: 0.2634 - acc: 0.90 - ETA: 11s - loss: 0.2595 - acc: 0.90 - ETA: 10s - loss: 0.2622 - acc: 0.90 - ETA: 10s - loss: 0.2628 - acc: 0.90 - ETA: 9s - loss: 0.2622 - acc: 0.9025 - ETA: 8s - loss: 0.2639 - acc: 0.900 - ETA: 8s - loss: 0.2641 - acc: 0.899 - ETA: 7s - loss: 0.2680 - acc: 0.896 - ETA: 6s - loss: 0.2679 - acc: 0.896 - ETA: 5s - loss: 0.2689 - acc: 0.895 - ETA: 5s - loss: 0.2703 - acc: 0.895 - ETA: 4s - loss: 0.2697 - acc: 0.895 - ETA: 3s - loss: 0.2678 - acc: 0.896 - ETA: 3s - loss: 0.2651 - acc: 0.898 - ETA: 2s - loss: 0.2674 - acc: 0.895 - ETA: 1s - loss: 0.2645 - acc: 0.897 - ETA: 1s - loss: 0.2615 - acc: 0.899 - ETA: 0s - loss: 0.2609 - acc: 0.900 - 32s 26ms/sample - loss: 0.2625 - acc: 0.8992 - val_loss: 0.2638 - val_acc: 0.8883\n",
      "Epoch 3/5\n",
      "1230/1230 [==============================] - ETA: 26s - loss: 0.1693 - acc: 0.96 - ETA: 25s - loss: 0.2209 - acc: 0.92 - ETA: 24s - loss: 0.2350 - acc: 0.91 - ETA: 23s - loss: 0.2213 - acc: 0.92 - ETA: 23s - loss: 0.2368 - acc: 0.90 - ETA: 22s - loss: 0.2481 - acc: 0.90 - ETA: 21s - loss: 0.2481 - acc: 0.90 - ETA: 21s - loss: 0.2509 - acc: 0.89 - ETA: 20s - loss: 0.2385 - acc: 0.90 - ETA: 19s - loss: 0.2302 - acc: 0.91 - ETA: 19s - loss: 0.2318 - acc: 0.90 - ETA: 18s - loss: 0.2270 - acc: 0.91 - ETA: 17s - loss: 0.2218 - acc: 0.91 - ETA: 16s - loss: 0.2284 - acc: 0.91 - ETA: 16s - loss: 0.2296 - acc: 0.91 - ETA: 15s - loss: 0.2246 - acc: 0.91 - ETA: 14s - loss: 0.2283 - acc: 0.91 - ETA: 14s - loss: 0.2235 - acc: 0.91 - ETA: 13s - loss: 0.2220 - acc: 0.91 - ETA: 12s - loss: 0.2205 - acc: 0.91 - ETA: 12s - loss: 0.2217 - acc: 0.91 - ETA: 11s - loss: 0.2230 - acc: 0.91 - ETA: 10s - loss: 0.2283 - acc: 0.90 - ETA: 9s - loss: 0.2265 - acc: 0.9102 - ETA: 9s - loss: 0.2298 - acc: 0.908 - ETA: 8s - loss: 0.2322 - acc: 0.907 - ETA: 7s - loss: 0.2299 - acc: 0.909 - ETA: 7s - loss: 0.2276 - acc: 0.910 - ETA: 6s - loss: 0.2244 - acc: 0.911 - ETA: 5s - loss: 0.2322 - acc: 0.908 - ETA: 5s - loss: 0.2371 - acc: 0.906 - ETA: 4s - loss: 0.2329 - acc: 0.908 - ETA: 3s - loss: 0.2318 - acc: 0.908 - ETA: 3s - loss: 0.2343 - acc: 0.907 - ETA: 2s - loss: 0.2336 - acc: 0.907 - ETA: 1s - loss: 0.2341 - acc: 0.907 - ETA: 0s - loss: 0.2345 - acc: 0.907 - ETA: 0s - loss: 0.2320 - acc: 0.908 - 32s 26ms/sample - loss: 0.2333 - acc: 0.9081 - val_loss: 0.2351 - val_acc: 0.8977\n",
      "Epoch 4/5\n",
      "1230/1230 [==============================] - ETA: 27s - loss: 0.1335 - acc: 0.93 - ETA: 25s - loss: 0.1487 - acc: 0.93 - ETA: 25s - loss: 0.1826 - acc: 0.91 - ETA: 24s - loss: 0.1897 - acc: 0.92 - ETA: 23s - loss: 0.1759 - acc: 0.94 - ETA: 22s - loss: 0.1726 - acc: 0.94 - ETA: 22s - loss: 0.1596 - acc: 0.95 - ETA: 21s - loss: 0.1673 - acc: 0.94 - ETA: 20s - loss: 0.1764 - acc: 0.94 - ETA: 19s - loss: 0.1946 - acc: 0.92 - ETA: 19s - loss: 0.2111 - acc: 0.92 - ETA: 18s - loss: 0.2036 - acc: 0.92 - ETA: 17s - loss: 0.1992 - acc: 0.92 - ETA: 17s - loss: 0.1967 - acc: 0.92 - ETA: 16s - loss: 0.1987 - acc: 0.92 - ETA: 15s - loss: 0.2012 - acc: 0.92 - ETA: 14s - loss: 0.1967 - acc: 0.93 - ETA: 14s - loss: 0.1941 - acc: 0.93 - ETA: 13s - loss: 0.1906 - acc: 0.93 - ETA: 12s - loss: 0.1947 - acc: 0.93 - ETA: 12s - loss: 0.1923 - acc: 0.93 - ETA: 11s - loss: 0.1871 - acc: 0.93 - ETA: 10s - loss: 0.1865 - acc: 0.93 - ETA: 10s - loss: 0.1896 - acc: 0.92 - ETA: 9s - loss: 0.1881 - acc: 0.9300 - ETA: 8s - loss: 0.1938 - acc: 0.925 - ETA: 7s - loss: 0.1959 - acc: 0.924 - ETA: 7s - loss: 0.1990 - acc: 0.923 - ETA: 6s - loss: 0.2023 - acc: 0.919 - ETA: 5s - loss: 0.2074 - acc: 0.916 - ETA: 5s - loss: 0.2075 - acc: 0.917 - ETA: 4s - loss: 0.2099 - acc: 0.916 - ETA: 3s - loss: 0.2093 - acc: 0.915 - ETA: 3s - loss: 0.2099 - acc: 0.915 - ETA: 2s - loss: 0.2120 - acc: 0.914 - ETA: 1s - loss: 0.2101 - acc: 0.914 - ETA: 0s - loss: 0.2066 - acc: 0.917 - ETA: 0s - loss: 0.2082 - acc: 0.916 - 32s 26ms/sample - loss: 0.2073 - acc: 0.9179 - val_loss: 0.2038 - val_acc: 0.9148\n",
      "Epoch 5/5\n",
      "1230/1230 [==============================] - ETA: 26s - loss: 0.0701 - acc: 0.96 - ETA: 25s - loss: 0.1477 - acc: 0.92 - ETA: 24s - loss: 0.1235 - acc: 0.94 - ETA: 23s - loss: 0.1199 - acc: 0.94 - ETA: 23s - loss: 0.1014 - acc: 0.95 - ETA: 22s - loss: 0.1312 - acc: 0.94 - ETA: 21s - loss: 0.1357 - acc: 0.94 - ETA: 21s - loss: 0.1314 - acc: 0.94 - ETA: 20s - loss: 0.1472 - acc: 0.93 - ETA: 19s - loss: 0.1504 - acc: 0.93 - ETA: 18s - loss: 0.1422 - acc: 0.94 - ETA: 18s - loss: 0.1409 - acc: 0.94 - ETA: 17s - loss: 0.1585 - acc: 0.93 - ETA: 16s - loss: 0.1803 - acc: 0.93 - ETA: 16s - loss: 0.1808 - acc: 0.93 - ETA: 15s - loss: 0.1788 - acc: 0.93 - ETA: 14s - loss: 0.1776 - acc: 0.93 - ETA: 14s - loss: 0.1740 - acc: 0.93 - ETA: 13s - loss: 0.1681 - acc: 0.93 - ETA: 12s - loss: 0.1752 - acc: 0.93 - ETA: 12s - loss: 0.1724 - acc: 0.93 - ETA: 11s - loss: 0.1655 - acc: 0.93 - ETA: 10s - loss: 0.1743 - acc: 0.93 - ETA: 10s - loss: 0.1872 - acc: 0.93 - ETA: 9s - loss: 0.2001 - acc: 0.9237 - ETA: 8s - loss: 0.1977 - acc: 0.925 - ETA: 7s - loss: 0.1987 - acc: 0.925 - ETA: 7s - loss: 0.2018 - acc: 0.925 - ETA: 6s - loss: 0.2052 - acc: 0.922 - ETA: 5s - loss: 0.2076 - acc: 0.920 - ETA: 5s - loss: 0.2089 - acc: 0.920 - ETA: 4s - loss: 0.2082 - acc: 0.921 - ETA: 3s - loss: 0.2061 - acc: 0.923 - ETA: 3s - loss: 0.2051 - acc: 0.923 - ETA: 2s - loss: 0.2061 - acc: 0.923 - ETA: 1s - loss: 0.2059 - acc: 0.923 - ETA: 0s - loss: 0.2099 - acc: 0.921 - ETA: 0s - loss: 0.2138 - acc: 0.918 - 32s 26ms/sample - loss: 0.2122 - acc: 0.9195 - val_loss: 0.2644 - val_acc: 0.8864\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ce20ce2780>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "\n",
    "NAME = \"Pokemon-model3-{}\".format(int(time.time()))\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))\n",
    "\n",
    "\n",
    "model3.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
    "model3.add(Activation('softsign'))\n",
    "model3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model3.add(Conv2D(256, (3, 3)))\n",
    "model3.add(Activation('softsign'))\n",
    "model3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model3.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "\n",
    "model3.add(Dense(64))\n",
    "model3.add(Activation('softsign'))\n",
    "\n",
    "model3.add(Dense(1))\n",
    "model3.add(Activation('sigmoid'))\n",
    "\n",
    "model3.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model3.fit(X, y, batch_size=32, epochs=5, validation_split=0.3, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow out put for accuracy\n",
    "\n",
    "<img src=\"Images/TensorBoard/epoch_acc_model3.svg?sanitize=true\" width=\"500\">\n",
    "\n",
    "Softsign helps get a good accuracy at the beginning, but plateaus later as it goes.\n",
    "\n",
    "#### Now, lets compare Model1,Model2,Model3 based on the activation function given:\n",
    "\n",
    "<img src=\"Images/TensorBoard/epoch_acc_activation.svg?sanitize=true\" width=\"500\">\n",
    "\n",
    "#### Seems, `Model1` turns our to be the best in model accuracy. We can show a comparision based on the validation test done on the validation images as shown below with max accuracy at \n",
    "\n",
    "<img src=\"Images/comparision_value.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Part C` - Cost Function\n",
    "\n",
    "Now, as we had already used one of the Cost function to show case its performance, which was `CrossEntropy`, we now try to run it for more Epochs to check its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4 - CNN - Softsign - CrossEntropy - <span class=\"mark\">10 Epoch</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1230 samples, validate on 528 samples\n",
      "Epoch 1/10\n",
      "1230/1230 [==============================] - ETA: 39s - loss: 0.6895 - acc: 0.65 - ETA: 32s - loss: 1.4516 - acc: 0.57 - ETA: 29s - loss: 2.0310 - acc: 0.57 - ETA: 27s - loss: 2.3549 - acc: 0.57 - ETA: 25s - loss: 2.2934 - acc: 0.58 - ETA: 24s - loss: 2.0005 - acc: 0.63 - ETA: 23s - loss: 1.8942 - acc: 0.64 - ETA: 22s - loss: 1.6966 - acc: 0.68 - ETA: 21s - loss: 1.5442 - acc: 0.70 - ETA: 20s - loss: 1.4347 - acc: 0.70 - ETA: 20s - loss: 1.3432 - acc: 0.70 - ETA: 19s - loss: 1.2582 - acc: 0.72 - ETA: 18s - loss: 1.1857 - acc: 0.73 - ETA: 17s - loss: 1.1184 - acc: 0.74 - ETA: 16s - loss: 1.0704 - acc: 0.75 - ETA: 16s - loss: 1.0177 - acc: 0.76 - ETA: 15s - loss: 0.9643 - acc: 0.77 - ETA: 14s - loss: 0.9228 - acc: 0.78 - ETA: 13s - loss: 0.9007 - acc: 0.78 - ETA: 13s - loss: 0.8837 - acc: 0.78 - ETA: 12s - loss: 0.8538 - acc: 0.78 - ETA: 11s - loss: 0.8300 - acc: 0.79 - ETA: 11s - loss: 0.8089 - acc: 0.79 - ETA: 10s - loss: 0.7793 - acc: 0.80 - ETA: 9s - loss: 0.7620 - acc: 0.8062 - ETA: 8s - loss: 0.7405 - acc: 0.811 - ETA: 8s - loss: 0.7236 - acc: 0.813 - ETA: 7s - loss: 0.7077 - acc: 0.817 - ETA: 6s - loss: 0.6896 - acc: 0.821 - ETA: 5s - loss: 0.6728 - acc: 0.825 - ETA: 5s - loss: 0.6577 - acc: 0.828 - ETA: 4s - loss: 0.6445 - acc: 0.831 - ETA: 3s - loss: 0.6295 - acc: 0.835 - ETA: 3s - loss: 0.6168 - acc: 0.838 - ETA: 2s - loss: 0.6072 - acc: 0.840 - ETA: 1s - loss: 0.5940 - acc: 0.843 - ETA: 1s - loss: 0.5853 - acc: 0.844 - ETA: 0s - loss: 0.5746 - acc: 0.847 - 32s 26ms/sample - loss: 0.5706 - acc: 0.8480 - val_loss: 0.2744 - val_acc: 0.8864\n",
      "Epoch 2/10\n",
      "1230/1230 [==============================] - ETA: 27s - loss: 0.1064 - acc: 1.00 - ETA: 26s - loss: 0.1480 - acc: 0.96 - ETA: 24s - loss: 0.2412 - acc: 0.91 - ETA: 24s - loss: 0.2471 - acc: 0.91 - ETA: 23s - loss: 0.2429 - acc: 0.91 - ETA: 22s - loss: 0.2516 - acc: 0.90 - ETA: 21s - loss: 0.2465 - acc: 0.90 - ETA: 21s - loss: 0.2413 - acc: 0.91 - ETA: 20s - loss: 0.2373 - acc: 0.91 - ETA: 19s - loss: 0.2277 - acc: 0.91 - ETA: 18s - loss: 0.2193 - acc: 0.92 - ETA: 18s - loss: 0.2179 - acc: 0.91 - ETA: 17s - loss: 0.2190 - acc: 0.92 - ETA: 16s - loss: 0.2181 - acc: 0.92 - ETA: 16s - loss: 0.2236 - acc: 0.92 - ETA: 15s - loss: 0.2386 - acc: 0.91 - ETA: 14s - loss: 0.2469 - acc: 0.90 - ETA: 14s - loss: 0.2386 - acc: 0.91 - ETA: 13s - loss: 0.2407 - acc: 0.91 - ETA: 12s - loss: 0.2491 - acc: 0.90 - ETA: 12s - loss: 0.2562 - acc: 0.90 - ETA: 11s - loss: 0.2509 - acc: 0.90 - ETA: 10s - loss: 0.2573 - acc: 0.90 - ETA: 10s - loss: 0.2566 - acc: 0.90 - ETA: 9s - loss: 0.2596 - acc: 0.9050 - ETA: 8s - loss: 0.2571 - acc: 0.906 - ETA: 7s - loss: 0.2615 - acc: 0.903 - ETA: 7s - loss: 0.2589 - acc: 0.905 - ETA: 6s - loss: 0.2636 - acc: 0.903 - ETA: 5s - loss: 0.2690 - acc: 0.900 - ETA: 5s - loss: 0.2638 - acc: 0.902 - ETA: 4s - loss: 0.2672 - acc: 0.900 - ETA: 3s - loss: 0.2647 - acc: 0.901 - ETA: 3s - loss: 0.2599 - acc: 0.904 - ETA: 2s - loss: 0.2591 - acc: 0.904 - ETA: 1s - loss: 0.2601 - acc: 0.903 - ETA: 0s - loss: 0.2644 - acc: 0.900 - ETA: 0s - loss: 0.2600 - acc: 0.903 - 32s 26ms/sample - loss: 0.2598 - acc: 0.9033 - val_loss: 0.2625 - val_acc: 0.8883\n",
      "Epoch 3/10\n",
      "1230/1230 [==============================] - ETA: 25s - loss: 0.1692 - acc: 0.93 - ETA: 25s - loss: 0.1231 - acc: 0.96 - ETA: 26s - loss: 0.1980 - acc: 0.92 - ETA: 26s - loss: 0.1776 - acc: 0.93 - ETA: 26s - loss: 0.1923 - acc: 0.93 - ETA: 25s - loss: 0.2066 - acc: 0.92 - ETA: 25s - loss: 0.2012 - acc: 0.92 - ETA: 24s - loss: 0.2337 - acc: 0.90 - ETA: 23s - loss: 0.2298 - acc: 0.90 - ETA: 23s - loss: 0.2251 - acc: 0.90 - ETA: 22s - loss: 0.2246 - acc: 0.91 - ETA: 21s - loss: 0.2195 - acc: 0.91 - ETA: 20s - loss: 0.2192 - acc: 0.91 - ETA: 19s - loss: 0.2078 - acc: 0.91 - ETA: 18s - loss: 0.2227 - acc: 0.91 - ETA: 18s - loss: 0.2220 - acc: 0.91 - ETA: 17s - loss: 0.2339 - acc: 0.90 - ETA: 16s - loss: 0.2312 - acc: 0.90 - ETA: 15s - loss: 0.2295 - acc: 0.91 - ETA: 15s - loss: 0.2278 - acc: 0.91 - ETA: 14s - loss: 0.2278 - acc: 0.91 - ETA: 13s - loss: 0.2272 - acc: 0.91 - ETA: 12s - loss: 0.2289 - acc: 0.91 - ETA: 11s - loss: 0.2284 - acc: 0.91 - ETA: 11s - loss: 0.2280 - acc: 0.91 - ETA: 10s - loss: 0.2330 - acc: 0.91 - ETA: 9s - loss: 0.2349 - acc: 0.9109 - ETA: 8s - loss: 0.2333 - acc: 0.911 - ETA: 7s - loss: 0.2334 - acc: 0.910 - ETA: 6s - loss: 0.2321 - acc: 0.911 - ETA: 6s - loss: 0.2297 - acc: 0.913 - ETA: 5s - loss: 0.2322 - acc: 0.912 - ETA: 4s - loss: 0.2301 - acc: 0.913 - ETA: 3s - loss: 0.2308 - acc: 0.913 - ETA: 2s - loss: 0.2303 - acc: 0.913 - ETA: 1s - loss: 0.2291 - acc: 0.914 - ETA: 1s - loss: 0.2269 - acc: 0.915 - ETA: 0s - loss: 0.2252 - acc: 0.916 - 36s 30ms/sample - loss: 0.2265 - acc: 0.9154 - val_loss: 0.2410 - val_acc: 0.8996\n",
      "Epoch 4/10\n",
      "1230/1230 [==============================] - ETA: 27s - loss: 0.1396 - acc: 0.96 - ETA: 25s - loss: 0.1917 - acc: 0.93 - ETA: 25s - loss: 0.2227 - acc: 0.91 - ETA: 24s - loss: 0.2253 - acc: 0.91 - ETA: 23s - loss: 0.2235 - acc: 0.91 - ETA: 22s - loss: 0.2106 - acc: 0.92 - ETA: 21s - loss: 0.2219 - acc: 0.90 - ETA: 21s - loss: 0.2136 - acc: 0.91 - ETA: 20s - loss: 0.2116 - acc: 0.91 - ETA: 19s - loss: 0.2242 - acc: 0.90 - ETA: 19s - loss: 0.2265 - acc: 0.90 - ETA: 18s - loss: 0.2164 - acc: 0.91 - ETA: 17s - loss: 0.2058 - acc: 0.92 - ETA: 16s - loss: 0.2066 - acc: 0.91 - ETA: 16s - loss: 0.2111 - acc: 0.91 - ETA: 15s - loss: 0.2077 - acc: 0.91 - ETA: 14s - loss: 0.2073 - acc: 0.91 - ETA: 14s - loss: 0.2018 - acc: 0.91 - ETA: 13s - loss: 0.1979 - acc: 0.91 - ETA: 12s - loss: 0.1930 - acc: 0.92 - ETA: 12s - loss: 0.1984 - acc: 0.91 - ETA: 11s - loss: 0.1922 - acc: 0.92 - ETA: 10s - loss: 0.1893 - acc: 0.92 - ETA: 10s - loss: 0.1883 - acc: 0.92 - ETA: 9s - loss: 0.1844 - acc: 0.9275 - ETA: 8s - loss: 0.1915 - acc: 0.924 - ETA: 7s - loss: 0.1893 - acc: 0.924 - ETA: 7s - loss: 0.1901 - acc: 0.925 - ETA: 6s - loss: 0.1936 - acc: 0.923 - ETA: 5s - loss: 0.1909 - acc: 0.925 - ETA: 5s - loss: 0.1872 - acc: 0.926 - ETA: 4s - loss: 0.1868 - acc: 0.925 - ETA: 3s - loss: 0.1917 - acc: 0.923 - ETA: 3s - loss: 0.1938 - acc: 0.921 - ETA: 2s - loss: 0.1925 - acc: 0.923 - ETA: 1s - loss: 0.1904 - acc: 0.925 - ETA: 0s - loss: 0.1896 - acc: 0.925 - ETA: 0s - loss: 0.1896 - acc: 0.926 - 32s 26ms/sample - loss: 0.1888 - acc: 0.9260 - val_loss: 0.2027 - val_acc: 0.9186\n",
      "Epoch 5/10\n",
      "1230/1230 [==============================] - ETA: 25s - loss: 0.2572 - acc: 0.87 - ETA: 24s - loss: 0.2098 - acc: 0.92 - ETA: 24s - loss: 0.1970 - acc: 0.91 - ETA: 23s - loss: 0.1696 - acc: 0.92 - ETA: 23s - loss: 0.1519 - acc: 0.93 - ETA: 22s - loss: 0.1452 - acc: 0.93 - ETA: 21s - loss: 0.1275 - acc: 0.94 - ETA: 21s - loss: 0.1131 - acc: 0.95 - ETA: 20s - loss: 0.1130 - acc: 0.95 - ETA: 19s - loss: 0.1440 - acc: 0.94 - ETA: 19s - loss: 0.1464 - acc: 0.94 - ETA: 18s - loss: 0.1463 - acc: 0.94 - ETA: 17s - loss: 0.1449 - acc: 0.94 - ETA: 16s - loss: 0.1499 - acc: 0.93 - ETA: 16s - loss: 0.1500 - acc: 0.94 - ETA: 15s - loss: 0.1429 - acc: 0.94 - ETA: 14s - loss: 0.1691 - acc: 0.93 - ETA: 14s - loss: 0.1706 - acc: 0.93 - ETA: 13s - loss: 0.1753 - acc: 0.93 - ETA: 12s - loss: 0.1723 - acc: 0.93 - ETA: 12s - loss: 0.1746 - acc: 0.93 - ETA: 11s - loss: 0.1763 - acc: 0.93 - ETA: 10s - loss: 0.1727 - acc: 0.93 - ETA: 10s - loss: 0.1754 - acc: 0.93 - ETA: 9s - loss: 0.1711 - acc: 0.9400 - ETA: 8s - loss: 0.1706 - acc: 0.938 - ETA: 7s - loss: 0.1780 - acc: 0.934 - ETA: 7s - loss: 0.1756 - acc: 0.934 - ETA: 6s - loss: 0.1773 - acc: 0.932 - ETA: 5s - loss: 0.1771 - acc: 0.931 - ETA: 5s - loss: 0.1767 - acc: 0.930 - ETA: 4s - loss: 0.1774 - acc: 0.929 - ETA: 3s - loss: 0.1801 - acc: 0.928 - ETA: 3s - loss: 0.1801 - acc: 0.929 - ETA: 2s - loss: 0.1787 - acc: 0.929 - ETA: 1s - loss: 0.1763 - acc: 0.931 - ETA: 0s - loss: 0.1744 - acc: 0.932 - ETA: 0s - loss: 0.1754 - acc: 0.932 - 31s 26ms/sample - loss: 0.1747 - acc: 0.9325 - val_loss: 0.2209 - val_acc: 0.9148\n",
      "Epoch 6/10\n",
      "1230/1230 [==============================] - ETA: 27s - loss: 0.2090 - acc: 0.93 - ETA: 25s - loss: 0.2677 - acc: 0.89 - ETA: 24s - loss: 0.2639 - acc: 0.89 - ETA: 23s - loss: 0.2398 - acc: 0.90 - ETA: 23s - loss: 0.2007 - acc: 0.92 - ETA: 22s - loss: 0.1782 - acc: 0.93 - ETA: 21s - loss: 0.1668 - acc: 0.94 - ETA: 21s - loss: 0.1637 - acc: 0.94 - ETA: 20s - loss: 0.1621 - acc: 0.93 - ETA: 19s - loss: 0.1670 - acc: 0.93 - ETA: 19s - loss: 0.1735 - acc: 0.93 - ETA: 18s - loss: 0.1766 - acc: 0.92 - ETA: 17s - loss: 0.1693 - acc: 0.93 - ETA: 16s - loss: 0.1680 - acc: 0.93 - ETA: 16s - loss: 0.1670 - acc: 0.93 - ETA: 15s - loss: 0.1832 - acc: 0.93 - ETA: 14s - loss: 0.1933 - acc: 0.92 - ETA: 14s - loss: 0.1903 - acc: 0.92 - ETA: 13s - loss: 0.1821 - acc: 0.93 - ETA: 12s - loss: 0.1855 - acc: 0.93 - ETA: 12s - loss: 0.1818 - acc: 0.93 - ETA: 11s - loss: 0.1830 - acc: 0.93 - ETA: 10s - loss: 0.1852 - acc: 0.93 - ETA: 9s - loss: 0.1823 - acc: 0.9336 - ETA: 9s - loss: 0.1799 - acc: 0.935 - ETA: 8s - loss: 0.1834 - acc: 0.931 - ETA: 7s - loss: 0.1845 - acc: 0.930 - ETA: 7s - loss: 0.1817 - acc: 0.931 - ETA: 6s - loss: 0.1803 - acc: 0.933 - ETA: 5s - loss: 0.1770 - acc: 0.934 - ETA: 5s - loss: 0.1768 - acc: 0.933 - ETA: 4s - loss: 0.1735 - acc: 0.934 - ETA: 3s - loss: 0.1762 - acc: 0.932 - ETA: 3s - loss: 0.1737 - acc: 0.932 - ETA: 2s - loss: 0.1783 - acc: 0.930 - ETA: 1s - loss: 0.1802 - acc: 0.930 - ETA: 0s - loss: 0.1818 - acc: 0.929 - ETA: 0s - loss: 0.1807 - acc: 0.928 - 32s 26ms/sample - loss: 0.1789 - acc: 0.9293 - val_loss: 0.2343 - val_acc: 0.9110\n",
      "Epoch 7/10\n",
      "1230/1230 [==============================] - ETA: 27s - loss: 0.0856 - acc: 0.96 - ETA: 26s - loss: 0.1182 - acc: 0.95 - ETA: 25s - loss: 0.1731 - acc: 0.93 - ETA: 24s - loss: 0.1942 - acc: 0.92 - ETA: 23s - loss: 0.1668 - acc: 0.94 - ETA: 22s - loss: 0.1514 - acc: 0.94 - ETA: 22s - loss: 0.1381 - acc: 0.95 - ETA: 21s - loss: 0.1483 - acc: 0.94 - ETA: 20s - loss: 0.1425 - acc: 0.95 - ETA: 19s - loss: 0.1442 - acc: 0.95 - ETA: 19s - loss: 0.1496 - acc: 0.94 - ETA: 18s - loss: 0.1516 - acc: 0.94 - ETA: 17s - loss: 0.1447 - acc: 0.94 - ETA: 17s - loss: 0.1427 - acc: 0.94 - ETA: 16s - loss: 0.1403 - acc: 0.95 - ETA: 15s - loss: 0.1370 - acc: 0.95 - ETA: 15s - loss: 0.1353 - acc: 0.95 - ETA: 14s - loss: 0.1375 - acc: 0.94 - ETA: 13s - loss: 0.1345 - acc: 0.95 - ETA: 12s - loss: 0.1377 - acc: 0.94 - ETA: 12s - loss: 0.1455 - acc: 0.94 - ETA: 11s - loss: 0.1516 - acc: 0.94 - ETA: 10s - loss: 0.1497 - acc: 0.94 - ETA: 10s - loss: 0.1501 - acc: 0.94 - ETA: 9s - loss: 0.1515 - acc: 0.9438 - ETA: 9s - loss: 0.1499 - acc: 0.944 - ETA: 8s - loss: 0.1461 - acc: 0.946 - ETA: 7s - loss: 0.1445 - acc: 0.947 - ETA: 6s - loss: 0.1427 - acc: 0.949 - ETA: 6s - loss: 0.1427 - acc: 0.949 - ETA: 5s - loss: 0.1496 - acc: 0.945 - ETA: 4s - loss: 0.1496 - acc: 0.943 - ETA: 3s - loss: 0.1472 - acc: 0.944 - ETA: 3s - loss: 0.1495 - acc: 0.941 - ETA: 2s - loss: 0.1524 - acc: 0.939 - ETA: 1s - loss: 0.1554 - acc: 0.939 - ETA: 1s - loss: 0.1582 - acc: 0.937 - ETA: 0s - loss: 0.1571 - acc: 0.938 - 34s 27ms/sample - loss: 0.1561 - acc: 0.9390 - val_loss: 0.2216 - val_acc: 0.9186\n",
      "Epoch 8/10\n",
      "1230/1230 [==============================] - ETA: 29s - loss: 0.2498 - acc: 0.87 - ETA: 28s - loss: 0.2527 - acc: 0.87 - ETA: 27s - loss: 0.2122 - acc: 0.90 - ETA: 25s - loss: 0.1895 - acc: 0.92 - ETA: 24s - loss: 0.1811 - acc: 0.93 - ETA: 23s - loss: 0.1669 - acc: 0.93 - ETA: 23s - loss: 0.1595 - acc: 0.94 - ETA: 22s - loss: 0.1461 - acc: 0.94 - ETA: 21s - loss: 0.1415 - acc: 0.95 - ETA: 20s - loss: 0.1302 - acc: 0.95 - ETA: 20s - loss: 0.1473 - acc: 0.94 - ETA: 19s - loss: 0.1398 - acc: 0.95 - ETA: 18s - loss: 0.1307 - acc: 0.95 - ETA: 18s - loss: 0.1244 - acc: 0.95 - ETA: 17s - loss: 0.1188 - acc: 0.96 - ETA: 16s - loss: 0.1146 - acc: 0.96 - ETA: 15s - loss: 0.1285 - acc: 0.96 - ETA: 14s - loss: 0.1259 - acc: 0.96 - ETA: 14s - loss: 0.1282 - acc: 0.96 - ETA: 13s - loss: 0.1249 - acc: 0.96 - ETA: 12s - loss: 0.1273 - acc: 0.96 - ETA: 12s - loss: 0.1268 - acc: 0.96 - ETA: 11s - loss: 0.1319 - acc: 0.95 - ETA: 10s - loss: 0.1285 - acc: 0.95 - ETA: 9s - loss: 0.1365 - acc: 0.9525 - ETA: 9s - loss: 0.1408 - acc: 0.950 - ETA: 8s - loss: 0.1430 - acc: 0.950 - ETA: 7s - loss: 0.1446 - acc: 0.948 - ETA: 6s - loss: 0.1475 - acc: 0.947 - ETA: 6s - loss: 0.1445 - acc: 0.949 - ETA: 5s - loss: 0.1446 - acc: 0.948 - ETA: 4s - loss: 0.1498 - acc: 0.945 - ETA: 3s - loss: 0.1492 - acc: 0.945 - ETA: 3s - loss: 0.1477 - acc: 0.945 - ETA: 2s - loss: 0.1495 - acc: 0.944 - ETA: 1s - loss: 0.1506 - acc: 0.943 - ETA: 1s - loss: 0.1500 - acc: 0.944 - ETA: 0s - loss: 0.1495 - acc: 0.943 - 33s 27ms/sample - loss: 0.1503 - acc: 0.9423 - val_loss: 0.1909 - val_acc: 0.9223\n",
      "Epoch 9/10\n",
      "1230/1230 [==============================] - ETA: 29s - loss: 0.0897 - acc: 1.00 - ETA: 27s - loss: 0.0700 - acc: 1.00 - ETA: 26s - loss: 0.0949 - acc: 0.97 - ETA: 25s - loss: 0.0962 - acc: 0.97 - ETA: 24s - loss: 0.0978 - acc: 0.98 - ETA: 23s - loss: 0.1018 - acc: 0.97 - ETA: 22s - loss: 0.1009 - acc: 0.97 - ETA: 21s - loss: 0.0941 - acc: 0.97 - ETA: 21s - loss: 0.1124 - acc: 0.96 - ETA: 20s - loss: 0.1056 - acc: 0.96 - ETA: 19s - loss: 0.1047 - acc: 0.96 - ETA: 18s - loss: 0.1110 - acc: 0.96 - ETA: 18s - loss: 0.1120 - acc: 0.96 - ETA: 17s - loss: 0.1191 - acc: 0.95 - ETA: 16s - loss: 0.1182 - acc: 0.96 - ETA: 15s - loss: 0.1212 - acc: 0.95 - ETA: 15s - loss: 0.1332 - acc: 0.95 - ETA: 14s - loss: 0.1301 - acc: 0.95 - ETA: 13s - loss: 0.1261 - acc: 0.95 - ETA: 13s - loss: 0.1273 - acc: 0.95 - ETA: 12s - loss: 0.1341 - acc: 0.95 - ETA: 11s - loss: 0.1362 - acc: 0.95 - ETA: 10s - loss: 0.1373 - acc: 0.95 - ETA: 10s - loss: 0.1351 - acc: 0.95 - ETA: 9s - loss: 0.1346 - acc: 0.9538 - ETA: 8s - loss: 0.1321 - acc: 0.955 - ETA: 8s - loss: 0.1353 - acc: 0.953 - ETA: 7s - loss: 0.1353 - acc: 0.953 - ETA: 6s - loss: 0.1351 - acc: 0.953 - ETA: 5s - loss: 0.1395 - acc: 0.951 - ETA: 5s - loss: 0.1391 - acc: 0.950 - ETA: 4s - loss: 0.1376 - acc: 0.951 - ETA: 3s - loss: 0.1377 - acc: 0.950 - ETA: 3s - loss: 0.1361 - acc: 0.952 - ETA: 2s - loss: 0.1335 - acc: 0.953 - ETA: 1s - loss: 0.1355 - acc: 0.953 - ETA: 1s - loss: 0.1353 - acc: 0.952 - ETA: 0s - loss: 0.1379 - acc: 0.951 - 32s 26ms/sample - loss: 0.1381 - acc: 0.9512 - val_loss: 0.2182 - val_acc: 0.9167\n",
      "Epoch 10/10\n",
      "1230/1230 [==============================] - ETA: 26s - loss: 0.0778 - acc: 0.96 - ETA: 25s - loss: 0.0924 - acc: 0.96 - ETA: 24s - loss: 0.0888 - acc: 0.96 - ETA: 24s - loss: 0.0929 - acc: 0.96 - ETA: 23s - loss: 0.0950 - acc: 0.97 - ETA: 22s - loss: 0.0877 - acc: 0.97 - ETA: 21s - loss: 0.0902 - acc: 0.97 - ETA: 21s - loss: 0.1001 - acc: 0.97 - ETA: 20s - loss: 0.1162 - acc: 0.96 - ETA: 19s - loss: 0.1208 - acc: 0.96 - ETA: 19s - loss: 0.1278 - acc: 0.95 - ETA: 18s - loss: 0.1234 - acc: 0.95 - ETA: 17s - loss: 0.1179 - acc: 0.96 - ETA: 16s - loss: 0.1175 - acc: 0.96 - ETA: 16s - loss: 0.1200 - acc: 0.96 - ETA: 15s - loss: 0.1163 - acc: 0.96 - ETA: 14s - loss: 0.1201 - acc: 0.96 - ETA: 14s - loss: 0.1189 - acc: 0.96 - ETA: 13s - loss: 0.1139 - acc: 0.96 - ETA: 12s - loss: 0.1169 - acc: 0.96 - ETA: 12s - loss: 0.1223 - acc: 0.95 - ETA: 11s - loss: 0.1215 - acc: 0.96 - ETA: 10s - loss: 0.1241 - acc: 0.95 - ETA: 10s - loss: 0.1285 - acc: 0.95 - ETA: 9s - loss: 0.1270 - acc: 0.9575 - ETA: 8s - loss: 0.1360 - acc: 0.951 - ETA: 7s - loss: 0.1347 - acc: 0.952 - ETA: 7s - loss: 0.1336 - acc: 0.952 - ETA: 6s - loss: 0.1351 - acc: 0.951 - ETA: 5s - loss: 0.1336 - acc: 0.952 - ETA: 5s - loss: 0.1310 - acc: 0.952 - ETA: 4s - loss: 0.1312 - acc: 0.953 - ETA: 3s - loss: 0.1304 - acc: 0.953 - ETA: 3s - loss: 0.1289 - acc: 0.954 - ETA: 2s - loss: 0.1338 - acc: 0.952 - ETA: 1s - loss: 0.1328 - acc: 0.953 - ETA: 1s - loss: 0.1302 - acc: 0.954 - ETA: 0s - loss: 0.1298 - acc: 0.954 - 32s 26ms/sample - loss: 0.1300 - acc: 0.9553 - val_loss: 0.1923 - val_acc: 0.9242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ce2193c6d8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4 = Sequential()\n",
    "\n",
    "NAME = \"Pokemon-model4-{}\".format(int(time.time()))\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))\n",
    "\n",
    "\n",
    "model4.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
    "model4.add(Activation('softsign'))\n",
    "model4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model4.add(Conv2D(256, (3, 3)))\n",
    "model4.add(Activation('softsign'))\n",
    "model4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model4.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "\n",
    "model4.add(Dense(64))\n",
    "model4.add(Activation('softsign'))\n",
    "\n",
    "model4.add(Dense(1))\n",
    "model4.add(Activation('sigmoid'))\n",
    "\n",
    "model4.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model4.fit(X, y, batch_size=32, epochs=10, validation_split=0.3, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow out put for accuracy\n",
    "\n",
    "<img src=\"Images/TensorBoard/epoch_acc_model4.svg?sanitize=true\" width=\"500\">\n",
    "\n",
    "\n",
    "Thus, seems model 4 as well gives a good prediction accuracy for our model running for 10 Epoch on `binary_crossentropy`. As it goes, it plateaus with the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5 - CNN - Softsign - <span class=\"mark\">QuadraticCost</span> - 10 Epoch - <span class=\"mark\">Adam</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Cost Function:\n",
    "\n",
    "If there is diminishing return to the variable factor the cost function becomes quadratic. With this we even performed a optimization called `Adam` to check with the performance\n",
    "\n",
    "###### `Adam` is different to classical stochastic gradient descent.\n",
    "\n",
    "Stochastic gradient descent maintains a single learning rate (termed alpha) for all weight updates and the learning rate does not change during training.\n",
    "\n",
    "A learning rate is maintained for each network weight (parameter) and separately adapted as learning unfolds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\prabh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1230 samples, validate on 528 samples\n",
      "Epoch 1/10\n",
      "1230/1230 [==============================] - ETA: 36s - loss: 0.2459 - acc: 0.68 - ETA: 30s - loss: 0.3311 - acc: 0.59 - ETA: 27s - loss: 0.4141 - acc: 0.53 - ETA: 26s - loss: 0.4264 - acc: 0.53 - ETA: 24s - loss: 0.4293 - acc: 0.53 - ETA: 24s - loss: 0.3835 - acc: 0.58 - ETA: 23s - loss: 0.3480 - acc: 0.62 - ETA: 22s - loss: 0.3198 - acc: 0.64 - ETA: 21s - loss: 0.2879 - acc: 0.68 - ETA: 20s - loss: 0.2716 - acc: 0.70 - ETA: 19s - loss: 0.2581 - acc: 0.71 - ETA: 18s - loss: 0.2440 - acc: 0.73 - ETA: 18s - loss: 0.2351 - acc: 0.74 - ETA: 17s - loss: 0.2265 - acc: 0.75 - ETA: 16s - loss: 0.2175 - acc: 0.76 - ETA: 15s - loss: 0.2083 - acc: 0.76 - ETA: 15s - loss: 0.1999 - acc: 0.77 - ETA: 14s - loss: 0.1949 - acc: 0.78 - ETA: 13s - loss: 0.1846 - acc: 0.79 - ETA: 13s - loss: 0.1863 - acc: 0.79 - ETA: 12s - loss: 0.1831 - acc: 0.79 - ETA: 11s - loss: 0.1811 - acc: 0.80 - ETA: 10s - loss: 0.1747 - acc: 0.80 - ETA: 10s - loss: 0.1706 - acc: 0.81 - ETA: 9s - loss: 0.1661 - acc: 0.8175 - ETA: 8s - loss: 0.1621 - acc: 0.820 - ETA: 8s - loss: 0.1583 - acc: 0.825 - ETA: 7s - loss: 0.1536 - acc: 0.830 - ETA: 6s - loss: 0.1535 - acc: 0.829 - ETA: 5s - loss: 0.1500 - acc: 0.833 - ETA: 5s - loss: 0.1471 - acc: 0.835 - ETA: 4s - loss: 0.1447 - acc: 0.837 - ETA: 3s - loss: 0.1407 - acc: 0.842 - ETA: 3s - loss: 0.1398 - acc: 0.842 - ETA: 2s - loss: 0.1384 - acc: 0.843 - ETA: 1s - loss: 0.1362 - acc: 0.845 - ETA: 1s - loss: 0.1342 - acc: 0.847 - ETA: 0s - loss: 0.1334 - acc: 0.847 - 32s 26ms/sample - loss: 0.1338 - acc: 0.8463 - val_loss: 0.0782 - val_acc: 0.8920\n",
      "Epoch 2/10\n",
      "1230/1230 [==============================] - ETA: 26s - loss: 0.0567 - acc: 0.93 - ETA: 25s - loss: 0.0708 - acc: 0.92 - ETA: 24s - loss: 0.0780 - acc: 0.89 - ETA: 24s - loss: 0.0704 - acc: 0.89 - ETA: 23s - loss: 0.0761 - acc: 0.88 - ETA: 22s - loss: 0.0786 - acc: 0.89 - ETA: 21s - loss: 0.0873 - acc: 0.87 - ETA: 21s - loss: 0.0780 - acc: 0.89 - ETA: 20s - loss: 0.0796 - acc: 0.89 - ETA: 19s - loss: 0.0766 - acc: 0.89 - ETA: 19s - loss: 0.0773 - acc: 0.89 - ETA: 18s - loss: 0.0747 - acc: 0.90 - ETA: 17s - loss: 0.0714 - acc: 0.90 - ETA: 17s - loss: 0.0759 - acc: 0.90 - ETA: 16s - loss: 0.0715 - acc: 0.90 - ETA: 15s - loss: 0.0734 - acc: 0.90 - ETA: 14s - loss: 0.0712 - acc: 0.90 - ETA: 14s - loss: 0.0719 - acc: 0.90 - ETA: 13s - loss: 0.0708 - acc: 0.90 - ETA: 12s - loss: 0.0693 - acc: 0.90 - ETA: 12s - loss: 0.0687 - acc: 0.90 - ETA: 11s - loss: 0.0696 - acc: 0.90 - ETA: 10s - loss: 0.0688 - acc: 0.90 - ETA: 9s - loss: 0.0684 - acc: 0.9062 - ETA: 9s - loss: 0.0691 - acc: 0.903 - ETA: 8s - loss: 0.0690 - acc: 0.901 - ETA: 7s - loss: 0.0698 - acc: 0.900 - ETA: 7s - loss: 0.0679 - acc: 0.902 - ETA: 6s - loss: 0.0672 - acc: 0.905 - ETA: 5s - loss: 0.0682 - acc: 0.903 - ETA: 5s - loss: 0.0688 - acc: 0.902 - ETA: 4s - loss: 0.0689 - acc: 0.900 - ETA: 3s - loss: 0.0690 - acc: 0.899 - ETA: 3s - loss: 0.0682 - acc: 0.901 - ETA: 2s - loss: 0.0672 - acc: 0.903 - ETA: 1s - loss: 0.0661 - acc: 0.905 - ETA: 0s - loss: 0.0666 - acc: 0.905 - ETA: 0s - loss: 0.0666 - acc: 0.905 - 32s 26ms/sample - loss: 0.0674 - acc: 0.9041 - val_loss: 0.0714 - val_acc: 0.8977\n",
      "Epoch 3/10\n",
      "1230/1230 [==============================] - ETA: 27s - loss: 0.0398 - acc: 0.93 - ETA: 26s - loss: 0.0556 - acc: 0.93 - ETA: 25s - loss: 0.0543 - acc: 0.94 - ETA: 24s - loss: 0.0467 - acc: 0.95 - ETA: 23s - loss: 0.0452 - acc: 0.95 - ETA: 22s - loss: 0.0523 - acc: 0.93 - ETA: 22s - loss: 0.0554 - acc: 0.93 - ETA: 21s - loss: 0.0657 - acc: 0.92 - ETA: 20s - loss: 0.0709 - acc: 0.91 - ETA: 20s - loss: 0.0670 - acc: 0.91 - ETA: 19s - loss: 0.0713 - acc: 0.91 - ETA: 18s - loss: 0.0718 - acc: 0.90 - ETA: 17s - loss: 0.0714 - acc: 0.91 - ETA: 17s - loss: 0.0736 - acc: 0.90 - ETA: 16s - loss: 0.0739 - acc: 0.91 - ETA: 15s - loss: 0.0736 - acc: 0.90 - ETA: 15s - loss: 0.0715 - acc: 0.90 - ETA: 14s - loss: 0.0717 - acc: 0.90 - ETA: 13s - loss: 0.0727 - acc: 0.90 - ETA: 12s - loss: 0.0745 - acc: 0.90 - ETA: 12s - loss: 0.0724 - acc: 0.90 - ETA: 11s - loss: 0.0757 - acc: 0.90 - ETA: 10s - loss: 0.0753 - acc: 0.90 - ETA: 10s - loss: 0.0733 - acc: 0.90 - ETA: 9s - loss: 0.0705 - acc: 0.9100 - ETA: 8s - loss: 0.0689 - acc: 0.911 - ETA: 8s - loss: 0.0701 - acc: 0.908 - ETA: 7s - loss: 0.0712 - acc: 0.907 - ETA: 6s - loss: 0.0703 - acc: 0.908 - ETA: 5s - loss: 0.0705 - acc: 0.907 - ETA: 5s - loss: 0.0700 - acc: 0.907 - ETA: 4s - loss: 0.0682 - acc: 0.910 - ETA: 3s - loss: 0.0682 - acc: 0.910 - ETA: 3s - loss: 0.0681 - acc: 0.910 - ETA: 2s - loss: 0.0677 - acc: 0.912 - ETA: 1s - loss: 0.0675 - acc: 0.913 - ETA: 1s - loss: 0.0674 - acc: 0.913 - ETA: 0s - loss: 0.0688 - acc: 0.910 - 32s 26ms/sample - loss: 0.0686 - acc: 0.9106 - val_loss: 0.0689 - val_acc: 0.9186\n",
      "Epoch 4/10\n",
      "1230/1230 [==============================] - ETA: 26s - loss: 0.0301 - acc: 0.96 - ETA: 25s - loss: 0.0248 - acc: 0.96 - ETA: 24s - loss: 0.0263 - acc: 0.95 - ETA: 24s - loss: 0.0320 - acc: 0.95 - ETA: 23s - loss: 0.0368 - acc: 0.95 - ETA: 22s - loss: 0.0321 - acc: 0.95 - ETA: 21s - loss: 0.0311 - acc: 0.95 - ETA: 21s - loss: 0.0358 - acc: 0.95 - ETA: 20s - loss: 0.0370 - acc: 0.95 - ETA: 19s - loss: 0.0396 - acc: 0.94 - ETA: 19s - loss: 0.0387 - acc: 0.94 - ETA: 18s - loss: 0.0418 - acc: 0.93 - ETA: 17s - loss: 0.0430 - acc: 0.93 - ETA: 16s - loss: 0.0458 - acc: 0.93 - ETA: 16s - loss: 0.0450 - acc: 0.93 - ETA: 15s - loss: 0.0478 - acc: 0.92 - ETA: 14s - loss: 0.0477 - acc: 0.93 - ETA: 14s - loss: 0.0473 - acc: 0.93 - ETA: 13s - loss: 0.0467 - acc: 0.93 - ETA: 12s - loss: 0.0453 - acc: 0.93 - ETA: 12s - loss: 0.0442 - acc: 0.93 - ETA: 11s - loss: 0.0463 - acc: 0.93 - ETA: 10s - loss: 0.0461 - acc: 0.93 - ETA: 9s - loss: 0.0459 - acc: 0.9349 - ETA: 9s - loss: 0.0482 - acc: 0.931 - ETA: 8s - loss: 0.0484 - acc: 0.931 - ETA: 7s - loss: 0.0489 - acc: 0.931 - ETA: 7s - loss: 0.0502 - acc: 0.931 - ETA: 6s - loss: 0.0559 - acc: 0.926 - ETA: 5s - loss: 0.0597 - acc: 0.922 - ETA: 5s - loss: 0.0581 - acc: 0.925 - ETA: 4s - loss: 0.0573 - acc: 0.926 - ETA: 3s - loss: 0.0575 - acc: 0.927 - ETA: 3s - loss: 0.0614 - acc: 0.923 - ETA: 2s - loss: 0.0614 - acc: 0.924 - ETA: 1s - loss: 0.0636 - acc: 0.921 - ETA: 0s - loss: 0.0653 - acc: 0.920 - ETA: 0s - loss: 0.0693 - acc: 0.916 - 32s 26ms/sample - loss: 0.0685 - acc: 0.9179 - val_loss: 0.1155 - val_acc: 0.8845\n",
      "Epoch 5/10\n",
      "1230/1230 [==============================] - ETA: 27s - loss: 0.0312 - acc: 0.96 - ETA: 25s - loss: 0.0781 - acc: 0.92 - ETA: 25s - loss: 0.0729 - acc: 0.92 - ETA: 24s - loss: 0.0703 - acc: 0.92 - ETA: 23s - loss: 0.0812 - acc: 0.91 - ETA: 22s - loss: 0.0885 - acc: 0.91 - ETA: 21s - loss: 0.0758 - acc: 0.92 - ETA: 21s - loss: 0.0820 - acc: 0.91 - ETA: 20s - loss: 0.0937 - acc: 0.90 - ETA: 19s - loss: 0.0937 - acc: 0.90 - ETA: 19s - loss: 0.0965 - acc: 0.90 - ETA: 18s - loss: 0.0963 - acc: 0.90 - ETA: 17s - loss: 0.0938 - acc: 0.90 - ETA: 16s - loss: 0.0961 - acc: 0.90 - ETA: 16s - loss: 0.0897 - acc: 0.91 - ETA: 15s - loss: 0.0919 - acc: 0.90 - ETA: 14s - loss: 0.0883 - acc: 0.91 - ETA: 14s - loss: 0.0914 - acc: 0.90 - ETA: 13s - loss: 0.0899 - acc: 0.90 - ETA: 12s - loss: 0.0880 - acc: 0.91 - ETA: 12s - loss: 0.0867 - acc: 0.91 - ETA: 11s - loss: 0.0871 - acc: 0.91 - ETA: 10s - loss: 0.0860 - acc: 0.91 - ETA: 9s - loss: 0.0895 - acc: 0.9076 - ETA: 9s - loss: 0.0895 - acc: 0.907 - ETA: 8s - loss: 0.0918 - acc: 0.903 - ETA: 7s - loss: 0.0936 - acc: 0.901 - ETA: 7s - loss: 0.0937 - acc: 0.900 - ETA: 6s - loss: 0.0925 - acc: 0.900 - ETA: 5s - loss: 0.0918 - acc: 0.901 - ETA: 5s - loss: 0.0912 - acc: 0.901 - ETA: 4s - loss: 0.0896 - acc: 0.903 - ETA: 3s - loss: 0.0882 - acc: 0.904 - ETA: 3s - loss: 0.0875 - acc: 0.903 - ETA: 2s - loss: 0.0869 - acc: 0.903 - ETA: 1s - loss: 0.0857 - acc: 0.904 - ETA: 0s - loss: 0.0853 - acc: 0.904 - ETA: 0s - loss: 0.0856 - acc: 0.903 - 32s 26ms/sample - loss: 0.0850 - acc: 0.9033 - val_loss: 0.0761 - val_acc: 0.9091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "1230/1230 [==============================] - ETA: 26s - loss: 0.0667 - acc: 0.93 - ETA: 25s - loss: 0.0612 - acc: 0.92 - ETA: 24s - loss: 0.0429 - acc: 0.94 - ETA: 23s - loss: 0.0570 - acc: 0.92 - ETA: 23s - loss: 0.0516 - acc: 0.93 - ETA: 22s - loss: 0.0577 - acc: 0.92 - ETA: 21s - loss: 0.0662 - acc: 0.91 - ETA: 21s - loss: 0.0583 - acc: 0.92 - ETA: 20s - loss: 0.0529 - acc: 0.93 - ETA: 19s - loss: 0.0608 - acc: 0.92 - ETA: 19s - loss: 0.0627 - acc: 0.92 - ETA: 18s - loss: 0.0656 - acc: 0.91 - ETA: 17s - loss: 0.0694 - acc: 0.91 - ETA: 16s - loss: 0.0688 - acc: 0.91 - ETA: 16s - loss: 0.0665 - acc: 0.91 - ETA: 15s - loss: 0.0634 - acc: 0.91 - ETA: 14s - loss: 0.0644 - acc: 0.91 - ETA: 14s - loss: 0.0627 - acc: 0.92 - ETA: 13s - loss: 0.0646 - acc: 0.91 - ETA: 12s - loss: 0.0661 - acc: 0.91 - ETA: 12s - loss: 0.0645 - acc: 0.91 - ETA: 11s - loss: 0.0649 - acc: 0.91 - ETA: 10s - loss: 0.0649 - acc: 0.91 - ETA: 10s - loss: 0.0635 - acc: 0.91 - ETA: 9s - loss: 0.0612 - acc: 0.9212 - ETA: 8s - loss: 0.0603 - acc: 0.921 - ETA: 7s - loss: 0.0599 - acc: 0.922 - ETA: 7s - loss: 0.0613 - acc: 0.919 - ETA: 6s - loss: 0.0622 - acc: 0.918 - ETA: 5s - loss: 0.0619 - acc: 0.918 - ETA: 5s - loss: 0.0620 - acc: 0.919 - ETA: 4s - loss: 0.0624 - acc: 0.918 - ETA: 3s - loss: 0.0608 - acc: 0.921 - ETA: 3s - loss: 0.0602 - acc: 0.921 - ETA: 2s - loss: 0.0608 - acc: 0.921 - ETA: 1s - loss: 0.0612 - acc: 0.920 - ETA: 0s - loss: 0.0628 - acc: 0.918 - ETA: 0s - loss: 0.0625 - acc: 0.919 - 32s 26ms/sample - loss: 0.0619 - acc: 0.9203 - val_loss: 0.0633 - val_acc: 0.9167\n",
      "Epoch 7/10\n",
      "1230/1230 [==============================] - ETA: 25s - loss: 0.0818 - acc: 0.87 - ETA: 25s - loss: 0.0774 - acc: 0.89 - ETA: 24s - loss: 0.0666 - acc: 0.90 - ETA: 23s - loss: 0.0608 - acc: 0.91 - ETA: 23s - loss: 0.0523 - acc: 0.93 - ETA: 22s - loss: 0.0513 - acc: 0.93 - ETA: 21s - loss: 0.0554 - acc: 0.92 - ETA: 20s - loss: 0.0513 - acc: 0.92 - ETA: 20s - loss: 0.0493 - acc: 0.93 - ETA: 19s - loss: 0.0487 - acc: 0.93 - ETA: 18s - loss: 0.0477 - acc: 0.93 - ETA: 18s - loss: 0.0492 - acc: 0.93 - ETA: 17s - loss: 0.0486 - acc: 0.93 - ETA: 16s - loss: 0.0460 - acc: 0.93 - ETA: 16s - loss: 0.0491 - acc: 0.93 - ETA: 15s - loss: 0.0496 - acc: 0.93 - ETA: 14s - loss: 0.0507 - acc: 0.93 - ETA: 14s - loss: 0.0525 - acc: 0.93 - ETA: 13s - loss: 0.0515 - acc: 0.93 - ETA: 12s - loss: 0.0510 - acc: 0.93 - ETA: 12s - loss: 0.0508 - acc: 0.93 - ETA: 11s - loss: 0.0521 - acc: 0.93 - ETA: 10s - loss: 0.0515 - acc: 0.93 - ETA: 9s - loss: 0.0529 - acc: 0.9336 - ETA: 9s - loss: 0.0530 - acc: 0.933 - ETA: 8s - loss: 0.0520 - acc: 0.936 - ETA: 7s - loss: 0.0514 - acc: 0.936 - ETA: 7s - loss: 0.0504 - acc: 0.937 - ETA: 6s - loss: 0.0506 - acc: 0.937 - ETA: 5s - loss: 0.0503 - acc: 0.937 - ETA: 5s - loss: 0.0499 - acc: 0.937 - ETA: 4s - loss: 0.0496 - acc: 0.938 - ETA: 3s - loss: 0.0504 - acc: 0.937 - ETA: 3s - loss: 0.0519 - acc: 0.934 - ETA: 2s - loss: 0.0511 - acc: 0.936 - ETA: 1s - loss: 0.0523 - acc: 0.935 - ETA: 0s - loss: 0.0517 - acc: 0.936 - ETA: 0s - loss: 0.0520 - acc: 0.936 - 32s 26ms/sample - loss: 0.0519 - acc: 0.9374 - val_loss: 0.0719 - val_acc: 0.8977\n",
      "Epoch 8/10\n",
      "1230/1230 [==============================] - ETA: 26s - loss: 0.0757 - acc: 0.87 - ETA: 25s - loss: 0.0842 - acc: 0.87 - ETA: 24s - loss: 0.0686 - acc: 0.90 - ETA: 23s - loss: 0.0710 - acc: 0.90 - ETA: 23s - loss: 0.0784 - acc: 0.89 - ETA: 22s - loss: 0.0835 - acc: 0.88 - ETA: 21s - loss: 0.0725 - acc: 0.90 - ETA: 20s - loss: 0.0659 - acc: 0.91 - ETA: 20s - loss: 0.0650 - acc: 0.91 - ETA: 19s - loss: 0.0666 - acc: 0.90 - ETA: 18s - loss: 0.0641 - acc: 0.91 - ETA: 18s - loss: 0.0637 - acc: 0.91 - ETA: 17s - loss: 0.0604 - acc: 0.92 - ETA: 16s - loss: 0.0603 - acc: 0.92 - ETA: 16s - loss: 0.0575 - acc: 0.92 - ETA: 15s - loss: 0.0561 - acc: 0.92 - ETA: 14s - loss: 0.0562 - acc: 0.92 - ETA: 14s - loss: 0.0597 - acc: 0.92 - ETA: 13s - loss: 0.0571 - acc: 0.92 - ETA: 12s - loss: 0.0574 - acc: 0.92 - ETA: 12s - loss: 0.0557 - acc: 0.92 - ETA: 11s - loss: 0.0583 - acc: 0.92 - ETA: 10s - loss: 0.0585 - acc: 0.92 - ETA: 10s - loss: 0.0602 - acc: 0.92 - ETA: 9s - loss: 0.0587 - acc: 0.9237 - ETA: 8s - loss: 0.0579 - acc: 0.924 - ETA: 8s - loss: 0.0560 - acc: 0.927 - ETA: 7s - loss: 0.0558 - acc: 0.926 - ETA: 6s - loss: 0.0572 - acc: 0.924 - ETA: 6s - loss: 0.0558 - acc: 0.926 - ETA: 5s - loss: 0.0542 - acc: 0.928 - ETA: 4s - loss: 0.0554 - acc: 0.926 - ETA: 4s - loss: 0.0548 - acc: 0.927 - ETA: 3s - loss: 0.0542 - acc: 0.927 - ETA: 2s - loss: 0.0551 - acc: 0.925 - ETA: 1s - loss: 0.0552 - acc: 0.926 - ETA: 1s - loss: 0.0552 - acc: 0.926 - ETA: 0s - loss: 0.0542 - acc: 0.928 - 35s 28ms/sample - loss: 0.0538 - acc: 0.9293 - val_loss: 0.0614 - val_acc: 0.9167\n",
      "Epoch 9/10\n",
      "1230/1230 [==============================] - ETA: 29s - loss: 0.0101 - acc: 1.00 - ETA: 29s - loss: 0.0065 - acc: 1.00 - ETA: 28s - loss: 0.0199 - acc: 0.97 - ETA: 27s - loss: 0.0273 - acc: 0.96 - ETA: 26s - loss: 0.0238 - acc: 0.96 - ETA: 26s - loss: 0.0316 - acc: 0.95 - ETA: 25s - loss: 0.0365 - acc: 0.95 - ETA: 24s - loss: 0.0433 - acc: 0.94 - ETA: 23s - loss: 0.0450 - acc: 0.93 - ETA: 22s - loss: 0.0480 - acc: 0.93 - ETA: 22s - loss: 0.0464 - acc: 0.93 - ETA: 21s - loss: 0.0447 - acc: 0.94 - ETA: 20s - loss: 0.0459 - acc: 0.93 - ETA: 19s - loss: 0.0477 - acc: 0.93 - ETA: 18s - loss: 0.0463 - acc: 0.93 - ETA: 18s - loss: 0.0464 - acc: 0.93 - ETA: 17s - loss: 0.0446 - acc: 0.94 - ETA: 16s - loss: 0.0445 - acc: 0.94 - ETA: 15s - loss: 0.0458 - acc: 0.93 - ETA: 14s - loss: 0.0443 - acc: 0.93 - ETA: 13s - loss: 0.0429 - acc: 0.94 - ETA: 13s - loss: 0.0444 - acc: 0.93 - ETA: 12s - loss: 0.0435 - acc: 0.94 - ETA: 11s - loss: 0.0429 - acc: 0.94 - ETA: 10s - loss: 0.0426 - acc: 0.94 - ETA: 9s - loss: 0.0430 - acc: 0.9447 - ETA: 8s - loss: 0.0446 - acc: 0.943 - ETA: 8s - loss: 0.0457 - acc: 0.942 - ETA: 7s - loss: 0.0473 - acc: 0.938 - ETA: 6s - loss: 0.0479 - acc: 0.937 - ETA: 5s - loss: 0.0476 - acc: 0.937 - ETA: 5s - loss: 0.0470 - acc: 0.938 - ETA: 4s - loss: 0.0461 - acc: 0.939 - ETA: 3s - loss: 0.0448 - acc: 0.941 - ETA: 2s - loss: 0.0436 - acc: 0.942 - ETA: 1s - loss: 0.0443 - acc: 0.941 - ETA: 1s - loss: 0.0442 - acc: 0.942 - ETA: 0s - loss: 0.0438 - acc: 0.943 - 35s 28ms/sample - loss: 0.0433 - acc: 0.9439 - val_loss: 0.0627 - val_acc: 0.9129\n",
      "Epoch 10/10\n",
      "1230/1230 [==============================] - ETA: 29s - loss: 0.0390 - acc: 0.93 - ETA: 27s - loss: 0.0404 - acc: 0.93 - ETA: 26s - loss: 0.0437 - acc: 0.93 - ETA: 25s - loss: 0.0498 - acc: 0.92 - ETA: 25s - loss: 0.0550 - acc: 0.91 - ETA: 24s - loss: 0.0549 - acc: 0.91 - ETA: 23s - loss: 0.0497 - acc: 0.92 - ETA: 22s - loss: 0.0473 - acc: 0.92 - ETA: 21s - loss: 0.0476 - acc: 0.93 - ETA: 20s - loss: 0.0485 - acc: 0.93 - ETA: 20s - loss: 0.0534 - acc: 0.92 - ETA: 19s - loss: 0.0549 - acc: 0.92 - ETA: 18s - loss: 0.0551 - acc: 0.92 - ETA: 17s - loss: 0.0583 - acc: 0.92 - ETA: 16s - loss: 0.0602 - acc: 0.91 - ETA: 16s - loss: 0.0600 - acc: 0.91 - ETA: 15s - loss: 0.0580 - acc: 0.92 - ETA: 14s - loss: 0.0553 - acc: 0.92 - ETA: 14s - loss: 0.0569 - acc: 0.92 - ETA: 13s - loss: 0.0568 - acc: 0.92 - ETA: 12s - loss: 0.0579 - acc: 0.92 - ETA: 11s - loss: 0.0565 - acc: 0.92 - ETA: 11s - loss: 0.0569 - acc: 0.92 - ETA: 10s - loss: 0.0580 - acc: 0.92 - ETA: 9s - loss: 0.0587 - acc: 0.9237 - ETA: 8s - loss: 0.0569 - acc: 0.926 - ETA: 8s - loss: 0.0563 - acc: 0.927 - ETA: 7s - loss: 0.0564 - acc: 0.926 - ETA: 6s - loss: 0.0549 - acc: 0.928 - ETA: 6s - loss: 0.0555 - acc: 0.927 - ETA: 5s - loss: 0.0544 - acc: 0.928 - ETA: 4s - loss: 0.0549 - acc: 0.926 - ETA: 3s - loss: 0.0541 - acc: 0.928 - ETA: 3s - loss: 0.0529 - acc: 0.930 - ETA: 2s - loss: 0.0528 - acc: 0.930 - ETA: 1s - loss: 0.0516 - acc: 0.932 - ETA: 1s - loss: 0.0519 - acc: 0.933 - ETA: 0s - loss: 0.0507 - acc: 0.935 - 33s 27ms/sample - loss: 0.0504 - acc: 0.9358 - val_loss: 0.0620 - val_acc: 0.9148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ce4337eeb8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5 = Sequential()\n",
    "\n",
    "NAME = \"Pokemon-model5-{}\".format(int(time.time()))\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))\n",
    "\n",
    "\n",
    "model5.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
    "model5.add(Activation('softsign'))\n",
    "model5.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model5.add(Conv2D(256, (3, 3)))\n",
    "model5.add(Activation('softsign'))\n",
    "model5.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model5.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "\n",
    "model5.add(Dense(64))\n",
    "model5.add(Activation('softsign'))\n",
    "\n",
    "model5.add(Dense(1))\n",
    "model5.add(Activation('sigmoid'))\n",
    "\n",
    "model5.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model5.fit(X, y, batch_size=32, epochs=10, validation_split=0.3, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow output for the accuracy\n",
    "\n",
    "<img src=\"Images/TensorBoard/epoch_acc_model5.svg?sanitize=true\" width=\"500\">\n",
    "\n",
    "Similar to the other best models as above, the accuracy starts at a good point, and then increases and then plateaus and then decreases slightly and then seems to increase for a given Epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets compare the model generated for the Cost Function - Model4 & Model5\n",
    "\n",
    "<img src=\"Images/TensorBoard/epoch_acc_cost_func.svg?sanitize=true\" width=\"500\">\n",
    "\n",
    "The models are almost similar in having the same accuracy, the only difference slightly would be at the Epoch of 4 where the `QuadraticCost` reduces the accuracy of the model slightly\n",
    "\n",
    "<img src=\"Images/comparision_2.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Part D` - Epochs\n",
    "\n",
    "We use the best model that has been run till now, and it turns out to be `Model1` as it has been used with `ReLLU` which usually is the best Activation function that turned out to be for our image dataset. We take the same model which was run for 5 Epochs and run it further for `10 Epochs` to see its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model1 again with  - <span class=\"mark\">Epoch 10</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1230 samples, validate on 528 samples\n",
      "Epoch 1/10\n",
      "1230/1230 [==============================] - ETA: 2s - loss: 0.0861 - acc: 0.968 - ETA: 2s - loss: 0.1078 - acc: 0.953 - ETA: 2s - loss: 0.0930 - acc: 0.968 - ETA: 2s - loss: 0.1133 - acc: 0.960 - ETA: 2s - loss: 0.1378 - acc: 0.950 - ETA: 2s - loss: 0.1533 - acc: 0.947 - ETA: 2s - loss: 0.1561 - acc: 0.950 - ETA: 2s - loss: 0.1530 - acc: 0.953 - ETA: 2s - loss: 0.1420 - acc: 0.954 - ETA: 2s - loss: 0.1350 - acc: 0.956 - ETA: 2s - loss: 0.1505 - acc: 0.946 - ETA: 2s - loss: 0.1424 - acc: 0.947 - ETA: 2s - loss: 0.1366 - acc: 0.951 - ETA: 2s - loss: 0.1366 - acc: 0.950 - ETA: 2s - loss: 0.1336 - acc: 0.954 - ETA: 2s - loss: 0.1357 - acc: 0.951 - ETA: 1s - loss: 0.1322 - acc: 0.952 - ETA: 1s - loss: 0.1288 - acc: 0.953 - ETA: 1s - loss: 0.1345 - acc: 0.950 - ETA: 1s - loss: 0.1469 - acc: 0.948 - ETA: 1s - loss: 0.1428 - acc: 0.949 - ETA: 1s - loss: 0.1377 - acc: 0.951 - ETA: 1s - loss: 0.1332 - acc: 0.953 - ETA: 1s - loss: 0.1305 - acc: 0.954 - ETA: 1s - loss: 0.1297 - acc: 0.955 - ETA: 1s - loss: 0.1321 - acc: 0.953 - ETA: 1s - loss: 0.1315 - acc: 0.953 - ETA: 0s - loss: 0.1347 - acc: 0.950 - ETA: 0s - loss: 0.1386 - acc: 0.950 - ETA: 0s - loss: 0.1374 - acc: 0.951 - ETA: 0s - loss: 0.1364 - acc: 0.950 - ETA: 0s - loss: 0.1340 - acc: 0.951 - ETA: 0s - loss: 0.1315 - acc: 0.952 - ETA: 0s - loss: 0.1311 - acc: 0.953 - ETA: 0s - loss: 0.1308 - acc: 0.953 - ETA: 0s - loss: 0.1282 - acc: 0.954 - ETA: 0s - loss: 0.1250 - acc: 0.955 - ETA: 0s - loss: 0.1226 - acc: 0.956 - 4s 3ms/sample - loss: 0.1214 - acc: 0.9569 - val_loss: 0.1960 - val_acc: 0.9299\n",
      "Epoch 2/10\n",
      "1230/1230 [==============================] - ETA: 3s - loss: 0.0508 - acc: 1.000 - ETA: 3s - loss: 0.0766 - acc: 0.984 - ETA: 3s - loss: 0.0663 - acc: 0.979 - ETA: 3s - loss: 0.0850 - acc: 0.968 - ETA: 3s - loss: 0.0921 - acc: 0.968 - ETA: 3s - loss: 0.1103 - acc: 0.958 - ETA: 3s - loss: 0.1334 - acc: 0.955 - ETA: 2s - loss: 0.1313 - acc: 0.957 - ETA: 2s - loss: 0.1325 - acc: 0.954 - ETA: 2s - loss: 0.1657 - acc: 0.950 - ETA: 2s - loss: 0.1777 - acc: 0.946 - ETA: 2s - loss: 0.1738 - acc: 0.947 - ETA: 2s - loss: 0.1794 - acc: 0.944 - ETA: 2s - loss: 0.1852 - acc: 0.939 - ETA: 2s - loss: 0.1814 - acc: 0.941 - ETA: 2s - loss: 0.1811 - acc: 0.941 - ETA: 2s - loss: 0.1793 - acc: 0.941 - ETA: 2s - loss: 0.1839 - acc: 0.935 - ETA: 2s - loss: 0.1806 - acc: 0.937 - ETA: 1s - loss: 0.1822 - acc: 0.935 - ETA: 1s - loss: 0.1746 - acc: 0.939 - ETA: 1s - loss: 0.1746 - acc: 0.938 - ETA: 1s - loss: 0.1709 - acc: 0.938 - ETA: 1s - loss: 0.1722 - acc: 0.938 - ETA: 1s - loss: 0.1738 - acc: 0.937 - ETA: 1s - loss: 0.1697 - acc: 0.938 - ETA: 1s - loss: 0.1673 - acc: 0.939 - ETA: 1s - loss: 0.1634 - acc: 0.942 - ETA: 0s - loss: 0.1634 - acc: 0.942 - ETA: 0s - loss: 0.1659 - acc: 0.942 - ETA: 0s - loss: 0.1620 - acc: 0.944 - ETA: 0s - loss: 0.1603 - acc: 0.944 - ETA: 0s - loss: 0.1610 - acc: 0.944 - ETA: 0s - loss: 0.1595 - acc: 0.943 - ETA: 0s - loss: 0.1577 - acc: 0.943 - ETA: 0s - loss: 0.1601 - acc: 0.942 - ETA: 0s - loss: 0.1654 - acc: 0.940 - ETA: 0s - loss: 0.1635 - acc: 0.940 - 4s 4ms/sample - loss: 0.1652 - acc: 0.9398 - val_loss: 0.2145 - val_acc: 0.9015\n",
      "Epoch 3/10\n",
      "1230/1230 [==============================] - ETA: 3s - loss: 0.1535 - acc: 0.968 - ETA: 3s - loss: 0.1339 - acc: 0.984 - ETA: 3s - loss: 0.1794 - acc: 0.947 - ETA: 3s - loss: 0.1570 - acc: 0.953 - ETA: 3s - loss: 0.1588 - acc: 0.937 - ETA: 3s - loss: 0.1427 - acc: 0.942 - ETA: 2s - loss: 0.1517 - acc: 0.937 - ETA: 2s - loss: 0.1438 - acc: 0.941 - ETA: 2s - loss: 0.1386 - acc: 0.944 - ETA: 2s - loss: 0.1433 - acc: 0.940 - ETA: 2s - loss: 0.1461 - acc: 0.934 - ETA: 2s - loss: 0.1428 - acc: 0.937 - ETA: 2s - loss: 0.1358 - acc: 0.942 - ETA: 2s - loss: 0.1350 - acc: 0.942 - ETA: 2s - loss: 0.1377 - acc: 0.941 - ETA: 2s - loss: 0.1375 - acc: 0.943 - ETA: 1s - loss: 0.1373 - acc: 0.946 - ETA: 1s - loss: 0.1353 - acc: 0.949 - ETA: 1s - loss: 0.1362 - acc: 0.949 - ETA: 1s - loss: 0.1332 - acc: 0.950 - ETA: 1s - loss: 0.1382 - acc: 0.947 - ETA: 1s - loss: 0.1467 - acc: 0.944 - ETA: 1s - loss: 0.1425 - acc: 0.947 - ETA: 1s - loss: 0.1419 - acc: 0.947 - ETA: 1s - loss: 0.1394 - acc: 0.950 - ETA: 1s - loss: 0.1389 - acc: 0.950 - ETA: 1s - loss: 0.1375 - acc: 0.951 - ETA: 0s - loss: 0.1363 - acc: 0.952 - ETA: 0s - loss: 0.1375 - acc: 0.952 - ETA: 0s - loss: 0.1366 - acc: 0.952 - ETA: 0s - loss: 0.1350 - acc: 0.952 - ETA: 0s - loss: 0.1370 - acc: 0.951 - ETA: 0s - loss: 0.1349 - acc: 0.951 - ETA: 0s - loss: 0.1357 - acc: 0.952 - ETA: 0s - loss: 0.1343 - acc: 0.952 - ETA: 0s - loss: 0.1336 - acc: 0.953 - ETA: 0s - loss: 0.1318 - acc: 0.954 - ETA: 0s - loss: 0.1298 - acc: 0.954 - 4s 3ms/sample - loss: 0.1311 - acc: 0.9545 - val_loss: 0.1892 - val_acc: 0.9223\n",
      "Epoch 4/10\n",
      "1230/1230 [==============================] - ETA: 3s - loss: 0.0385 - acc: 1.000 - ETA: 3s - loss: 0.0506 - acc: 0.984 - ETA: 3s - loss: 0.0984 - acc: 0.968 - ETA: 3s - loss: 0.0807 - acc: 0.976 - ETA: 3s - loss: 0.0748 - acc: 0.981 - ETA: 3s - loss: 0.0763 - acc: 0.979 - ETA: 2s - loss: 0.0725 - acc: 0.977 - ETA: 2s - loss: 0.0723 - acc: 0.976 - ETA: 2s - loss: 0.0722 - acc: 0.975 - ETA: 2s - loss: 0.0796 - acc: 0.971 - ETA: 2s - loss: 0.0818 - acc: 0.971 - ETA: 2s - loss: 0.0799 - acc: 0.971 - ETA: 2s - loss: 0.0871 - acc: 0.963 - ETA: 2s - loss: 0.0870 - acc: 0.966 - ETA: 2s - loss: 0.0880 - acc: 0.966 - ETA: 2s - loss: 0.0880 - acc: 0.968 - ETA: 2s - loss: 0.0845 - acc: 0.970 - ETA: 1s - loss: 0.0813 - acc: 0.972 - ETA: 1s - loss: 0.0865 - acc: 0.970 - ETA: 1s - loss: 0.0856 - acc: 0.970 - ETA: 1s - loss: 0.0920 - acc: 0.968 - ETA: 1s - loss: 0.0888 - acc: 0.970 - ETA: 1s - loss: 0.0877 - acc: 0.970 - ETA: 1s - loss: 0.0905 - acc: 0.968 - ETA: 1s - loss: 0.0901 - acc: 0.968 - ETA: 1s - loss: 0.0915 - acc: 0.967 - ETA: 1s - loss: 0.0943 - acc: 0.965 - ETA: 1s - loss: 0.0969 - acc: 0.964 - ETA: 0s - loss: 0.0942 - acc: 0.965 - ETA: 0s - loss: 0.0989 - acc: 0.962 - ETA: 0s - loss: 0.1034 - acc: 0.960 - ETA: 0s - loss: 0.1115 - acc: 0.958 - ETA: 0s - loss: 0.1097 - acc: 0.959 - ETA: 0s - loss: 0.1119 - acc: 0.957 - ETA: 0s - loss: 0.1137 - acc: 0.957 - ETA: 0s - loss: 0.1172 - acc: 0.955 - ETA: 0s - loss: 0.1161 - acc: 0.956 - ETA: 0s - loss: 0.1151 - acc: 0.956 - 4s 3ms/sample - loss: 0.1150 - acc: 0.9561 - val_loss: 0.2905 - val_acc: 0.9053\n",
      "Epoch 5/10\n",
      "1230/1230 [==============================] - ETA: 3s - loss: 0.3117 - acc: 0.812 - ETA: 3s - loss: 0.3001 - acc: 0.859 - ETA: 3s - loss: 0.2254 - acc: 0.895 - ETA: 3s - loss: 0.1902 - acc: 0.921 - ETA: 3s - loss: 0.1868 - acc: 0.918 - ETA: 3s - loss: 0.1934 - acc: 0.916 - ETA: 3s - loss: 0.1768 - acc: 0.928 - ETA: 3s - loss: 0.1828 - acc: 0.921 - ETA: 2s - loss: 0.1682 - acc: 0.930 - ETA: 2s - loss: 0.1588 - acc: 0.934 - ETA: 2s - loss: 0.1460 - acc: 0.940 - ETA: 2s - loss: 0.1575 - acc: 0.934 - ETA: 2s - loss: 0.1491 - acc: 0.937 - ETA: 2s - loss: 0.1448 - acc: 0.937 - ETA: 2s - loss: 0.1424 - acc: 0.937 - ETA: 2s - loss: 0.1362 - acc: 0.941 - ETA: 2s - loss: 0.1362 - acc: 0.941 - ETA: 2s - loss: 0.1316 - acc: 0.944 - ETA: 1s - loss: 0.1295 - acc: 0.945 - ETA: 1s - loss: 0.1260 - acc: 0.948 - ETA: 1s - loss: 0.1209 - acc: 0.950 - ETA: 1s - loss: 0.1167 - acc: 0.953 - ETA: 1s - loss: 0.1143 - acc: 0.953 - ETA: 1s - loss: 0.1106 - acc: 0.955 - ETA: 1s - loss: 0.1142 - acc: 0.953 - ETA: 1s - loss: 0.1108 - acc: 0.955 - ETA: 1s - loss: 0.1126 - acc: 0.954 - ETA: 1s - loss: 0.1097 - acc: 0.956 - ETA: 0s - loss: 0.1071 - acc: 0.958 - ETA: 0s - loss: 0.1075 - acc: 0.958 - ETA: 0s - loss: 0.1057 - acc: 0.959 - ETA: 0s - loss: 0.1037 - acc: 0.960 - ETA: 0s - loss: 0.1056 - acc: 0.959 - ETA: 0s - loss: 0.1049 - acc: 0.959 - ETA: 0s - loss: 0.1043 - acc: 0.958 - ETA: 0s - loss: 0.1047 - acc: 0.959 - ETA: 0s - loss: 0.1022 - acc: 0.960 - ETA: 0s - loss: 0.1005 - acc: 0.961 - 4s 4ms/sample - loss: 0.1002 - acc: 0.9610 - val_loss: 0.2203 - val_acc: 0.9242\n",
      "Epoch 6/10\n",
      "1230/1230 [==============================] - ETA: 3s - loss: 0.1535 - acc: 0.968 - ETA: 3s - loss: 0.1470 - acc: 0.953 - ETA: 3s - loss: 0.1084 - acc: 0.968 - ETA: 3s - loss: 0.1187 - acc: 0.968 - ETA: 3s - loss: 0.1100 - acc: 0.975 - ETA: 3s - loss: 0.1025 - acc: 0.979 - ETA: 3s - loss: 0.0954 - acc: 0.977 - ETA: 3s - loss: 0.1037 - acc: 0.968 - ETA: 3s - loss: 0.0991 - acc: 0.968 - ETA: 2s - loss: 0.0916 - acc: 0.971 - ETA: 2s - loss: 0.0896 - acc: 0.971 - ETA: 2s - loss: 0.0903 - acc: 0.971 - ETA: 2s - loss: 0.0956 - acc: 0.966 - ETA: 2s - loss: 0.0954 - acc: 0.966 - ETA: 2s - loss: 0.0925 - acc: 0.968 - ETA: 2s - loss: 0.1082 - acc: 0.960 - ETA: 2s - loss: 0.1067 - acc: 0.961 - ETA: 2s - loss: 0.1046 - acc: 0.961 - ETA: 2s - loss: 0.1032 - acc: 0.960 - ETA: 1s - loss: 0.1169 - acc: 0.956 - ETA: 1s - loss: 0.1145 - acc: 0.956 - ETA: 1s - loss: 0.1135 - acc: 0.956 - ETA: 1s - loss: 0.1105 - acc: 0.957 - ETA: 1s - loss: 0.1095 - acc: 0.959 - ETA: 1s - loss: 0.1106 - acc: 0.958 - ETA: 1s - loss: 0.1110 - acc: 0.957 - ETA: 1s - loss: 0.1090 - acc: 0.959 - ETA: 1s - loss: 0.1073 - acc: 0.959 - ETA: 0s - loss: 0.1085 - acc: 0.960 - ETA: 0s - loss: 0.1061 - acc: 0.960 - ETA: 0s - loss: 0.1057 - acc: 0.959 - ETA: 0s - loss: 0.1081 - acc: 0.957 - ETA: 0s - loss: 0.1101 - acc: 0.955 - ETA: 0s - loss: 0.1075 - acc: 0.956 - ETA: 0s - loss: 0.1068 - acc: 0.958 - ETA: 0s - loss: 0.1067 - acc: 0.957 - ETA: 0s - loss: 0.1100 - acc: 0.955 - ETA: 0s - loss: 0.1085 - acc: 0.956 - 4s 4ms/sample - loss: 0.1078 - acc: 0.9569 - val_loss: 0.2557 - val_acc: 0.9129\n",
      "Epoch 7/10\n",
      "1230/1230 [==============================] - ETA: 3s - loss: 0.0975 - acc: 0.968 - ETA: 3s - loss: 0.0741 - acc: 0.968 - ETA: 3s - loss: 0.1375 - acc: 0.937 - ETA: 3s - loss: 0.1620 - acc: 0.921 - ETA: 3s - loss: 0.1372 - acc: 0.937 - ETA: 3s - loss: 0.1301 - acc: 0.937 - ETA: 3s - loss: 0.1259 - acc: 0.937 - ETA: 3s - loss: 0.1484 - acc: 0.929 - ETA: 2s - loss: 0.1412 - acc: 0.934 - ETA: 2s - loss: 0.1333 - acc: 0.940 - ETA: 2s - loss: 0.1350 - acc: 0.940 - ETA: 2s - loss: 0.1265 - acc: 0.945 - ETA: 2s - loss: 0.1259 - acc: 0.947 - ETA: 2s - loss: 0.1209 - acc: 0.948 - ETA: 2s - loss: 0.1152 - acc: 0.952 - ETA: 2s - loss: 0.1089 - acc: 0.955 - ETA: 2s - loss: 0.1040 - acc: 0.957 - ETA: 2s - loss: 0.0991 - acc: 0.960 - ETA: 1s - loss: 0.0969 - acc: 0.960 - ETA: 1s - loss: 0.0940 - acc: 0.962 - ETA: 1s - loss: 0.0905 - acc: 0.964 - ETA: 1s - loss: 0.0889 - acc: 0.964 - ETA: 1s - loss: 0.0921 - acc: 0.962 - ETA: 1s - loss: 0.0897 - acc: 0.963 - ETA: 1s - loss: 0.0891 - acc: 0.963 - ETA: 1s - loss: 0.0878 - acc: 0.965 - ETA: 1s - loss: 0.0862 - acc: 0.966 - ETA: 1s - loss: 0.0875 - acc: 0.966 - ETA: 0s - loss: 0.0849 - acc: 0.967 - ETA: 0s - loss: 0.0829 - acc: 0.968 - ETA: 0s - loss: 0.0834 - acc: 0.968 - ETA: 0s - loss: 0.0816 - acc: 0.969 - ETA: 0s - loss: 0.0804 - acc: 0.970 - ETA: 0s - loss: 0.0808 - acc: 0.970 - ETA: 0s - loss: 0.0805 - acc: 0.970 - ETA: 0s - loss: 0.0801 - acc: 0.970 - ETA: 0s - loss: 0.0785 - acc: 0.971 - ETA: 0s - loss: 0.0778 - acc: 0.971 - 4s 4ms/sample - loss: 0.0772 - acc: 0.9715 - val_loss: 0.2432 - val_acc: 0.9280\n",
      "Epoch 8/10\n",
      "1230/1230 [==============================] - ETA: 4s - loss: 0.1669 - acc: 0.937 - ETA: 4s - loss: 0.1212 - acc: 0.953 - ETA: 3s - loss: 0.0883 - acc: 0.968 - ETA: 3s - loss: 0.0786 - acc: 0.976 - ETA: 3s - loss: 0.0786 - acc: 0.968 - ETA: 3s - loss: 0.0726 - acc: 0.974 - ETA: 3s - loss: 0.0650 - acc: 0.977 - ETA: 3s - loss: 0.0579 - acc: 0.980 - ETA: 3s - loss: 0.0585 - acc: 0.979 - ETA: 3s - loss: 0.0549 - acc: 0.981 - ETA: 2s - loss: 0.0588 - acc: 0.977 - ETA: 2s - loss: 0.0560 - acc: 0.979 - ETA: 2s - loss: 0.0570 - acc: 0.978 - ETA: 2s - loss: 0.0561 - acc: 0.979 - ETA: 2s - loss: 0.0542 - acc: 0.981 - ETA: 2s - loss: 0.0523 - acc: 0.982 - ETA: 2s - loss: 0.0499 - acc: 0.983 - ETA: 2s - loss: 0.0587 - acc: 0.979 - ETA: 2s - loss: 0.0624 - acc: 0.977 - ETA: 1s - loss: 0.0610 - acc: 0.978 - ETA: 1s - loss: 0.0596 - acc: 0.979 - ETA: 1s - loss: 0.0613 - acc: 0.978 - ETA: 1s - loss: 0.0596 - acc: 0.979 - ETA: 1s - loss: 0.0581 - acc: 0.980 - ETA: 1s - loss: 0.0574 - acc: 0.980 - ETA: 1s - loss: 0.0610 - acc: 0.978 - ETA: 1s - loss: 0.0592 - acc: 0.979 - ETA: 1s - loss: 0.0610 - acc: 0.978 - ETA: 0s - loss: 0.0599 - acc: 0.979 - ETA: 0s - loss: 0.0592 - acc: 0.980 - ETA: 0s - loss: 0.0627 - acc: 0.978 - ETA: 0s - loss: 0.0613 - acc: 0.979 - ETA: 0s - loss: 0.0627 - acc: 0.979 - ETA: 0s - loss: 0.0618 - acc: 0.979 - ETA: 0s - loss: 0.0612 - acc: 0.980 - ETA: 0s - loss: 0.0613 - acc: 0.980 - ETA: 0s - loss: 0.0600 - acc: 0.980 - ETA: 0s - loss: 0.0601 - acc: 0.980 - 4s 4ms/sample - loss: 0.0596 - acc: 0.9805 - val_loss: 0.1997 - val_acc: 0.9261\n",
      "Epoch 9/10\n",
      "1230/1230 [==============================] - ETA: 3s - loss: 0.0293 - acc: 1.000 - ETA: 3s - loss: 0.0472 - acc: 0.984 - ETA: 3s - loss: 0.0432 - acc: 0.989 - ETA: 3s - loss: 0.0388 - acc: 0.992 - ETA: 3s - loss: 0.0362 - acc: 0.993 - ETA: 3s - loss: 0.0333 - acc: 0.994 - ETA: 3s - loss: 0.0373 - acc: 0.986 - ETA: 3s - loss: 0.0346 - acc: 0.988 - ETA: 3s - loss: 0.0338 - acc: 0.989 - ETA: 2s - loss: 0.0310 - acc: 0.990 - ETA: 2s - loss: 0.0295 - acc: 0.991 - ETA: 2s - loss: 0.0305 - acc: 0.992 - ETA: 2s - loss: 0.0306 - acc: 0.992 - ETA: 2s - loss: 0.0299 - acc: 0.993 - ETA: 2s - loss: 0.0321 - acc: 0.991 - ETA: 2s - loss: 0.0329 - acc: 0.990 - ETA: 2s - loss: 0.0317 - acc: 0.990 - ETA: 2s - loss: 0.0373 - acc: 0.989 - ETA: 1s - loss: 0.0363 - acc: 0.990 - ETA: 1s - loss: 0.0349 - acc: 0.990 - ETA: 1s - loss: 0.0334 - acc: 0.991 - ETA: 1s - loss: 0.0327 - acc: 0.991 - ETA: 1s - loss: 0.0315 - acc: 0.991 - ETA: 1s - loss: 0.0332 - acc: 0.990 - ETA: 1s - loss: 0.0343 - acc: 0.990 - ETA: 1s - loss: 0.0376 - acc: 0.989 - ETA: 1s - loss: 0.0385 - acc: 0.988 - ETA: 1s - loss: 0.0384 - acc: 0.988 - ETA: 0s - loss: 0.0387 - acc: 0.989 - ETA: 0s - loss: 0.0381 - acc: 0.989 - ETA: 0s - loss: 0.0374 - acc: 0.989 - ETA: 0s - loss: 0.0376 - acc: 0.989 - ETA: 0s - loss: 0.0368 - acc: 0.989 - ETA: 0s - loss: 0.0362 - acc: 0.989 - ETA: 0s - loss: 0.0358 - acc: 0.990 - ETA: 0s - loss: 0.0367 - acc: 0.988 - ETA: 0s - loss: 0.0382 - acc: 0.988 - ETA: 0s - loss: 0.0376 - acc: 0.988 - 4s 4ms/sample - loss: 0.0375 - acc: 0.9886 - val_loss: 0.2404 - val_acc: 0.9167\n",
      "Epoch 10/10\n",
      "1230/1230 [==============================] - ETA: 3s - loss: 0.0711 - acc: 0.968 - ETA: 3s - loss: 0.0471 - acc: 0.984 - ETA: 3s - loss: 0.0371 - acc: 0.989 - ETA: 3s - loss: 0.0608 - acc: 0.968 - ETA: 3s - loss: 0.0553 - acc: 0.968 - ETA: 3s - loss: 0.0502 - acc: 0.974 - ETA: 3s - loss: 0.0523 - acc: 0.973 - ETA: 2s - loss: 0.0474 - acc: 0.976 - ETA: 2s - loss: 0.0425 - acc: 0.979 - ETA: 2s - loss: 0.0445 - acc: 0.978 - ETA: 2s - loss: 0.0437 - acc: 0.980 - ETA: 2s - loss: 0.0403 - acc: 0.981 - ETA: 2s - loss: 0.0382 - acc: 0.983 - ETA: 2s - loss: 0.0436 - acc: 0.982 - ETA: 2s - loss: 0.0422 - acc: 0.983 - ETA: 2s - loss: 0.0414 - acc: 0.984 - ETA: 2s - loss: 0.0397 - acc: 0.985 - ETA: 1s - loss: 0.0397 - acc: 0.986 - ETA: 1s - loss: 0.0393 - acc: 0.986 - ETA: 1s - loss: 0.0377 - acc: 0.987 - ETA: 1s - loss: 0.0377 - acc: 0.988 - ETA: 1s - loss: 0.0388 - acc: 0.987 - ETA: 1s - loss: 0.0377 - acc: 0.987 - ETA: 1s - loss: 0.0369 - acc: 0.988 - ETA: 1s - loss: 0.0357 - acc: 0.988 - ETA: 1s - loss: 0.0354 - acc: 0.989 - ETA: 1s - loss: 0.0343 - acc: 0.989 - ETA: 1s - loss: 0.0336 - acc: 0.990 - ETA: 0s - loss: 0.0362 - acc: 0.988 - ETA: 0s - loss: 0.0354 - acc: 0.988 - ETA: 0s - loss: 0.0344 - acc: 0.988 - ETA: 0s - loss: 0.0351 - acc: 0.988 - ETA: 0s - loss: 0.0344 - acc: 0.988 - ETA: 0s - loss: 0.0348 - acc: 0.989 - ETA: 0s - loss: 0.0346 - acc: 0.989 - ETA: 0s - loss: 0.0339 - acc: 0.989 - ETA: 0s - loss: 0.0365 - acc: 0.988 - ETA: 0s - loss: 0.0361 - acc: 0.988 - 4s 4ms/sample - loss: 0.0360 - acc: 0.9886 - val_loss: 0.1982 - val_acc: 0.9318\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ce67927a20>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NAME = \"Pokemon-model1-EPOCH10-{}\".format(int(time.time()))\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))\n",
    "\n",
    "model1.fit(X, y, batch_size=32, epochs=10, validation_split=0.3, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the run, TensorBoard output for the accuracy was as below, which is good:\n",
    "\n",
    "<img src=\"Images/image2.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Part E` - Gradient estimation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 2 optimizers as below:\n",
    "* Adadelta\n",
    "* Adagrad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6 - CNN - Softsign - QuadraticCost - 5 Epoch - <span class=\"mark\">Adadelta</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1230 samples, validate on 528 samples\n",
      "Epoch 1/10\n",
      "1230/1230 [==============================] - ETA: 43s - loss: 0.2509 - acc: 0.50 - ETA: 34s - loss: 0.1999 - acc: 0.67 - ETA: 32s - loss: 0.2924 - acc: 0.60 - ETA: 29s - loss: 0.3140 - acc: 0.54 - ETA: 28s - loss: 0.3059 - acc: 0.55 - ETA: 27s - loss: 0.2671 - acc: 0.61 - ETA: 26s - loss: 0.2373 - acc: 0.66 - ETA: 25s - loss: 0.2227 - acc: 0.68 - ETA: 24s - loss: 0.2180 - acc: 0.70 - ETA: 23s - loss: 0.1993 - acc: 0.72 - ETA: 21s - loss: 0.1879 - acc: 0.74 - ETA: 21s - loss: 0.1883 - acc: 0.73 - ETA: 20s - loss: 0.1842 - acc: 0.74 - ETA: 19s - loss: 0.1797 - acc: 0.75 - ETA: 18s - loss: 0.1733 - acc: 0.76 - ETA: 17s - loss: 0.1691 - acc: 0.76 - ETA: 16s - loss: 0.1614 - acc: 0.77 - ETA: 15s - loss: 0.1561 - acc: 0.78 - ETA: 15s - loss: 0.1527 - acc: 0.79 - ETA: 14s - loss: 0.1481 - acc: 0.79 - ETA: 13s - loss: 0.1482 - acc: 0.80 - ETA: 12s - loss: 0.1455 - acc: 0.80 - ETA: 12s - loss: 0.1404 - acc: 0.81 - ETA: 11s - loss: 0.1392 - acc: 0.81 - ETA: 10s - loss: 0.1370 - acc: 0.81 - ETA: 9s - loss: 0.1340 - acc: 0.8233 - ETA: 8s - loss: 0.1301 - acc: 0.828 - ETA: 8s - loss: 0.1269 - acc: 0.832 - ETA: 7s - loss: 0.1248 - acc: 0.836 - ETA: 6s - loss: 0.1244 - acc: 0.837 - ETA: 5s - loss: 0.1245 - acc: 0.838 - ETA: 4s - loss: 0.1216 - acc: 0.842 - ETA: 4s - loss: 0.1241 - acc: 0.840 - ETA: 3s - loss: 0.1266 - acc: 0.838 - ETA: 2s - loss: 0.1242 - acc: 0.842 - ETA: 1s - loss: 0.1212 - acc: 0.845 - ETA: 1s - loss: 0.1208 - acc: 0.846 - ETA: 0s - loss: 0.1190 - acc: 0.849 - 35s 28ms/sample - loss: 0.1199 - acc: 0.8488 - val_loss: 0.2538 - val_acc: 0.6061\n",
      "Epoch 2/10\n",
      "1230/1230 [==============================] - ETA: 28s - loss: 0.2595 - acc: 0.56 - ETA: 26s - loss: 0.2234 - acc: 0.67 - ETA: 25s - loss: 0.1801 - acc: 0.75 - ETA: 24s - loss: 0.1661 - acc: 0.78 - ETA: 24s - loss: 0.1455 - acc: 0.81 - ETA: 23s - loss: 0.1471 - acc: 0.81 - ETA: 22s - loss: 0.1305 - acc: 0.83 - ETA: 21s - loss: 0.1338 - acc: 0.83 - ETA: 21s - loss: 0.1362 - acc: 0.84 - ETA: 20s - loss: 0.1265 - acc: 0.85 - ETA: 19s - loss: 0.1207 - acc: 0.86 - ETA: 19s - loss: 0.1133 - acc: 0.86 - ETA: 18s - loss: 0.1117 - acc: 0.87 - ETA: 17s - loss: 0.1103 - acc: 0.87 - ETA: 17s - loss: 0.1088 - acc: 0.87 - ETA: 16s - loss: 0.1078 - acc: 0.87 - ETA: 15s - loss: 0.1086 - acc: 0.87 - ETA: 14s - loss: 0.1090 - acc: 0.87 - ETA: 14s - loss: 0.1129 - acc: 0.87 - ETA: 13s - loss: 0.1103 - acc: 0.87 - ETA: 12s - loss: 0.1094 - acc: 0.87 - ETA: 12s - loss: 0.1072 - acc: 0.88 - ETA: 11s - loss: 0.1088 - acc: 0.88 - ETA: 10s - loss: 0.1069 - acc: 0.88 - ETA: 9s - loss: 0.1082 - acc: 0.8813 - ETA: 9s - loss: 0.1085 - acc: 0.881 - ETA: 8s - loss: 0.1084 - acc: 0.880 - ETA: 7s - loss: 0.1088 - acc: 0.880 - ETA: 6s - loss: 0.1071 - acc: 0.882 - ETA: 6s - loss: 0.1073 - acc: 0.882 - ETA: 5s - loss: 0.1076 - acc: 0.882 - ETA: 4s - loss: 0.1060 - acc: 0.883 - ETA: 3s - loss: 0.1061 - acc: 0.883 - ETA: 3s - loss: 0.1030 - acc: 0.886 - ETA: 2s - loss: 0.1025 - acc: 0.887 - ETA: 1s - loss: 0.1033 - acc: 0.886 - ETA: 1s - loss: 0.1024 - acc: 0.886 - ETA: 0s - loss: 0.1017 - acc: 0.887 - 33s 27ms/sample - loss: 0.1009 - acc: 0.8886 - val_loss: 0.0934 - val_acc: 0.8902\n",
      "Epoch 3/10\n",
      "1230/1230 [==============================] - ETA: 28s - loss: 0.0382 - acc: 0.96 - ETA: 26s - loss: 0.0266 - acc: 0.96 - ETA: 26s - loss: 0.0343 - acc: 0.95 - ETA: 25s - loss: 0.0691 - acc: 0.91 - ETA: 24s - loss: 0.0913 - acc: 0.86 - ETA: 23s - loss: 0.0900 - acc: 0.87 - ETA: 23s - loss: 0.0813 - acc: 0.88 - ETA: 22s - loss: 0.0797 - acc: 0.89 - ETA: 21s - loss: 0.0776 - acc: 0.89 - ETA: 20s - loss: 0.0784 - acc: 0.89 - ETA: 20s - loss: 0.0760 - acc: 0.90 - ETA: 19s - loss: 0.0723 - acc: 0.90 - ETA: 18s - loss: 0.0712 - acc: 0.90 - ETA: 17s - loss: 0.0782 - acc: 0.89 - ETA: 17s - loss: 0.0804 - acc: 0.89 - ETA: 16s - loss: 0.0809 - acc: 0.89 - ETA: 15s - loss: 0.0781 - acc: 0.90 - ETA: 14s - loss: 0.0773 - acc: 0.90 - ETA: 14s - loss: 0.0779 - acc: 0.90 - ETA: 13s - loss: 0.0782 - acc: 0.90 - ETA: 12s - loss: 0.0793 - acc: 0.90 - ETA: 12s - loss: 0.0820 - acc: 0.89 - ETA: 11s - loss: 0.0832 - acc: 0.89 - ETA: 10s - loss: 0.0855 - acc: 0.89 - ETA: 10s - loss: 0.0854 - acc: 0.89 - ETA: 9s - loss: 0.0851 - acc: 0.8954 - ETA: 8s - loss: 0.0832 - acc: 0.898 - ETA: 7s - loss: 0.0838 - acc: 0.897 - ETA: 7s - loss: 0.0831 - acc: 0.898 - ETA: 6s - loss: 0.0832 - acc: 0.899 - ETA: 5s - loss: 0.0818 - acc: 0.901 - ETA: 4s - loss: 0.0817 - acc: 0.901 - ETA: 4s - loss: 0.0815 - acc: 0.901 - ETA: 3s - loss: 0.0808 - acc: 0.902 - ETA: 2s - loss: 0.0794 - acc: 0.904 - ETA: 1s - loss: 0.0806 - acc: 0.902 - ETA: 1s - loss: 0.0804 - acc: 0.902 - ETA: 0s - loss: 0.0807 - acc: 0.902 - 33s 27ms/sample - loss: 0.0800 - acc: 0.9033 - val_loss: 0.0847 - val_acc: 0.8939\n",
      "Epoch 4/10\n",
      "1230/1230 [==============================] - ETA: 27s - loss: 0.1317 - acc: 0.81 - ETA: 27s - loss: 0.1045 - acc: 0.85 - ETA: 26s - loss: 0.1258 - acc: 0.83 - ETA: 25s - loss: 0.1111 - acc: 0.85 - ETA: 24s - loss: 0.1009 - acc: 0.86 - ETA: 24s - loss: 0.0995 - acc: 0.86 - ETA: 23s - loss: 0.0961 - acc: 0.87 - ETA: 22s - loss: 0.0951 - acc: 0.87 - ETA: 21s - loss: 0.0922 - acc: 0.88 - ETA: 20s - loss: 0.0863 - acc: 0.89 - ETA: 20s - loss: 0.0856 - acc: 0.89 - ETA: 19s - loss: 0.0805 - acc: 0.90 - ETA: 18s - loss: 0.0785 - acc: 0.90 - ETA: 18s - loss: 0.0748 - acc: 0.91 - ETA: 17s - loss: 0.0736 - acc: 0.91 - ETA: 16s - loss: 0.0720 - acc: 0.91 - ETA: 16s - loss: 0.0721 - acc: 0.91 - ETA: 15s - loss: 0.0695 - acc: 0.91 - ETA: 14s - loss: 0.0723 - acc: 0.91 - ETA: 13s - loss: 0.0717 - acc: 0.91 - ETA: 13s - loss: 0.0714 - acc: 0.91 - ETA: 12s - loss: 0.0731 - acc: 0.91 - ETA: 11s - loss: 0.0722 - acc: 0.91 - ETA: 10s - loss: 0.0693 - acc: 0.91 - ETA: 10s - loss: 0.0730 - acc: 0.91 - ETA: 9s - loss: 0.0722 - acc: 0.9135 - ETA: 8s - loss: 0.0733 - acc: 0.912 - ETA: 7s - loss: 0.0721 - acc: 0.914 - ETA: 7s - loss: 0.0715 - acc: 0.914 - ETA: 6s - loss: 0.0703 - acc: 0.915 - ETA: 5s - loss: 0.0721 - acc: 0.913 - ETA: 4s - loss: 0.0701 - acc: 0.916 - ETA: 4s - loss: 0.0685 - acc: 0.917 - ETA: 3s - loss: 0.0683 - acc: 0.917 - ETA: 2s - loss: 0.0685 - acc: 0.917 - ETA: 1s - loss: 0.0696 - acc: 0.916 - ETA: 1s - loss: 0.0707 - acc: 0.915 - ETA: 0s - loss: 0.0731 - acc: 0.912 - 35s 28ms/sample - loss: 0.0745 - acc: 0.9098 - val_loss: 0.3540 - val_acc: 0.5625\n",
      "Epoch 5/10\n",
      "1230/1230 [==============================] - ETA: 28s - loss: 0.3294 - acc: 0.53 - ETA: 27s - loss: 0.2130 - acc: 0.70 - ETA: 26s - loss: 0.1741 - acc: 0.76 - ETA: 25s - loss: 0.1547 - acc: 0.78 - ETA: 25s - loss: 0.1287 - acc: 0.82 - ETA: 24s - loss: 0.1116 - acc: 0.84 - ETA: 23s - loss: 0.1074 - acc: 0.85 - ETA: 22s - loss: 0.1068 - acc: 0.85 - ETA: 22s - loss: 0.1036 - acc: 0.86 - ETA: 21s - loss: 0.0982 - acc: 0.87 - ETA: 20s - loss: 0.0903 - acc: 0.88 - ETA: 19s - loss: 0.0911 - acc: 0.88 - ETA: 19s - loss: 0.0863 - acc: 0.88 - ETA: 18s - loss: 0.0838 - acc: 0.89 - ETA: 17s - loss: 0.0860 - acc: 0.88 - ETA: 16s - loss: 0.0855 - acc: 0.89 - ETA: 16s - loss: 0.0819 - acc: 0.89 - ETA: 15s - loss: 0.0780 - acc: 0.90 - ETA: 14s - loss: 0.0796 - acc: 0.89 - ETA: 13s - loss: 0.0783 - acc: 0.90 - ETA: 13s - loss: 0.0750 - acc: 0.90 - ETA: 12s - loss: 0.0750 - acc: 0.90 - ETA: 11s - loss: 0.0750 - acc: 0.90 - ETA: 10s - loss: 0.0774 - acc: 0.90 - ETA: 10s - loss: 0.0786 - acc: 0.89 - ETA: 9s - loss: 0.0821 - acc: 0.8930 - ETA: 8s - loss: 0.0807 - acc: 0.894 - ETA: 7s - loss: 0.0805 - acc: 0.895 - ETA: 7s - loss: 0.0794 - acc: 0.896 - ETA: 6s - loss: 0.0831 - acc: 0.891 - ETA: 5s - loss: 0.0823 - acc: 0.893 - ETA: 4s - loss: 0.0816 - acc: 0.894 - ETA: 4s - loss: 0.0798 - acc: 0.897 - ETA: 3s - loss: 0.0819 - acc: 0.894 - ETA: 2s - loss: 0.0816 - acc: 0.893 - ETA: 1s - loss: 0.0809 - acc: 0.895 - ETA: 1s - loss: 0.0793 - acc: 0.897 - ETA: 0s - loss: 0.0793 - acc: 0.896 - 35s 28ms/sample - loss: 0.0798 - acc: 0.8959 - val_loss: 0.0801 - val_acc: 0.8939\n",
      "Epoch 6/10\n",
      "1230/1230 [==============================] - ETA: 28s - loss: 0.0918 - acc: 0.87 - ETA: 29s - loss: 0.0805 - acc: 0.89 - ETA: 28s - loss: 0.0713 - acc: 0.90 - ETA: 27s - loss: 0.0711 - acc: 0.90 - ETA: 26s - loss: 0.0710 - acc: 0.90 - ETA: 25s - loss: 0.0720 - acc: 0.90 - ETA: 25s - loss: 0.0762 - acc: 0.89 - ETA: 24s - loss: 0.0719 - acc: 0.89 - ETA: 23s - loss: 0.0667 - acc: 0.90 - ETA: 22s - loss: 0.0623 - acc: 0.91 - ETA: 21s - loss: 0.0674 - acc: 0.90 - ETA: 20s - loss: 0.0718 - acc: 0.89 - ETA: 19s - loss: 0.0675 - acc: 0.90 - ETA: 18s - loss: 0.0681 - acc: 0.90 - ETA: 18s - loss: 0.0672 - acc: 0.90 - ETA: 17s - loss: 0.0642 - acc: 0.90 - ETA: 16s - loss: 0.0663 - acc: 0.90 - ETA: 15s - loss: 0.0649 - acc: 0.90 - ETA: 14s - loss: 0.0625 - acc: 0.91 - ETA: 14s - loss: 0.0626 - acc: 0.91 - ETA: 13s - loss: 0.0621 - acc: 0.91 - ETA: 12s - loss: 0.0613 - acc: 0.91 - ETA: 11s - loss: 0.0596 - acc: 0.91 - ETA: 11s - loss: 0.0611 - acc: 0.91 - ETA: 10s - loss: 0.0605 - acc: 0.91 - ETA: 9s - loss: 0.0596 - acc: 0.9195 - ETA: 8s - loss: 0.0592 - acc: 0.920 - ETA: 7s - loss: 0.0590 - acc: 0.919 - ETA: 7s - loss: 0.0605 - acc: 0.917 - ETA: 6s - loss: 0.0612 - acc: 0.917 - ETA: 5s - loss: 0.0622 - acc: 0.917 - ETA: 4s - loss: 0.0623 - acc: 0.917 - ETA: 4s - loss: 0.0646 - acc: 0.913 - ETA: 3s - loss: 0.0646 - acc: 0.914 - ETA: 2s - loss: 0.0644 - acc: 0.914 - ETA: 1s - loss: 0.0638 - acc: 0.914 - ETA: 1s - loss: 0.0633 - acc: 0.914 - ETA: 0s - loss: 0.0630 - acc: 0.915 - 34s 28ms/sample - loss: 0.0631 - acc: 0.9154 - val_loss: 0.0884 - val_acc: 0.8939\n",
      "Epoch 7/10\n",
      "1230/1230 [==============================] - ETA: 32s - loss: 0.0408 - acc: 0.93 - ETA: 29s - loss: 0.0492 - acc: 0.92 - ETA: 27s - loss: 0.0408 - acc: 0.93 - ETA: 26s - loss: 0.0495 - acc: 0.92 - ETA: 25s - loss: 0.0470 - acc: 0.93 - ETA: 24s - loss: 0.0463 - acc: 0.93 - ETA: 23s - loss: 0.0470 - acc: 0.93 - ETA: 22s - loss: 0.0451 - acc: 0.93 - ETA: 21s - loss: 0.0406 - acc: 0.94 - ETA: 20s - loss: 0.0500 - acc: 0.93 - ETA: 19s - loss: 0.0508 - acc: 0.93 - ETA: 19s - loss: 0.0542 - acc: 0.92 - ETA: 18s - loss: 0.0598 - acc: 0.91 - ETA: 17s - loss: 0.0600 - acc: 0.91 - ETA: 16s - loss: 0.0611 - acc: 0.91 - ETA: 16s - loss: 0.0606 - acc: 0.91 - ETA: 15s - loss: 0.0631 - acc: 0.91 - ETA: 14s - loss: 0.0627 - acc: 0.91 - ETA: 13s - loss: 0.0634 - acc: 0.91 - ETA: 13s - loss: 0.0626 - acc: 0.91 - ETA: 12s - loss: 0.0603 - acc: 0.91 - ETA: 11s - loss: 0.0582 - acc: 0.92 - ETA: 10s - loss: 0.0574 - acc: 0.92 - ETA: 10s - loss: 0.0557 - acc: 0.92 - ETA: 9s - loss: 0.0547 - acc: 0.9250 - ETA: 8s - loss: 0.0542 - acc: 0.925 - ETA: 8s - loss: 0.0559 - acc: 0.922 - ETA: 7s - loss: 0.0557 - acc: 0.923 - ETA: 6s - loss: 0.0582 - acc: 0.920 - ETA: 5s - loss: 0.0582 - acc: 0.921 - ETA: 5s - loss: 0.0577 - acc: 0.923 - ETA: 4s - loss: 0.0602 - acc: 0.920 - ETA: 3s - loss: 0.0591 - acc: 0.922 - ETA: 3s - loss: 0.0602 - acc: 0.921 - ETA: 2s - loss: 0.0593 - acc: 0.923 - ETA: 1s - loss: 0.0589 - acc: 0.924 - ETA: 1s - loss: 0.0593 - acc: 0.923 - ETA: 0s - loss: 0.0604 - acc: 0.922 - 32s 26ms/sample - loss: 0.0599 - acc: 0.9236 - val_loss: 0.0809 - val_acc: 0.8977\n",
      "Epoch 8/10\n",
      "1230/1230 [==============================] - ETA: 27s - loss: 0.0319 - acc: 0.93 - ETA: 27s - loss: 0.0245 - acc: 0.95 - ETA: 27s - loss: 0.0235 - acc: 0.95 - ETA: 25s - loss: 0.0251 - acc: 0.96 - ETA: 25s - loss: 0.0277 - acc: 0.95 - ETA: 25s - loss: 0.0357 - acc: 0.94 - ETA: 24s - loss: 0.0325 - acc: 0.95 - ETA: 24s - loss: 0.0387 - acc: 0.93 - ETA: 23s - loss: 0.0446 - acc: 0.92 - ETA: 22s - loss: 0.0437 - acc: 0.93 - ETA: 21s - loss: 0.0489 - acc: 0.92 - ETA: 21s - loss: 0.0486 - acc: 0.92 - ETA: 20s - loss: 0.0483 - acc: 0.93 - ETA: 19s - loss: 0.0488 - acc: 0.93 - ETA: 18s - loss: 0.0459 - acc: 0.93 - ETA: 17s - loss: 0.0478 - acc: 0.93 - ETA: 17s - loss: 0.0488 - acc: 0.93 - ETA: 16s - loss: 0.0498 - acc: 0.93 - ETA: 15s - loss: 0.0488 - acc: 0.93 - ETA: 14s - loss: 0.0479 - acc: 0.93 - ETA: 13s - loss: 0.0484 - acc: 0.93 - ETA: 13s - loss: 0.0464 - acc: 0.93 - ETA: 12s - loss: 0.0458 - acc: 0.93 - ETA: 11s - loss: 0.0459 - acc: 0.93 - ETA: 10s - loss: 0.0461 - acc: 0.93 - ETA: 9s - loss: 0.0460 - acc: 0.9363 - ETA: 8s - loss: 0.0463 - acc: 0.936 - ETA: 8s - loss: 0.0470 - acc: 0.935 - ETA: 7s - loss: 0.0470 - acc: 0.935 - ETA: 6s - loss: 0.0476 - acc: 0.934 - ETA: 5s - loss: 0.0471 - acc: 0.936 - ETA: 5s - loss: 0.0457 - acc: 0.938 - ETA: 4s - loss: 0.0486 - acc: 0.933 - ETA: 3s - loss: 0.0495 - acc: 0.932 - ETA: 2s - loss: 0.0491 - acc: 0.933 - ETA: 1s - loss: 0.0511 - acc: 0.930 - ETA: 1s - loss: 0.0517 - acc: 0.929 - ETA: 0s - loss: 0.0537 - acc: 0.926 - 36s 29ms/sample - loss: 0.0540 - acc: 0.9268 - val_loss: 0.0689 - val_acc: 0.9129\n",
      "Epoch 9/10\n",
      "1230/1230 [==============================] - ETA: 27s - loss: 0.0335 - acc: 0.96 - ETA: 26s - loss: 0.0298 - acc: 0.96 - ETA: 26s - loss: 0.0378 - acc: 0.93 - ETA: 25s - loss: 0.0417 - acc: 0.93 - ETA: 25s - loss: 0.0406 - acc: 0.94 - ETA: 24s - loss: 0.0487 - acc: 0.93 - ETA: 23s - loss: 0.0495 - acc: 0.93 - ETA: 22s - loss: 0.0491 - acc: 0.93 - ETA: 21s - loss: 0.0471 - acc: 0.93 - ETA: 20s - loss: 0.0444 - acc: 0.94 - ETA: 19s - loss: 0.0431 - acc: 0.94 - ETA: 19s - loss: 0.0437 - acc: 0.94 - ETA: 18s - loss: 0.0463 - acc: 0.93 - ETA: 17s - loss: 0.0451 - acc: 0.94 - ETA: 17s - loss: 0.0472 - acc: 0.94 - ETA: 16s - loss: 0.0447 - acc: 0.94 - ETA: 15s - loss: 0.0434 - acc: 0.94 - ETA: 14s - loss: 0.0423 - acc: 0.94 - ETA: 14s - loss: 0.0413 - acc: 0.94 - ETA: 13s - loss: 0.0414 - acc: 0.94 - ETA: 12s - loss: 0.0405 - acc: 0.94 - ETA: 11s - loss: 0.0393 - acc: 0.95 - ETA: 11s - loss: 0.0380 - acc: 0.95 - ETA: 10s - loss: 0.0409 - acc: 0.94 - ETA: 9s - loss: 0.0442 - acc: 0.9400 - ETA: 8s - loss: 0.0426 - acc: 0.942 - ETA: 8s - loss: 0.0423 - acc: 0.943 - ETA: 7s - loss: 0.0456 - acc: 0.939 - ETA: 6s - loss: 0.0490 - acc: 0.934 - ETA: 6s - loss: 0.0483 - acc: 0.935 - ETA: 5s - loss: 0.0511 - acc: 0.932 - ETA: 4s - loss: 0.0503 - acc: 0.933 - ETA: 3s - loss: 0.0533 - acc: 0.929 - ETA: 3s - loss: 0.0536 - acc: 0.930 - ETA: 2s - loss: 0.0572 - acc: 0.925 - ETA: 1s - loss: 0.0573 - acc: 0.925 - ETA: 1s - loss: 0.0567 - acc: 0.926 - ETA: 0s - loss: 0.0560 - acc: 0.927 - 32s 26ms/sample - loss: 0.0566 - acc: 0.9268 - val_loss: 0.0685 - val_acc: 0.9242\n",
      "Epoch 10/10\n",
      "1230/1230 [==============================] - ETA: 27s - loss: 0.0379 - acc: 0.96 - ETA: 26s - loss: 0.0785 - acc: 0.89 - ETA: 25s - loss: 0.0723 - acc: 0.90 - ETA: 24s - loss: 0.0638 - acc: 0.91 - ETA: 23s - loss: 0.0577 - acc: 0.91 - ETA: 22s - loss: 0.0534 - acc: 0.92 - ETA: 22s - loss: 0.0504 - acc: 0.93 - ETA: 21s - loss: 0.0489 - acc: 0.93 - ETA: 20s - loss: 0.0511 - acc: 0.93 - ETA: 20s - loss: 0.0519 - acc: 0.93 - ETA: 19s - loss: 0.0548 - acc: 0.92 - ETA: 19s - loss: 0.0512 - acc: 0.93 - ETA: 18s - loss: 0.0497 - acc: 0.93 - ETA: 17s - loss: 0.0475 - acc: 0.93 - ETA: 16s - loss: 0.0456 - acc: 0.93 - ETA: 16s - loss: 0.0481 - acc: 0.93 - ETA: 15s - loss: 0.0475 - acc: 0.93 - ETA: 14s - loss: 0.0492 - acc: 0.93 - ETA: 14s - loss: 0.0475 - acc: 0.93 - ETA: 13s - loss: 0.0455 - acc: 0.94 - ETA: 12s - loss: 0.0471 - acc: 0.93 - ETA: 11s - loss: 0.0461 - acc: 0.94 - ETA: 11s - loss: 0.0470 - acc: 0.93 - ETA: 10s - loss: 0.0475 - acc: 0.93 - ETA: 9s - loss: 0.0473 - acc: 0.9375 - ETA: 8s - loss: 0.0490 - acc: 0.935 - ETA: 8s - loss: 0.0508 - acc: 0.932 - ETA: 7s - loss: 0.0507 - acc: 0.933 - ETA: 6s - loss: 0.0503 - acc: 0.934 - ETA: 6s - loss: 0.0496 - acc: 0.935 - ETA: 5s - loss: 0.0488 - acc: 0.936 - ETA: 4s - loss: 0.0483 - acc: 0.937 - ETA: 3s - loss: 0.0482 - acc: 0.937 - ETA: 3s - loss: 0.0496 - acc: 0.935 - ETA: 2s - loss: 0.0497 - acc: 0.936 - ETA: 1s - loss: 0.0488 - acc: 0.937 - ETA: 1s - loss: 0.0484 - acc: 0.937 - ETA: 0s - loss: 0.0481 - acc: 0.937 - 34s 28ms/sample - loss: 0.0492 - acc: 0.9366 - val_loss: 0.0660 - val_acc: 0.9186\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ce21d259b0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model6 = Sequential()\n",
    "\n",
    "NAME = \"Pokemon-model6-{}\".format(int(time.time()))\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))\n",
    "\n",
    "\n",
    "model6.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
    "model6.add(Activation('softsign'))\n",
    "model6.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model6.add(Conv2D(256, (3, 3)))\n",
    "model6.add(Activation('softsign'))\n",
    "model6.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model6.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "\n",
    "model6.add(Dense(64))\n",
    "model6.add(Activation('softsign'))\n",
    "\n",
    "model6.add(Dense(1))\n",
    "model6.add(Activation('sigmoid'))\n",
    "\n",
    "model6.compile(loss='mean_squared_error',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model6.fit(X, y, batch_size=32, epochs=10, validation_split=0.3, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorBoard out put for accuracy\n",
    "\n",
    "<img src=\"Images/TensorBoard/epoch_acc_model6.svg?sanitize=true\" width=\"500\">\n",
    "\n",
    "\n",
    "##### As we can see the accuracy keeps increasing continouesly except at the Epoch 4. No plateuing in this scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 7 - CNN - Softsign - QuadraticCost - 5 Epoch - <span class=\"mark\">Adagrad</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1230 samples, validate on 528 samples\n",
      "Epoch 1/5\n",
      "1230/1230 [==============================] - ETA: 37s - loss: 0.2439 - acc: 0.40 - ETA: 32s - loss: 0.1940 - acc: 0.62 - ETA: 30s - loss: 0.3272 - acc: 0.55 - ETA: 28s - loss: 0.4016 - acc: 0.50 - ETA: 27s - loss: 0.4275 - acc: 0.50 - ETA: 26s - loss: 0.4292 - acc: 0.51 - ETA: 25s - loss: 0.4214 - acc: 0.52 - ETA: 24s - loss: 0.4195 - acc: 0.53 - ETA: 23s - loss: 0.4458 - acc: 0.51 - ETA: 22s - loss: 0.4481 - acc: 0.51 - ETA: 21s - loss: 0.4471 - acc: 0.51 - ETA: 20s - loss: 0.4489 - acc: 0.52 - ETA: 20s - loss: 0.4576 - acc: 0.51 - ETA: 19s - loss: 0.4606 - acc: 0.51 - ETA: 18s - loss: 0.4695 - acc: 0.50 - ETA: 17s - loss: 0.4714 - acc: 0.50 - ETA: 16s - loss: 0.4731 - acc: 0.50 - ETA: 15s - loss: 0.4641 - acc: 0.51 - ETA: 15s - loss: 0.4709 - acc: 0.50 - ETA: 14s - loss: 0.4771 - acc: 0.50 - ETA: 13s - loss: 0.4811 - acc: 0.50 - ETA: 12s - loss: 0.4806 - acc: 0.50 - ETA: 11s - loss: 0.4746 - acc: 0.50 - ETA: 11s - loss: 0.4770 - acc: 0.50 - ETA: 10s - loss: 0.4804 - acc: 0.50 - ETA: 9s - loss: 0.4823 - acc: 0.5036 - ETA: 8s - loss: 0.4818 - acc: 0.504 - ETA: 8s - loss: 0.4847 - acc: 0.502 - ETA: 7s - loss: 0.4820 - acc: 0.505 - ETA: 6s - loss: 0.4826 - acc: 0.505 - ETA: 5s - loss: 0.4821 - acc: 0.506 - ETA: 4s - loss: 0.4817 - acc: 0.506 - ETA: 4s - loss: 0.4746 - acc: 0.514 - ETA: 3s - loss: 0.4754 - acc: 0.513 - ETA: 2s - loss: 0.4752 - acc: 0.514 - ETA: 1s - loss: 0.4715 - acc: 0.518 - ETA: 1s - loss: 0.4698 - acc: 0.520 - ETA: 0s - loss: 0.4714 - acc: 0.518 - 35s 28ms/sample - loss: 0.4676 - acc: 0.5228 - val_loss: 0.4391 - val_acc: 0.5606\n",
      "Epoch 2/5\n",
      "1230/1230 [==============================] - ETA: 29s - loss: 0.6558 - acc: 0.34 - ETA: 27s - loss: 0.5309 - acc: 0.46 - ETA: 26s - loss: 0.5413 - acc: 0.45 - ETA: 25s - loss: 0.5231 - acc: 0.47 - ETA: 24s - loss: 0.4996 - acc: 0.50 - ETA: 23s - loss: 0.4840 - acc: 0.51 - ETA: 23s - loss: 0.4595 - acc: 0.54 - ETA: 22s - loss: 0.4528 - acc: 0.54 - ETA: 21s - loss: 0.4545 - acc: 0.54 - ETA: 20s - loss: 0.4527 - acc: 0.54 - ETA: 19s - loss: 0.4655 - acc: 0.53 - ETA: 19s - loss: 0.4475 - acc: 0.55 - ETA: 18s - loss: 0.4467 - acc: 0.55 - ETA: 17s - loss: 0.4482 - acc: 0.55 - ETA: 16s - loss: 0.4537 - acc: 0.54 - ETA: 16s - loss: 0.4507 - acc: 0.54 - ETA: 15s - loss: 0.4480 - acc: 0.55 - ETA: 14s - loss: 0.4561 - acc: 0.54 - ETA: 14s - loss: 0.4468 - acc: 0.55 - ETA: 13s - loss: 0.4463 - acc: 0.55 - ETA: 12s - loss: 0.4548 - acc: 0.54 - ETA: 11s - loss: 0.4567 - acc: 0.54 - ETA: 11s - loss: 0.4545 - acc: 0.54 - ETA: 10s - loss: 0.4589 - acc: 0.54 - ETA: 9s - loss: 0.4617 - acc: 0.5375 - ETA: 8s - loss: 0.4630 - acc: 0.536 - ETA: 8s - loss: 0.4665 - acc: 0.532 - ETA: 7s - loss: 0.4686 - acc: 0.530 - ETA: 6s - loss: 0.4736 - acc: 0.524 - ETA: 6s - loss: 0.4710 - acc: 0.527 - ETA: 5s - loss: 0.4694 - acc: 0.528 - ETA: 4s - loss: 0.4658 - acc: 0.531 - ETA: 3s - loss: 0.4647 - acc: 0.531 - ETA: 3s - loss: 0.4608 - acc: 0.534 - ETA: 2s - loss: 0.4588 - acc: 0.533 - ETA: 1s - loss: 0.4578 - acc: 0.531 - ETA: 1s - loss: 0.4564 - acc: 0.527 - ETA: 0s - loss: 0.4530 - acc: 0.523 - 33s 27ms/sample - loss: 0.4511 - acc: 0.5228 - val_loss: 0.2469 - val_acc: 0.5606\n",
      "Epoch 3/5\n",
      "1230/1230 [==============================] - ETA: 28s - loss: 0.2548 - acc: 0.50 - ETA: 27s - loss: 0.2453 - acc: 0.59 - ETA: 25s - loss: 0.2404 - acc: 0.61 - ETA: 24s - loss: 0.2389 - acc: 0.61 - ETA: 24s - loss: 0.2409 - acc: 0.59 - ETA: 23s - loss: 0.2562 - acc: 0.57 - ETA: 23s - loss: 0.2598 - acc: 0.53 - ETA: 22s - loss: 0.2538 - acc: 0.52 - ETA: 21s - loss: 0.2487 - acc: 0.52 - ETA: 21s - loss: 0.2399 - acc: 0.56 - ETA: 20s - loss: 0.2274 - acc: 0.59 - ETA: 19s - loss: 0.2182 - acc: 0.62 - ETA: 19s - loss: 0.2109 - acc: 0.64 - ETA: 18s - loss: 0.2043 - acc: 0.66 - ETA: 17s - loss: 0.1963 - acc: 0.68 - ETA: 16s - loss: 0.1895 - acc: 0.69 - ETA: 16s - loss: 0.1846 - acc: 0.70 - ETA: 15s - loss: 0.1783 - acc: 0.72 - ETA: 14s - loss: 0.1732 - acc: 0.73 - ETA: 13s - loss: 0.1707 - acc: 0.73 - ETA: 13s - loss: 0.1702 - acc: 0.73 - ETA: 12s - loss: 0.1687 - acc: 0.74 - ETA: 11s - loss: 0.1661 - acc: 0.74 - ETA: 10s - loss: 0.1632 - acc: 0.75 - ETA: 10s - loss: 0.1608 - acc: 0.76 - ETA: 9s - loss: 0.1593 - acc: 0.7644 - ETA: 8s - loss: 0.1552 - acc: 0.772 - ETA: 7s - loss: 0.1532 - acc: 0.776 - ETA: 7s - loss: 0.1501 - acc: 0.783 - ETA: 6s - loss: 0.1477 - acc: 0.787 - ETA: 5s - loss: 0.1450 - acc: 0.792 - ETA: 4s - loss: 0.1441 - acc: 0.793 - ETA: 4s - loss: 0.1427 - acc: 0.796 - ETA: 3s - loss: 0.1421 - acc: 0.797 - ETA: 2s - loss: 0.1420 - acc: 0.798 - ETA: 1s - loss: 0.1397 - acc: 0.802 - ETA: 1s - loss: 0.1388 - acc: 0.804 - ETA: 0s - loss: 0.1378 - acc: 0.805 - 34s 28ms/sample - loss: 0.1375 - acc: 0.8065 - val_loss: 0.0939 - val_acc: 0.8845\n",
      "Epoch 4/5\n",
      "1230/1230 [==============================] - ETA: 27s - loss: 0.1232 - acc: 0.84 - ETA: 26s - loss: 0.0816 - acc: 0.90 - ETA: 25s - loss: 0.1025 - acc: 0.87 - ETA: 24s - loss: 0.1018 - acc: 0.87 - ETA: 23s - loss: 0.0929 - acc: 0.88 - ETA: 22s - loss: 0.0906 - acc: 0.89 - ETA: 22s - loss: 0.0887 - acc: 0.89 - ETA: 21s - loss: 0.0855 - acc: 0.89 - ETA: 20s - loss: 0.0852 - acc: 0.90 - ETA: 20s - loss: 0.0836 - acc: 0.90 - ETA: 19s - loss: 0.0824 - acc: 0.91 - ETA: 18s - loss: 0.0842 - acc: 0.90 - ETA: 18s - loss: 0.0882 - acc: 0.90 - ETA: 17s - loss: 0.0873 - acc: 0.90 - ETA: 16s - loss: 0.0905 - acc: 0.89 - ETA: 15s - loss: 0.0904 - acc: 0.89 - ETA: 15s - loss: 0.0891 - acc: 0.89 - ETA: 14s - loss: 0.0893 - acc: 0.89 - ETA: 13s - loss: 0.0887 - acc: 0.89 - ETA: 13s - loss: 0.0868 - acc: 0.90 - ETA: 12s - loss: 0.0873 - acc: 0.90 - ETA: 11s - loss: 0.0868 - acc: 0.90 - ETA: 11s - loss: 0.0879 - acc: 0.89 - ETA: 10s - loss: 0.0869 - acc: 0.89 - ETA: 9s - loss: 0.0883 - acc: 0.8963 - ETA: 9s - loss: 0.0889 - acc: 0.896 - ETA: 8s - loss: 0.0900 - acc: 0.894 - ETA: 7s - loss: 0.0907 - acc: 0.896 - ETA: 6s - loss: 0.0908 - acc: 0.896 - ETA: 6s - loss: 0.0898 - acc: 0.897 - ETA: 5s - loss: 0.0887 - acc: 0.900 - ETA: 4s - loss: 0.0873 - acc: 0.902 - ETA: 3s - loss: 0.0871 - acc: 0.902 - ETA: 3s - loss: 0.0894 - acc: 0.898 - ETA: 2s - loss: 0.0894 - acc: 0.899 - ETA: 1s - loss: 0.0896 - acc: 0.898 - ETA: 1s - loss: 0.0905 - acc: 0.897 - ETA: 0s - loss: 0.0897 - acc: 0.898 - 33s 27ms/sample - loss: 0.0894 - acc: 0.8984 - val_loss: 0.0898 - val_acc: 0.8845\n",
      "Epoch 5/5\n",
      "1230/1230 [==============================] - ETA: 28s - loss: 0.0497 - acc: 0.96 - ETA: 27s - loss: 0.0673 - acc: 0.93 - ETA: 26s - loss: 0.0653 - acc: 0.93 - ETA: 26s - loss: 0.0933 - acc: 0.89 - ETA: 25s - loss: 0.0846 - acc: 0.90 - ETA: 24s - loss: 0.0834 - acc: 0.90 - ETA: 23s - loss: 0.0773 - acc: 0.91 - ETA: 22s - loss: 0.0748 - acc: 0.91 - ETA: 21s - loss: 0.0728 - acc: 0.92 - ETA: 20s - loss: 0.0751 - acc: 0.91 - ETA: 19s - loss: 0.0743 - acc: 0.91 - ETA: 19s - loss: 0.0751 - acc: 0.91 - ETA: 18s - loss: 0.0750 - acc: 0.91 - ETA: 17s - loss: 0.0740 - acc: 0.91 - ETA: 17s - loss: 0.0782 - acc: 0.91 - ETA: 16s - loss: 0.0802 - acc: 0.90 - ETA: 15s - loss: 0.0804 - acc: 0.90 - ETA: 14s - loss: 0.0828 - acc: 0.89 - ETA: 14s - loss: 0.0811 - acc: 0.90 - ETA: 13s - loss: 0.0828 - acc: 0.90 - ETA: 12s - loss: 0.0835 - acc: 0.90 - ETA: 12s - loss: 0.0829 - acc: 0.90 - ETA: 11s - loss: 0.0826 - acc: 0.90 - ETA: 10s - loss: 0.0825 - acc: 0.90 - ETA: 9s - loss: 0.0815 - acc: 0.9062 - ETA: 9s - loss: 0.0794 - acc: 0.909 - ETA: 8s - loss: 0.0788 - acc: 0.910 - ETA: 7s - loss: 0.0802 - acc: 0.908 - ETA: 6s - loss: 0.0817 - acc: 0.906 - ETA: 6s - loss: 0.0814 - acc: 0.906 - ETA: 5s - loss: 0.0825 - acc: 0.904 - ETA: 4s - loss: 0.0848 - acc: 0.900 - ETA: 3s - loss: 0.0858 - acc: 0.898 - ETA: 3s - loss: 0.0856 - acc: 0.898 - ETA: 2s - loss: 0.0850 - acc: 0.900 - ETA: 1s - loss: 0.0849 - acc: 0.900 - ETA: 1s - loss: 0.0858 - acc: 0.898 - ETA: 0s - loss: 0.0864 - acc: 0.897 - 34s 27ms/sample - loss: 0.0858 - acc: 0.8984 - val_loss: 0.0919 - val_acc: 0.8845\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ce47d2f588>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model7 = Sequential()\n",
    "\n",
    "NAME = \"Pokemon-model7-{}\".format(int(time.time()))\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))\n",
    "\n",
    "\n",
    "model7.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
    "model7.add(Activation('softsign'))\n",
    "model7.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model7.add(Conv2D(256, (3, 3)))\n",
    "model7.add(Activation('softsign'))\n",
    "model7.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model7.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "\n",
    "model7.add(Dense(64))\n",
    "model7.add(Activation('softsign'))\n",
    "\n",
    "model7.add(Dense(1))\n",
    "model7.add(Activation('sigmoid'))\n",
    "\n",
    "model7.compile(loss='mean_squared_error',\n",
    "              optimizer='adagrad',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model7.fit(X, y, batch_size=32, epochs=5, validation_split=0.3, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorBoard output for the accuracy\n",
    "\n",
    "<img src=\"Images/TensorBoard/epoch_acc_model7.svg?sanitize=true\" width=\"500\">\n",
    "\n",
    "Seems, the perfromance starts at a very bad level of accuracy. It then increases steadily after 1st Epoch. So, a conclusion can be made from this is that the `adagrad`doesnt help much with the network layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets compare the 2 models generated using `adadelta` & `adagrad` optimizers. The below is the graph of comparision in TensorBoard\n",
    "\n",
    "<img  src=\"Images/image3.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part F - Network Architecture\n",
    "\n",
    "We have hidden layers in the network that we create. Till now, we had 2 layers in each model that has been created as shown below:\n",
    "\n",
    "<img src=\"Images/hiddenlayer.svg?sanitize=true\">\n",
    "\n",
    "We remove one of them from the place as shown in the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 8 - With a change in Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1230 samples, validate on 528 samples\n",
      "Epoch 1/5\n",
      "1230/1230 [==============================] - ETA: 29s - loss: 0.2510 - acc: 0.46 - ETA: 24s - loss: 0.3755 - acc: 0.48 - ETA: 22s - loss: 0.2710 - acc: 0.62 - ETA: 20s - loss: 0.3048 - acc: 0.61 - ETA: 19s - loss: 0.3251 - acc: 0.61 - ETA: 18s - loss: 0.3439 - acc: 0.60 - ETA: 18s - loss: 0.3617 - acc: 0.59 - ETA: 17s - loss: 0.3751 - acc: 0.58 - ETA: 16s - loss: 0.3959 - acc: 0.56 - ETA: 16s - loss: 0.4157 - acc: 0.55 - ETA: 15s - loss: 0.4148 - acc: 0.55 - ETA: 15s - loss: 0.4141 - acc: 0.55 - ETA: 14s - loss: 0.4255 - acc: 0.55 - ETA: 14s - loss: 0.4286 - acc: 0.54 - ETA: 13s - loss: 0.4334 - acc: 0.54 - ETA: 13s - loss: 0.4473 - acc: 0.53 - ETA: 12s - loss: 0.4431 - acc: 0.53 - ETA: 11s - loss: 0.4532 - acc: 0.52 - ETA: 11s - loss: 0.4589 - acc: 0.52 - ETA: 10s - loss: 0.4594 - acc: 0.52 - ETA: 10s - loss: 0.4584 - acc: 0.52 - ETA: 9s - loss: 0.4603 - acc: 0.5256 - ETA: 9s - loss: 0.4606 - acc: 0.525 - ETA: 8s - loss: 0.4610 - acc: 0.526 - ETA: 7s - loss: 0.4600 - acc: 0.527 - ETA: 7s - loss: 0.4580 - acc: 0.530 - ETA: 6s - loss: 0.4572 - acc: 0.531 - ETA: 6s - loss: 0.4587 - acc: 0.530 - ETA: 5s - loss: 0.4580 - acc: 0.531 - ETA: 4s - loss: 0.4604 - acc: 0.529 - ETA: 4s - loss: 0.4597 - acc: 0.530 - ETA: 3s - loss: 0.4590 - acc: 0.531 - ETA: 3s - loss: 0.4602 - acc: 0.530 - ETA: 2s - loss: 0.4605 - acc: 0.530 - ETA: 1s - loss: 0.4616 - acc: 0.529 - ETA: 1s - loss: 0.4627 - acc: 0.528 - ETA: 0s - loss: 0.4612 - acc: 0.530 - ETA: 0s - loss: 0.4622 - acc: 0.529 - 27s 22ms/sample - loss: 0.4626 - acc: 0.5293 - val_loss: 0.4394 - val_acc: 0.5606\n",
      "Epoch 2/5\n",
      "1230/1230 [==============================] - ETA: 23s - loss: 0.4375 - acc: 0.56 - ETA: 21s - loss: 0.4219 - acc: 0.57 - ETA: 20s - loss: 0.4583 - acc: 0.54 - ETA: 19s - loss: 0.4766 - acc: 0.52 - ETA: 18s - loss: 0.4625 - acc: 0.53 - ETA: 18s - loss: 0.4583 - acc: 0.54 - ETA: 17s - loss: 0.4821 - acc: 0.51 - ETA: 16s - loss: 0.4766 - acc: 0.52 - ETA: 16s - loss: 0.4688 - acc: 0.53 - ETA: 15s - loss: 0.4656 - acc: 0.53 - ETA: 15s - loss: 0.4744 - acc: 0.52 - ETA: 14s - loss: 0.4661 - acc: 0.53 - ETA: 14s - loss: 0.4639 - acc: 0.53 - ETA: 13s - loss: 0.4665 - acc: 0.53 - ETA: 13s - loss: 0.4625 - acc: 0.53 - ETA: 12s - loss: 0.4648 - acc: 0.53 - ETA: 12s - loss: 0.4724 - acc: 0.52 - ETA: 11s - loss: 0.4705 - acc: 0.52 - ETA: 10s - loss: 0.4770 - acc: 0.52 - ETA: 10s - loss: 0.4797 - acc: 0.52 - ETA: 9s - loss: 0.4821 - acc: 0.5179 - ETA: 9s - loss: 0.4744 - acc: 0.525 - ETA: 8s - loss: 0.4769 - acc: 0.523 - ETA: 8s - loss: 0.4792 - acc: 0.520 - ETA: 7s - loss: 0.4838 - acc: 0.516 - ETA: 6s - loss: 0.4832 - acc: 0.516 - ETA: 6s - loss: 0.4803 - acc: 0.519 - ETA: 5s - loss: 0.4777 - acc: 0.522 - ETA: 5s - loss: 0.4774 - acc: 0.522 - ETA: 4s - loss: 0.4771 - acc: 0.522 - ETA: 4s - loss: 0.4778 - acc: 0.522 - ETA: 3s - loss: 0.4775 - acc: 0.522 - ETA: 2s - loss: 0.4763 - acc: 0.523 - ETA: 2s - loss: 0.4789 - acc: 0.521 - ETA: 1s - loss: 0.4786 - acc: 0.521 - ETA: 1s - loss: 0.4783 - acc: 0.521 - ETA: 0s - loss: 0.4780 - acc: 0.522 - ETA: 0s - loss: 0.4786 - acc: 0.521 - 25s 21ms/sample - loss: 0.4772 - acc: 0.5228 - val_loss: 0.4394 - val_acc: 0.5606\n",
      "Epoch 3/5\n",
      "1230/1230 [==============================] - ETA: 23s - loss: 0.3438 - acc: 0.65 - ETA: 21s - loss: 0.3281 - acc: 0.67 - ETA: 20s - loss: 0.3646 - acc: 0.63 - ETA: 19s - loss: 0.4062 - acc: 0.59 - ETA: 18s - loss: 0.4250 - acc: 0.57 - ETA: 17s - loss: 0.4375 - acc: 0.56 - ETA: 17s - loss: 0.4464 - acc: 0.55 - ETA: 16s - loss: 0.4492 - acc: 0.55 - ETA: 16s - loss: 0.4583 - acc: 0.54 - ETA: 15s - loss: 0.4469 - acc: 0.55 - ETA: 15s - loss: 0.4460 - acc: 0.55 - ETA: 14s - loss: 0.4557 - acc: 0.54 - ETA: 14s - loss: 0.4615 - acc: 0.53 - ETA: 13s - loss: 0.4643 - acc: 0.53 - ETA: 12s - loss: 0.4646 - acc: 0.53 - ETA: 12s - loss: 0.4688 - acc: 0.53 - ETA: 12s - loss: 0.4688 - acc: 0.53 - ETA: 11s - loss: 0.4688 - acc: 0.53 - ETA: 10s - loss: 0.4720 - acc: 0.52 - ETA: 10s - loss: 0.4719 - acc: 0.52 - ETA: 9s - loss: 0.4762 - acc: 0.5238 - ETA: 9s - loss: 0.4815 - acc: 0.518 - ETA: 8s - loss: 0.4755 - acc: 0.524 - ETA: 8s - loss: 0.4740 - acc: 0.526 - ETA: 7s - loss: 0.4763 - acc: 0.523 - ETA: 6s - loss: 0.4772 - acc: 0.522 - ETA: 6s - loss: 0.4757 - acc: 0.524 - ETA: 5s - loss: 0.4821 - acc: 0.517 - ETA: 5s - loss: 0.4817 - acc: 0.518 - ETA: 4s - loss: 0.4813 - acc: 0.518 - ETA: 4s - loss: 0.4788 - acc: 0.521 - ETA: 3s - loss: 0.4785 - acc: 0.521 - ETA: 3s - loss: 0.4773 - acc: 0.522 - ETA: 2s - loss: 0.4752 - acc: 0.524 - ETA: 1s - loss: 0.4759 - acc: 0.524 - ETA: 1s - loss: 0.4766 - acc: 0.523 - ETA: 0s - loss: 0.4780 - acc: 0.522 - ETA: 0s - loss: 0.4778 - acc: 0.522 - 26s 21ms/sample - loss: 0.4772 - acc: 0.5228 - val_loss: 0.4394 - val_acc: 0.5606\n",
      "Epoch 4/5\n",
      "1230/1230 [==============================] - ETA: 23s - loss: 0.4375 - acc: 0.56 - ETA: 22s - loss: 0.4688 - acc: 0.53 - ETA: 21s - loss: 0.4583 - acc: 0.54 - ETA: 20s - loss: 0.4609 - acc: 0.53 - ETA: 19s - loss: 0.4688 - acc: 0.53 - ETA: 19s - loss: 0.4688 - acc: 0.53 - ETA: 18s - loss: 0.4732 - acc: 0.52 - ETA: 18s - loss: 0.4844 - acc: 0.51 - ETA: 17s - loss: 0.4722 - acc: 0.52 - ETA: 16s - loss: 0.4594 - acc: 0.54 - ETA: 16s - loss: 0.4631 - acc: 0.53 - ETA: 15s - loss: 0.4531 - acc: 0.54 - ETA: 14s - loss: 0.4543 - acc: 0.54 - ETA: 14s - loss: 0.4576 - acc: 0.54 - ETA: 13s - loss: 0.4604 - acc: 0.53 - ETA: 12s - loss: 0.4629 - acc: 0.53 - ETA: 12s - loss: 0.4706 - acc: 0.52 - ETA: 11s - loss: 0.4653 - acc: 0.53 - ETA: 11s - loss: 0.4638 - acc: 0.53 - ETA: 10s - loss: 0.4703 - acc: 0.52 - ETA: 10s - loss: 0.4732 - acc: 0.52 - ETA: 9s - loss: 0.4759 - acc: 0.5241 - ETA: 8s - loss: 0.4728 - acc: 0.527 - ETA: 8s - loss: 0.4740 - acc: 0.526 - ETA: 7s - loss: 0.4750 - acc: 0.525 - ETA: 7s - loss: 0.4808 - acc: 0.519 - ETA: 6s - loss: 0.4850 - acc: 0.515 - ETA: 6s - loss: 0.4900 - acc: 0.510 - ETA: 5s - loss: 0.4903 - acc: 0.509 - ETA: 4s - loss: 0.4875 - acc: 0.512 - ETA: 4s - loss: 0.4889 - acc: 0.511 - ETA: 3s - loss: 0.4873 - acc: 0.512 - ETA: 3s - loss: 0.4858 - acc: 0.514 - ETA: 2s - loss: 0.4798 - acc: 0.520 - ETA: 1s - loss: 0.4813 - acc: 0.518 - ETA: 1s - loss: 0.4766 - acc: 0.523 - ETA: 0s - loss: 0.4780 - acc: 0.522 - ETA: 0s - loss: 0.4794 - acc: 0.520 - 27s 22ms/sample - loss: 0.4772 - acc: 0.5228 - val_loss: 0.4394 - val_acc: 0.5606\n",
      "Epoch 5/5\n",
      "1230/1230 [==============================] - ETA: 24s - loss: 0.4375 - acc: 0.56 - ETA: 21s - loss: 0.4062 - acc: 0.59 - ETA: 20s - loss: 0.4375 - acc: 0.56 - ETA: 19s - loss: 0.4531 - acc: 0.54 - ETA: 19s - loss: 0.4688 - acc: 0.53 - ETA: 18s - loss: 0.4688 - acc: 0.53 - ETA: 18s - loss: 0.4911 - acc: 0.50 - ETA: 17s - loss: 0.5000 - acc: 0.50 - ETA: 17s - loss: 0.4861 - acc: 0.51 - ETA: 16s - loss: 0.4750 - acc: 0.52 - ETA: 15s - loss: 0.4545 - acc: 0.54 - ETA: 15s - loss: 0.4479 - acc: 0.55 - ETA: 14s - loss: 0.4519 - acc: 0.54 - ETA: 13s - loss: 0.4665 - acc: 0.53 - ETA: 13s - loss: 0.4667 - acc: 0.53 - ETA: 12s - loss: 0.4707 - acc: 0.52 - ETA: 12s - loss: 0.4761 - acc: 0.52 - ETA: 11s - loss: 0.4670 - acc: 0.53 - ETA: 11s - loss: 0.4720 - acc: 0.52 - ETA: 10s - loss: 0.4813 - acc: 0.51 - ETA: 9s - loss: 0.4836 - acc: 0.5164 - ETA: 9s - loss: 0.4815 - acc: 0.518 - ETA: 8s - loss: 0.4851 - acc: 0.514 - ETA: 8s - loss: 0.4883 - acc: 0.511 - ETA: 7s - loss: 0.4875 - acc: 0.512 - ETA: 7s - loss: 0.4856 - acc: 0.514 - ETA: 6s - loss: 0.4838 - acc: 0.516 - ETA: 5s - loss: 0.4788 - acc: 0.521 - ETA: 5s - loss: 0.4838 - acc: 0.516 - ETA: 4s - loss: 0.4823 - acc: 0.517 - ETA: 4s - loss: 0.4829 - acc: 0.517 - ETA: 3s - loss: 0.4795 - acc: 0.520 - ETA: 3s - loss: 0.4782 - acc: 0.521 - ETA: 2s - loss: 0.4761 - acc: 0.523 - ETA: 1s - loss: 0.4741 - acc: 0.525 - ETA: 1s - loss: 0.4783 - acc: 0.521 - ETA: 0s - loss: 0.4780 - acc: 0.522 - ETA: 0s - loss: 0.4778 - acc: 0.522 - 26s 21ms/sample - loss: 0.4772 - acc: 0.5228 - val_loss: 0.4394 - val_acc: 0.5606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ce4a55a550>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model8 = Sequential()\n",
    "\n",
    "NAME = \"Pokemon-model8_Network_Reduced-{}\".format(int(time.time()))\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))\n",
    "\n",
    "\n",
    "model8.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
    "model8.add(Activation('relu'))\n",
    "model8.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model8.add(Conv2D(256, (3, 3)))\n",
    "model8.add(Activation('relu'))\n",
    "model8.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model8.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "\n",
    "# reduced a layer over here to see the change\n",
    "\n",
    "model8.add(Dense(1))\n",
    "model8.add(Activation('sigmoid'))\n",
    "\n",
    "model8.compile(loss='mean_squared_error',\n",
    "              optimizer='adagrad',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model8.fit(X, y, batch_size=32, epochs=5, validation_split=0.3, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorBoard output for accuracy\n",
    "\n",
    "<img src=\"Images/TensorBoard/epoch_acc_model8.svg?sanitize=true\" width=\"500\">\n",
    "\n",
    "As we can observe, the accuracy plummets through the Epochs. This performance didnt favor much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 9 - Network Increased to 3 layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have added an additional layer to operate with. The activation function for the new layer has been softsign that we are still continuing to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1230 samples, validate on 528 samples\n",
      "Epoch 1/5\n",
      "1230/1230 [==============================] - ETA: 31s - loss: 0.2478 - acc: 0.56 - ETA: 25s - loss: 0.2049 - acc: 0.57 - ETA: 22s - loss: 0.2672 - acc: 0.56 - ETA: 21s - loss: 0.2615 - acc: 0.56 - ETA: 20s - loss: 0.2655 - acc: 0.57 - ETA: 19s - loss: 0.2717 - acc: 0.55 - ETA: 18s - loss: 0.2573 - acc: 0.58 - ETA: 17s - loss: 0.2623 - acc: 0.59 - ETA: 16s - loss: 0.2704 - acc: 0.57 - ETA: 16s - loss: 0.2684 - acc: 0.56 - ETA: 15s - loss: 0.2657 - acc: 0.57 - ETA: 14s - loss: 0.2691 - acc: 0.55 - ETA: 14s - loss: 0.2674 - acc: 0.56 - ETA: 13s - loss: 0.2666 - acc: 0.55 - ETA: 13s - loss: 0.2652 - acc: 0.55 - ETA: 12s - loss: 0.2641 - acc: 0.54 - ETA: 11s - loss: 0.2676 - acc: 0.53 - ETA: 11s - loss: 0.2666 - acc: 0.53 - ETA: 10s - loss: 0.2651 - acc: 0.53 - ETA: 10s - loss: 0.2651 - acc: 0.53 - ETA: 9s - loss: 0.2633 - acc: 0.5551 - ETA: 9s - loss: 0.2633 - acc: 0.554 - ETA: 8s - loss: 0.2613 - acc: 0.562 - ETA: 8s - loss: 0.2624 - acc: 0.559 - ETA: 7s - loss: 0.2631 - acc: 0.550 - ETA: 6s - loss: 0.2629 - acc: 0.549 - ETA: 6s - loss: 0.2620 - acc: 0.552 - ETA: 5s - loss: 0.2622 - acc: 0.549 - ETA: 5s - loss: 0.2616 - acc: 0.546 - ETA: 4s - loss: 0.2618 - acc: 0.537 - ETA: 4s - loss: 0.2624 - acc: 0.534 - ETA: 3s - loss: 0.2618 - acc: 0.535 - ETA: 3s - loss: 0.2614 - acc: 0.535 - ETA: 2s - loss: 0.2608 - acc: 0.544 - ETA: 1s - loss: 0.2601 - acc: 0.547 - ETA: 1s - loss: 0.2594 - acc: 0.547 - ETA: 0s - loss: 0.2590 - acc: 0.550 - ETA: 0s - loss: 0.2588 - acc: 0.549 - 26s 21ms/sample - loss: 0.2586 - acc: 0.5520 - val_loss: 0.2453 - val_acc: 0.5606\n",
      "Epoch 2/5\n",
      "1230/1230 [==============================] - ETA: 22s - loss: 0.2402 - acc: 0.59 - ETA: 20s - loss: 0.2340 - acc: 0.60 - ETA: 20s - loss: 0.2402 - acc: 0.56 - ETA: 19s - loss: 0.2362 - acc: 0.57 - ETA: 18s - loss: 0.2299 - acc: 0.55 - ETA: 18s - loss: 0.2235 - acc: 0.60 - ETA: 17s - loss: 0.2173 - acc: 0.60 - ETA: 17s - loss: 0.2132 - acc: 0.62 - ETA: 16s - loss: 0.2104 - acc: 0.61 - ETA: 16s - loss: 0.2083 - acc: 0.63 - ETA: 15s - loss: 0.2165 - acc: 0.60 - ETA: 14s - loss: 0.2165 - acc: 0.60 - ETA: 14s - loss: 0.2167 - acc: 0.60 - ETA: 13s - loss: 0.2153 - acc: 0.62 - ETA: 13s - loss: 0.2122 - acc: 0.64 - ETA: 12s - loss: 0.2099 - acc: 0.65 - ETA: 12s - loss: 0.2072 - acc: 0.67 - ETA: 11s - loss: 0.2064 - acc: 0.67 - ETA: 10s - loss: 0.2061 - acc: 0.67 - ETA: 10s - loss: 0.2039 - acc: 0.69 - ETA: 9s - loss: 0.2062 - acc: 0.6935 - ETA: 9s - loss: 0.2086 - acc: 0.684 - ETA: 8s - loss: 0.2072 - acc: 0.692 - ETA: 8s - loss: 0.2058 - acc: 0.701 - ETA: 7s - loss: 0.2041 - acc: 0.696 - ETA: 6s - loss: 0.2018 - acc: 0.703 - ETA: 6s - loss: 0.2025 - acc: 0.694 - ETA: 5s - loss: 0.2005 - acc: 0.703 - ETA: 5s - loss: 0.1988 - acc: 0.709 - ETA: 4s - loss: 0.1969 - acc: 0.714 - ETA: 4s - loss: 0.1947 - acc: 0.719 - ETA: 3s - loss: 0.1930 - acc: 0.727 - ETA: 3s - loss: 0.1921 - acc: 0.732 - ETA: 2s - loss: 0.1907 - acc: 0.736 - ETA: 1s - loss: 0.1880 - acc: 0.742 - ETA: 1s - loss: 0.1856 - acc: 0.746 - ETA: 0s - loss: 0.1845 - acc: 0.748 - ETA: 0s - loss: 0.1822 - acc: 0.753 - 25s 21ms/sample - loss: 0.1812 - acc: 0.7553 - val_loss: 0.1008 - val_acc: 0.8845\n",
      "Epoch 3/5\n",
      "1230/1230 [==============================] - ETA: 23s - loss: 0.0784 - acc: 0.93 - ETA: 21s - loss: 0.1844 - acc: 0.73 - ETA: 20s - loss: 0.1771 - acc: 0.81 - ETA: 19s - loss: 0.1884 - acc: 0.78 - ETA: 19s - loss: 0.1815 - acc: 0.74 - ETA: 18s - loss: 0.1748 - acc: 0.77 - ETA: 18s - loss: 0.1796 - acc: 0.74 - ETA: 17s - loss: 0.1925 - acc: 0.71 - ETA: 16s - loss: 0.1978 - acc: 0.68 - ETA: 16s - loss: 0.2069 - acc: 0.66 - ETA: 15s - loss: 0.2098 - acc: 0.64 - ETA: 15s - loss: 0.2115 - acc: 0.63 - ETA: 14s - loss: 0.2145 - acc: 0.62 - ETA: 14s - loss: 0.2144 - acc: 0.62 - ETA: 13s - loss: 0.2133 - acc: 0.63 - ETA: 13s - loss: 0.2124 - acc: 0.63 - ETA: 12s - loss: 0.2123 - acc: 0.64 - ETA: 11s - loss: 0.2121 - acc: 0.65 - ETA: 11s - loss: 0.2120 - acc: 0.65 - ETA: 10s - loss: 0.2096 - acc: 0.66 - ETA: 10s - loss: 0.2072 - acc: 0.68 - ETA: 9s - loss: 0.2069 - acc: 0.6761 - ETA: 8s - loss: 0.2052 - acc: 0.683 - ETA: 8s - loss: 0.2035 - acc: 0.688 - ETA: 7s - loss: 0.2002 - acc: 0.697 - ETA: 7s - loss: 0.1975 - acc: 0.705 - ETA: 6s - loss: 0.1951 - acc: 0.711 - ETA: 6s - loss: 0.1921 - acc: 0.718 - ETA: 5s - loss: 0.1902 - acc: 0.725 - ETA: 4s - loss: 0.1884 - acc: 0.730 - ETA: 4s - loss: 0.1867 - acc: 0.733 - ETA: 3s - loss: 0.1839 - acc: 0.740 - ETA: 3s - loss: 0.1805 - acc: 0.747 - ETA: 2s - loss: 0.1797 - acc: 0.749 - ETA: 2s - loss: 0.1778 - acc: 0.752 - ETA: 1s - loss: 0.1754 - acc: 0.756 - ETA: 0s - loss: 0.1724 - acc: 0.762 - ETA: 0s - loss: 0.1714 - acc: 0.764 - 26s 22ms/sample - loss: 0.1704 - acc: 0.7667 - val_loss: 0.1066 - val_acc: 0.8845\n",
      "Epoch 4/5\n",
      "1230/1230 [==============================] - ETA: 22s - loss: 0.1860 - acc: 0.75 - ETA: 20s - loss: 0.1457 - acc: 0.82 - ETA: 19s - loss: 0.1221 - acc: 0.86 - ETA: 19s - loss: 0.0999 - acc: 0.89 - ETA: 18s - loss: 0.1024 - acc: 0.89 - ETA: 17s - loss: 0.1005 - acc: 0.89 - ETA: 17s - loss: 0.1078 - acc: 0.88 - ETA: 16s - loss: 0.1053 - acc: 0.88 - ETA: 16s - loss: 0.1056 - acc: 0.88 - ETA: 15s - loss: 0.1039 - acc: 0.88 - ETA: 15s - loss: 0.0972 - acc: 0.89 - ETA: 14s - loss: 0.0944 - acc: 0.90 - ETA: 13s - loss: 0.0967 - acc: 0.89 - ETA: 13s - loss: 0.0945 - acc: 0.89 - ETA: 12s - loss: 0.1005 - acc: 0.88 - ETA: 12s - loss: 0.1019 - acc: 0.88 - ETA: 11s - loss: 0.0983 - acc: 0.89 - ETA: 11s - loss: 0.0966 - acc: 0.89 - ETA: 10s - loss: 0.0959 - acc: 0.89 - ETA: 10s - loss: 0.0960 - acc: 0.89 - ETA: 9s - loss: 0.0948 - acc: 0.8958 - ETA: 8s - loss: 0.0933 - acc: 0.897 - ETA: 8s - loss: 0.0943 - acc: 0.895 - ETA: 7s - loss: 0.0966 - acc: 0.890 - ETA: 7s - loss: 0.0974 - acc: 0.888 - ETA: 6s - loss: 0.1043 - acc: 0.873 - ETA: 6s - loss: 0.1043 - acc: 0.875 - ETA: 5s - loss: 0.1047 - acc: 0.875 - ETA: 5s - loss: 0.1047 - acc: 0.876 - ETA: 4s - loss: 0.1044 - acc: 0.877 - ETA: 4s - loss: 0.1042 - acc: 0.878 - ETA: 3s - loss: 0.1026 - acc: 0.880 - ETA: 2s - loss: 0.1023 - acc: 0.881 - ETA: 2s - loss: 0.1013 - acc: 0.883 - ETA: 1s - loss: 0.1004 - acc: 0.884 - ETA: 1s - loss: 0.1000 - acc: 0.885 - ETA: 0s - loss: 0.1000 - acc: 0.887 - ETA: 0s - loss: 0.0997 - acc: 0.889 - 25s 20ms/sample - loss: 0.0998 - acc: 0.8886 - val_loss: 0.1060 - val_acc: 0.8845\n",
      "Epoch 5/5\n",
      "1230/1230 [==============================] - ETA: 23s - loss: 0.1176 - acc: 0.87 - ETA: 21s - loss: 0.1616 - acc: 0.79 - ETA: 20s - loss: 0.1509 - acc: 0.81 - ETA: 19s - loss: 0.1436 - acc: 0.82 - ETA: 19s - loss: 0.1444 - acc: 0.76 - ETA: 18s - loss: 0.1354 - acc: 0.79 - ETA: 17s - loss: 0.1344 - acc: 0.82 - ETA: 17s - loss: 0.1332 - acc: 0.82 - ETA: 16s - loss: 0.1360 - acc: 0.82 - ETA: 16s - loss: 0.1318 - acc: 0.83 - ETA: 15s - loss: 0.1277 - acc: 0.83 - ETA: 14s - loss: 0.1220 - acc: 0.84 - ETA: 14s - loss: 0.1182 - acc: 0.85 - ETA: 13s - loss: 0.1160 - acc: 0.85 - ETA: 13s - loss: 0.1137 - acc: 0.86 - ETA: 12s - loss: 0.1120 - acc: 0.86 - ETA: 12s - loss: 0.1117 - acc: 0.86 - ETA: 11s - loss: 0.1090 - acc: 0.86 - ETA: 10s - loss: 0.1079 - acc: 0.87 - ETA: 10s - loss: 0.1056 - acc: 0.87 - ETA: 9s - loss: 0.1068 - acc: 0.8735 - ETA: 9s - loss: 0.1049 - acc: 0.876 - ETA: 8s - loss: 0.1048 - acc: 0.876 - ETA: 8s - loss: 0.1038 - acc: 0.877 - ETA: 7s - loss: 0.1020 - acc: 0.880 - ETA: 6s - loss: 0.1005 - acc: 0.882 - ETA: 6s - loss: 0.1008 - acc: 0.881 - ETA: 5s - loss: 0.1017 - acc: 0.880 - ETA: 5s - loss: 0.1004 - acc: 0.882 - ETA: 4s - loss: 0.1005 - acc: 0.882 - ETA: 4s - loss: 0.0998 - acc: 0.883 - ETA: 3s - loss: 0.0993 - acc: 0.883 - ETA: 3s - loss: 0.0995 - acc: 0.883 - ETA: 2s - loss: 0.0977 - acc: 0.886 - ETA: 1s - loss: 0.1010 - acc: 0.881 - ETA: 1s - loss: 0.1004 - acc: 0.881 - ETA: 0s - loss: 0.0990 - acc: 0.884 - ETA: 0s - loss: 0.0985 - acc: 0.884 - 25s 21ms/sample - loss: 0.0977 - acc: 0.8862 - val_loss: 0.0967 - val_acc: 0.8845\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ce43512908>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model9 = Sequential()\n",
    "\n",
    "NAME = \"Pokemon-model9_Network_Increased3-{}\".format(int(time.time()))\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))\n",
    "\n",
    "\n",
    "model9.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
    "model9.add(Activation('relu'))\n",
    "model9.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model9.add(Conv2D(256, (3, 3)))\n",
    "model9.add(Activation('relu'))\n",
    "model9.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model9.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "\n",
    "# reduced a layer over here to see the change\n",
    "model9.add(Dense(64))\n",
    "model9.add(Activation('softsign'))\n",
    "\n",
    "model9.add(Dense(64))\n",
    "model9.add(Activation('softsign'))\n",
    "\n",
    "model9.add(Dense(64))\n",
    "model9.add(Activation('softsign'))\n",
    "\n",
    "model9.add(Dense(1))\n",
    "model9.add(Activation('sigmoid'))\n",
    "\n",
    "model9.compile(loss='mean_squared_error',\n",
    "              optimizer='adagrad',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model9.fit(X, y, batch_size=32, epochs=5, validation_split=0.3, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorBoard output for the accuracy\n",
    "\n",
    "<img src=\"Images/TensorBoard/epoch_acc_model10.svg?sanitize=true\" width=\"500\">\n",
    "\n",
    "As we observe, the accuracy increases strongly by starting from an average point of `0.66`\n",
    "\n",
    "Now, lets compare between the 2 models:\n",
    "* Model 8 - Having only one layer\n",
    "* Model 9 - Having 2 layers including the final layer with `softsign` activation function\n",
    "\n",
    "<img src=\"Images/image4.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Part G` - Network Initialization\n",
    "\n",
    "##### Why’s Xavier initialization important?\n",
    "In short, it helps signals reach deep into the network.\n",
    "\n",
    "If the weights in a network start too small, then the signal shrinks as it passes through each layer until it’s too tiny to be useful.\n",
    "If the weights in a network start too large, then the signal grows as it passes through each layer until it’s too massive to be useful.\n",
    "Xavier initialization makes sure the weights are ‘just right’, keeping the signal in a reasonable range of values through many layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 10 - Xavier Uniform Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1230 samples, validate on 528 samples\n",
      "Epoch 1/5\n",
      "1230/1230 [==============================] - ETA: 18s - loss: 0.6932 - acc: 0.37 - ETA: 10s - loss: 0.6933 - acc: 0.39 - ETA: 7s - loss: 0.6926 - acc: 0.5000 - ETA: 6s - loss: 0.6925 - acc: 0.507 - ETA: 5s - loss: 0.6919 - acc: 0.518 - ETA: 4s - loss: 0.6927 - acc: 0.510 - ETA: 4s - loss: 0.6944 - acc: 0.495 - ETA: 4s - loss: 0.6966 - acc: 0.472 - ETA: 3s - loss: 0.6961 - acc: 0.475 - ETA: 3s - loss: 0.6957 - acc: 0.478 - ETA: 3s - loss: 0.6957 - acc: 0.468 - ETA: 3s - loss: 0.6956 - acc: 0.463 - ETA: 3s - loss: 0.6954 - acc: 0.456 - ETA: 2s - loss: 0.6952 - acc: 0.473 - ETA: 2s - loss: 0.6951 - acc: 0.468 - ETA: 2s - loss: 0.6949 - acc: 0.474 - ETA: 2s - loss: 0.6948 - acc: 0.479 - ETA: 2s - loss: 0.6947 - acc: 0.480 - ETA: 2s - loss: 0.6947 - acc: 0.472 - ETA: 2s - loss: 0.6946 - acc: 0.473 - ETA: 1s - loss: 0.6945 - acc: 0.474 - ETA: 1s - loss: 0.6943 - acc: 0.481 - ETA: 1s - loss: 0.6943 - acc: 0.482 - ETA: 1s - loss: 0.6941 - acc: 0.487 - ETA: 1s - loss: 0.6939 - acc: 0.488 - ETA: 1s - loss: 0.6936 - acc: 0.492 - ETA: 1s - loss: 0.6937 - acc: 0.490 - ETA: 1s - loss: 0.6939 - acc: 0.487 - ETA: 0s - loss: 0.6937 - acc: 0.489 - ETA: 0s - loss: 0.6937 - acc: 0.488 - ETA: 0s - loss: 0.6935 - acc: 0.488 - ETA: 0s - loss: 0.6936 - acc: 0.485 - ETA: 0s - loss: 0.6932 - acc: 0.491 - ETA: 0s - loss: 0.6931 - acc: 0.492 - ETA: 0s - loss: 0.6929 - acc: 0.491 - ETA: 0s - loss: 0.6928 - acc: 0.490 - ETA: 0s - loss: 0.6924 - acc: 0.493 - ETA: 0s - loss: 0.6918 - acc: 0.498 - 5s 4ms/sample - loss: 0.6914 - acc: 0.5008 - val_loss: 0.6733 - val_acc: 0.5606\n",
      "Epoch 2/5\n",
      "1230/1230 [==============================] - ETA: 3s - loss: 0.6862 - acc: 0.500 - ETA: 3s - loss: 0.6833 - acc: 0.515 - ETA: 2s - loss: 0.6837 - acc: 0.520 - ETA: 2s - loss: 0.6825 - acc: 0.523 - ETA: 2s - loss: 0.6891 - acc: 0.506 - ETA: 2s - loss: 0.6880 - acc: 0.505 - ETA: 2s - loss: 0.6859 - acc: 0.508 - ETA: 2s - loss: 0.6850 - acc: 0.503 - ETA: 2s - loss: 0.6838 - acc: 0.496 - ETA: 2s - loss: 0.6827 - acc: 0.509 - ETA: 2s - loss: 0.6823 - acc: 0.505 - ETA: 2s - loss: 0.6820 - acc: 0.497 - ETA: 2s - loss: 0.6808 - acc: 0.497 - ETA: 2s - loss: 0.6799 - acc: 0.495 - ETA: 2s - loss: 0.6773 - acc: 0.502 - ETA: 1s - loss: 0.6775 - acc: 0.498 - ETA: 1s - loss: 0.6753 - acc: 0.501 - ETA: 1s - loss: 0.6738 - acc: 0.503 - ETA: 1s - loss: 0.6714 - acc: 0.508 - ETA: 1s - loss: 0.6701 - acc: 0.506 - ETA: 1s - loss: 0.6690 - acc: 0.506 - ETA: 1s - loss: 0.6680 - acc: 0.507 - ETA: 1s - loss: 0.6672 - acc: 0.506 - ETA: 1s - loss: 0.6642 - acc: 0.514 - ETA: 1s - loss: 0.6634 - acc: 0.511 - ETA: 1s - loss: 0.6635 - acc: 0.508 - ETA: 1s - loss: 0.6607 - acc: 0.511 - ETA: 1s - loss: 0.6599 - acc: 0.505 - ETA: 0s - loss: 0.6574 - acc: 0.511 - ETA: 0s - loss: 0.6552 - acc: 0.524 - ETA: 0s - loss: 0.6520 - acc: 0.527 - ETA: 0s - loss: 0.6502 - acc: 0.526 - ETA: 0s - loss: 0.6471 - acc: 0.529 - ETA: 0s - loss: 0.6432 - acc: 0.532 - ETA: 0s - loss: 0.6393 - acc: 0.535 - ETA: 0s - loss: 0.6371 - acc: 0.535 - ETA: 0s - loss: 0.6346 - acc: 0.537 - ETA: 0s - loss: 0.6322 - acc: 0.541 - 4s 4ms/sample - loss: 0.6308 - acc: 0.5431 - val_loss: 0.5182 - val_acc: 0.6496\n",
      "Epoch 3/5\n",
      "1230/1230 [==============================] - ETA: 3s - loss: 0.4617 - acc: 0.656 - ETA: 3s - loss: 0.4962 - acc: 0.593 - ETA: 3s - loss: 0.4923 - acc: 0.572 - ETA: 3s - loss: 0.5296 - acc: 0.515 - ETA: 3s - loss: 0.5245 - acc: 0.562 - ETA: 3s - loss: 0.5187 - acc: 0.619 - ETA: 3s - loss: 0.5096 - acc: 0.656 - ETA: 2s - loss: 0.5023 - acc: 0.671 - ETA: 2s - loss: 0.5154 - acc: 0.642 - ETA: 2s - loss: 0.5141 - acc: 0.637 - ETA: 2s - loss: 0.5151 - acc: 0.633 - ETA: 2s - loss: 0.5169 - acc: 0.632 - ETA: 2s - loss: 0.5133 - acc: 0.651 - ETA: 2s - loss: 0.5107 - acc: 0.669 - ETA: 2s - loss: 0.5087 - acc: 0.689 - ETA: 2s - loss: 0.5101 - acc: 0.703 - ETA: 2s - loss: 0.5037 - acc: 0.707 - ETA: 1s - loss: 0.4997 - acc: 0.704 - ETA: 1s - loss: 0.5004 - acc: 0.699 - ETA: 1s - loss: 0.4973 - acc: 0.704 - ETA: 1s - loss: 0.4942 - acc: 0.712 - ETA: 1s - loss: 0.4922 - acc: 0.723 - ETA: 1s - loss: 0.4888 - acc: 0.728 - ETA: 1s - loss: 0.4842 - acc: 0.734 - ETA: 1s - loss: 0.4803 - acc: 0.740 - ETA: 1s - loss: 0.4758 - acc: 0.744 - ETA: 1s - loss: 0.4728 - acc: 0.746 - ETA: 1s - loss: 0.4699 - acc: 0.752 - ETA: 0s - loss: 0.4640 - acc: 0.758 - ETA: 0s - loss: 0.4573 - acc: 0.765 - ETA: 0s - loss: 0.4548 - acc: 0.769 - ETA: 0s - loss: 0.4511 - acc: 0.773 - ETA: 0s - loss: 0.4468 - acc: 0.777 - ETA: 0s - loss: 0.4390 - acc: 0.784 - ETA: 0s - loss: 0.4344 - acc: 0.787 - ETA: 0s - loss: 0.4319 - acc: 0.789 - ETA: 0s - loss: 0.4257 - acc: 0.793 - ETA: 0s - loss: 0.4258 - acc: 0.794 - 4s 3ms/sample - loss: 0.4245 - acc: 0.7951 - val_loss: 0.2908 - val_acc: 0.8731\n",
      "Epoch 4/5\n",
      "1230/1230 [==============================] - ETA: 3s - loss: 0.4055 - acc: 0.781 - ETA: 3s - loss: 0.3641 - acc: 0.812 - ETA: 3s - loss: 0.3567 - acc: 0.812 - ETA: 3s - loss: 0.3253 - acc: 0.828 - ETA: 3s - loss: 0.3395 - acc: 0.831 - ETA: 2s - loss: 0.3294 - acc: 0.843 - ETA: 2s - loss: 0.3227 - acc: 0.852 - ETA: 2s - loss: 0.2928 - acc: 0.871 - ETA: 2s - loss: 0.2873 - acc: 0.878 - ETA: 2s - loss: 0.2820 - acc: 0.881 - ETA: 2s - loss: 0.2700 - acc: 0.889 - ETA: 2s - loss: 0.2792 - acc: 0.885 - ETA: 2s - loss: 0.2786 - acc: 0.882 - ETA: 2s - loss: 0.2724 - acc: 0.888 - ETA: 2s - loss: 0.2749 - acc: 0.889 - ETA: 2s - loss: 0.2803 - acc: 0.886 - ETA: 2s - loss: 0.2799 - acc: 0.887 - ETA: 1s - loss: 0.2904 - acc: 0.883 - ETA: 1s - loss: 0.2959 - acc: 0.881 - ETA: 1s - loss: 0.2884 - acc: 0.885 - ETA: 1s - loss: 0.2881 - acc: 0.885 - ETA: 1s - loss: 0.2839 - acc: 0.887 - ETA: 1s - loss: 0.2792 - acc: 0.891 - ETA: 1s - loss: 0.2797 - acc: 0.891 - ETA: 1s - loss: 0.2766 - acc: 0.893 - ETA: 1s - loss: 0.2733 - acc: 0.895 - ETA: 1s - loss: 0.2714 - acc: 0.897 - ETA: 0s - loss: 0.2722 - acc: 0.896 - ETA: 0s - loss: 0.2741 - acc: 0.895 - ETA: 0s - loss: 0.2771 - acc: 0.893 - ETA: 0s - loss: 0.2871 - acc: 0.888 - ETA: 0s - loss: 0.2826 - acc: 0.890 - ETA: 0s - loss: 0.2799 - acc: 0.893 - ETA: 0s - loss: 0.2771 - acc: 0.894 - ETA: 0s - loss: 0.2760 - acc: 0.894 - ETA: 0s - loss: 0.2762 - acc: 0.894 - ETA: 0s - loss: 0.2779 - acc: 0.892 - ETA: 0s - loss: 0.2808 - acc: 0.891 - 4s 3ms/sample - loss: 0.2801 - acc: 0.8919 - val_loss: 0.2994 - val_acc: 0.8845\n",
      "Epoch 5/5\n",
      "1230/1230 [==============================] - ETA: 4s - loss: 0.4032 - acc: 0.812 - ETA: 3s - loss: 0.3713 - acc: 0.843 - ETA: 3s - loss: 0.3017 - acc: 0.885 - ETA: 3s - loss: 0.2940 - acc: 0.890 - ETA: 3s - loss: 0.2887 - acc: 0.893 - ETA: 3s - loss: 0.2882 - acc: 0.895 - ETA: 3s - loss: 0.2893 - acc: 0.897 - ETA: 3s - loss: 0.3000 - acc: 0.890 - ETA: 2s - loss: 0.2912 - acc: 0.895 - ETA: 2s - loss: 0.3024 - acc: 0.884 - ETA: 2s - loss: 0.2990 - acc: 0.886 - ETA: 2s - loss: 0.3062 - acc: 0.877 - ETA: 2s - loss: 0.3119 - acc: 0.875 - ETA: 2s - loss: 0.3105 - acc: 0.877 - ETA: 2s - loss: 0.3087 - acc: 0.877 - ETA: 2s - loss: 0.3079 - acc: 0.878 - ETA: 2s - loss: 0.3070 - acc: 0.878 - ETA: 2s - loss: 0.3035 - acc: 0.880 - ETA: 1s - loss: 0.2970 - acc: 0.883 - ETA: 1s - loss: 0.2913 - acc: 0.885 - ETA: 1s - loss: 0.2872 - acc: 0.888 - ETA: 1s - loss: 0.2798 - acc: 0.892 - ETA: 1s - loss: 0.2774 - acc: 0.892 - ETA: 1s - loss: 0.2716 - acc: 0.895 - ETA: 1s - loss: 0.2760 - acc: 0.893 - ETA: 1s - loss: 0.2799 - acc: 0.891 - ETA: 1s - loss: 0.2754 - acc: 0.894 - ETA: 1s - loss: 0.2709 - acc: 0.896 - ETA: 0s - loss: 0.2700 - acc: 0.897 - ETA: 0s - loss: 0.2750 - acc: 0.893 - ETA: 0s - loss: 0.2752 - acc: 0.893 - ETA: 0s - loss: 0.2711 - acc: 0.894 - ETA: 0s - loss: 0.2682 - acc: 0.896 - ETA: 0s - loss: 0.2634 - acc: 0.899 - ETA: 0s - loss: 0.2610 - acc: 0.900 - ETA: 0s - loss: 0.2601 - acc: 0.901 - ETA: 0s - loss: 0.2625 - acc: 0.900 - ETA: 0s - loss: 0.2631 - acc: 0.900 - 4s 3ms/sample - loss: 0.2625 - acc: 0.9008 - val_loss: 0.2957 - val_acc: 0.8845\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ce44c6a390>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X/255.0\n",
    "\n",
    "NAME = \"Pokemon-model10-Unifor-Initialization-{}\".format(int(time.time()))\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))\n",
    "\n",
    "model10 = Sequential()\n",
    "\n",
    "# Conv2D in Convelution for a 2D image\n",
    "#3x3 in the window we are using\n",
    "model10.add(Conv2D(64, (3, 3), input_shape=X.shape[1:]))\n",
    "model10.add(Activation('relu'))\n",
    "#We do the pooling as size 2x2\n",
    "model10.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#We do it again, but dont need shapping\n",
    "model10.add(Conv2D(64, (3, 3)))\n",
    "model10.add(Activation('relu'))\n",
    "model10.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#Convolution is 2D so we flatten the data to compress it to understandable\n",
    "model10.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "\n",
    "model10.add(Dense(64, kernel_initializer='glorot_uniform'))\n",
    "model10.add(Activation('relu'))\n",
    "\n",
    "model10.add(Dense(1))\n",
    "model10.add(Activation('sigmoid'))\n",
    "\n",
    "model10.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model10.fit(X, y, batch_size=32, epochs=5, validation_split=0.3, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorBoard output for accuracy\n",
    "\n",
    "<img src=\"Images/TensorBoard/epoch_acc_model11.svg?sanitize=true\" width=\"500\">\n",
    "\n",
    "From the graph we can infer, that the acuracy increases with the Epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 11 - Zero Initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1230 samples, validate on 528 samples\n",
      "Epoch 1/5\n",
      "1230/1230 [==============================] - ETA: 3s - loss: 3.0503 - acc: 0.500 - ETA: 3s - loss: 2.1841 - acc: 0.531 - ETA: 3s - loss: 1.7553 - acc: 0.520 - ETA: 2s - loss: 1.5542 - acc: 0.539 - ETA: 2s - loss: 1.5429 - acc: 0.500 - ETA: 2s - loss: 1.4472 - acc: 0.515 - ETA: 2s - loss: 1.4599 - acc: 0.486 - ETA: 2s - loss: 1.4015 - acc: 0.496 - ETA: 2s - loss: 1.3785 - acc: 0.489 - ETA: 2s - loss: 1.3330 - acc: 0.496 - ETA: 2s - loss: 1.2926 - acc: 0.502 - ETA: 2s - loss: 1.2792 - acc: 0.492 - ETA: 2s - loss: 1.2599 - acc: 0.485 - ETA: 2s - loss: 1.2287 - acc: 0.488 - ETA: 2s - loss: 1.1995 - acc: 0.491 - ETA: 1s - loss: 1.1740 - acc: 0.492 - ETA: 1s - loss: 1.1552 - acc: 0.485 - ETA: 1s - loss: 1.1340 - acc: 0.482 - ETA: 1s - loss: 1.1141 - acc: 0.478 - ETA: 1s - loss: 1.0931 - acc: 0.481 - ETA: 1s - loss: 1.0729 - acc: 0.491 - ETA: 1s - loss: 1.0559 - acc: 0.488 - ETA: 1s - loss: 1.0401 - acc: 0.489 - ETA: 1s - loss: 1.0256 - acc: 0.492 - ETA: 1s - loss: 1.0124 - acc: 0.487 - ETA: 1s - loss: 1.0001 - acc: 0.489 - ETA: 0s - loss: 0.9888 - acc: 0.486 - ETA: 0s - loss: 0.9784 - acc: 0.481 - ETA: 0s - loss: 0.9687 - acc: 0.477 - ETA: 0s - loss: 0.9596 - acc: 0.475 - ETA: 0s - loss: 0.9510 - acc: 0.474 - ETA: 0s - loss: 0.9429 - acc: 0.478 - ETA: 0s - loss: 0.9354 - acc: 0.477 - ETA: 0s - loss: 0.9282 - acc: 0.478 - ETA: 0s - loss: 0.9214 - acc: 0.484 - ETA: 0s - loss: 0.9151 - acc: 0.483 - ETA: 0s - loss: 0.9091 - acc: 0.484 - ETA: 0s - loss: 0.9035 - acc: 0.480 - 4s 3ms/sample - loss: 0.9011 - acc: 0.4805 - val_loss: 0.6942 - val_acc: 0.4394\n",
      "Epoch 2/5\n",
      "1230/1230 [==============================] - ETA: 3s - loss: 0.6948 - acc: 0.406 - ETA: 3s - loss: 0.6935 - acc: 0.484 - ETA: 3s - loss: 0.6930 - acc: 0.510 - ETA: 3s - loss: 0.6932 - acc: 0.500 - ETA: 3s - loss: 0.6932 - acc: 0.500 - ETA: 3s - loss: 0.6933 - acc: 0.494 - ETA: 2s - loss: 0.6934 - acc: 0.486 - ETA: 2s - loss: 0.6936 - acc: 0.472 - ETA: 2s - loss: 0.6936 - acc: 0.472 - ETA: 2s - loss: 0.6937 - acc: 0.465 - ETA: 2s - loss: 0.6938 - acc: 0.457 - ETA: 2s - loss: 0.6936 - acc: 0.474 - ETA: 2s - loss: 0.6935 - acc: 0.480 - ETA: 2s - loss: 0.6935 - acc: 0.477 - ETA: 2s - loss: 0.6936 - acc: 0.475 - ETA: 1s - loss: 0.6935 - acc: 0.478 - ETA: 1s - loss: 0.6936 - acc: 0.474 - ETA: 1s - loss: 0.6935 - acc: 0.475 - ETA: 1s - loss: 0.6936 - acc: 0.470 - ETA: 1s - loss: 0.6936 - acc: 0.467 - ETA: 1s - loss: 0.6936 - acc: 0.471 - ETA: 1s - loss: 0.6935 - acc: 0.477 - ETA: 1s - loss: 0.6936 - acc: 0.470 - ETA: 1s - loss: 0.6935 - acc: 0.475 - ETA: 1s - loss: 0.6935 - acc: 0.477 - ETA: 1s - loss: 0.6935 - acc: 0.480 - ETA: 1s - loss: 0.6934 - acc: 0.485 - ETA: 0s - loss: 0.6934 - acc: 0.487 - ETA: 0s - loss: 0.6934 - acc: 0.486 - ETA: 0s - loss: 0.6934 - acc: 0.487 - ETA: 0s - loss: 0.6934 - acc: 0.484 - ETA: 0s - loss: 0.6934 - acc: 0.481 - ETA: 0s - loss: 0.6935 - acc: 0.479 - ETA: 0s - loss: 0.6934 - acc: 0.479 - ETA: 0s - loss: 0.6934 - acc: 0.479 - ETA: 0s - loss: 0.6934 - acc: 0.480 - ETA: 0s - loss: 0.6935 - acc: 0.478 - ETA: 0s - loss: 0.6935 - acc: 0.477 - 4s 3ms/sample - loss: 0.6935 - acc: 0.4772 - val_loss: 0.6937 - val_acc: 0.4394\n",
      "Epoch 3/5\n",
      "1230/1230 [==============================] - ETA: 3s - loss: 0.6926 - acc: 0.562 - ETA: 3s - loss: 0.6934 - acc: 0.468 - ETA: 3s - loss: 0.6932 - acc: 0.489 - ETA: 3s - loss: 0.6930 - acc: 0.523 - ETA: 3s - loss: 0.6931 - acc: 0.512 - ETA: 3s - loss: 0.6931 - acc: 0.510 - ETA: 3s - loss: 0.6930 - acc: 0.522 - ETA: 3s - loss: 0.6932 - acc: 0.492 - ETA: 2s - loss: 0.6933 - acc: 0.486 - ETA: 2s - loss: 0.6932 - acc: 0.487 - ETA: 2s - loss: 0.6931 - acc: 0.502 - ETA: 2s - loss: 0.6932 - acc: 0.497 - ETA: 2s - loss: 0.6932 - acc: 0.492 - ETA: 2s - loss: 0.6932 - acc: 0.500 - ETA: 2s - loss: 0.6931 - acc: 0.506 - ETA: 2s - loss: 0.6931 - acc: 0.502 - ETA: 2s - loss: 0.6931 - acc: 0.503 - ETA: 2s - loss: 0.6932 - acc: 0.498 - ETA: 1s - loss: 0.6931 - acc: 0.501 - ETA: 1s - loss: 0.6932 - acc: 0.496 - ETA: 1s - loss: 0.6932 - acc: 0.489 - ETA: 1s - loss: 0.6932 - acc: 0.491 - ETA: 1s - loss: 0.6932 - acc: 0.493 - ETA: 1s - loss: 0.6932 - acc: 0.497 - ETA: 1s - loss: 0.6932 - acc: 0.498 - ETA: 1s - loss: 0.6932 - acc: 0.496 - ETA: 1s - loss: 0.6932 - acc: 0.490 - ETA: 1s - loss: 0.6932 - acc: 0.488 - ETA: 0s - loss: 0.6932 - acc: 0.483 - ETA: 0s - loss: 0.6932 - acc: 0.482 - ETA: 0s - loss: 0.6932 - acc: 0.482 - ETA: 0s - loss: 0.6932 - acc: 0.484 - ETA: 0s - loss: 0.6932 - acc: 0.487 - ETA: 0s - loss: 0.6932 - acc: 0.482 - ETA: 0s - loss: 0.6933 - acc: 0.478 - ETA: 0s - loss: 0.6933 - acc: 0.478 - ETA: 0s - loss: 0.6932 - acc: 0.479 - ETA: 0s - loss: 0.6932 - acc: 0.481 - 4s 4ms/sample - loss: 0.6933 - acc: 0.4772 - val_loss: 0.6932 - val_acc: 0.4394\n",
      "Epoch 4/5\n",
      "1230/1230 [==============================] - ETA: 3s - loss: 0.6936 - acc: 0.218 - ETA: 3s - loss: 0.6934 - acc: 0.359 - ETA: 3s - loss: 0.6933 - acc: 0.427 - ETA: 3s - loss: 0.6932 - acc: 0.523 - ETA: 3s - loss: 0.6932 - acc: 0.525 - ETA: 3s - loss: 0.6932 - acc: 0.531 - ETA: 3s - loss: 0.6932 - acc: 0.526 - ETA: 2s - loss: 0.6932 - acc: 0.527 - ETA: 2s - loss: 0.6931 - acc: 0.534 - ETA: 2s - loss: 0.6931 - acc: 0.528 - ETA: 2s - loss: 0.6932 - acc: 0.508 - ETA: 2s - loss: 0.6932 - acc: 0.500 - ETA: 2s - loss: 0.6932 - acc: 0.507 - ETA: 2s - loss: 0.6932 - acc: 0.506 - ETA: 2s - loss: 0.6931 - acc: 0.518 - ETA: 2s - loss: 0.6931 - acc: 0.515 - ETA: 2s - loss: 0.6931 - acc: 0.520 - ETA: 1s - loss: 0.6931 - acc: 0.522 - ETA: 1s - loss: 0.6930 - acc: 0.534 - ETA: 1s - loss: 0.6930 - acc: 0.535 - ETA: 1s - loss: 0.6930 - acc: 0.538 - ETA: 1s - loss: 0.6930 - acc: 0.532 - ETA: 1s - loss: 0.6930 - acc: 0.531 - ETA: 1s - loss: 0.6930 - acc: 0.533 - ETA: 1s - loss: 0.6930 - acc: 0.528 - ETA: 1s - loss: 0.6931 - acc: 0.526 - ETA: 1s - loss: 0.6931 - acc: 0.524 - ETA: 0s - loss: 0.6931 - acc: 0.521 - ETA: 0s - loss: 0.6931 - acc: 0.519 - ETA: 0s - loss: 0.6931 - acc: 0.520 - ETA: 0s - loss: 0.6931 - acc: 0.523 - ETA: 0s - loss: 0.6931 - acc: 0.523 - ETA: 0s - loss: 0.6931 - acc: 0.518 - ETA: 0s - loss: 0.6931 - acc: 0.516 - ETA: 0s - loss: 0.6931 - acc: 0.517 - ETA: 0s - loss: 0.6931 - acc: 0.513 - ETA: 0s - loss: 0.6932 - acc: 0.511 - ETA: 0s - loss: 0.6932 - acc: 0.511 - 4s 3ms/sample - loss: 0.6932 - acc: 0.5114 - val_loss: 0.6926 - val_acc: 0.5606\n",
      "Epoch 5/5\n",
      "1230/1230 [==============================] - ETA: 3s - loss: 0.6919 - acc: 0.625 - ETA: 3s - loss: 0.6938 - acc: 0.437 - ETA: 3s - loss: 0.6932 - acc: 0.500 - ETA: 3s - loss: 0.6934 - acc: 0.476 - ETA: 3s - loss: 0.6937 - acc: 0.443 - ETA: 2s - loss: 0.6936 - acc: 0.458 - ETA: 2s - loss: 0.6933 - acc: 0.482 - ETA: 2s - loss: 0.6933 - acc: 0.484 - ETA: 2s - loss: 0.6933 - acc: 0.482 - ETA: 2s - loss: 0.6932 - acc: 0.496 - ETA: 2s - loss: 0.6932 - acc: 0.502 - ETA: 2s - loss: 0.6931 - acc: 0.513 - ETA: 2s - loss: 0.6931 - acc: 0.504 - ETA: 2s - loss: 0.6931 - acc: 0.504 - ETA: 2s - loss: 0.6932 - acc: 0.495 - ETA: 2s - loss: 0.6932 - acc: 0.498 - ETA: 1s - loss: 0.6931 - acc: 0.503 - ETA: 1s - loss: 0.6930 - acc: 0.513 - ETA: 1s - loss: 0.6931 - acc: 0.513 - ETA: 1s - loss: 0.6931 - acc: 0.507 - ETA: 1s - loss: 0.6931 - acc: 0.511 - ETA: 1s - loss: 0.6931 - acc: 0.511 - ETA: 1s - loss: 0.6931 - acc: 0.509 - ETA: 1s - loss: 0.6930 - acc: 0.513 - ETA: 1s - loss: 0.6930 - acc: 0.516 - ETA: 1s - loss: 0.6930 - acc: 0.520 - ETA: 1s - loss: 0.6930 - acc: 0.522 - ETA: 0s - loss: 0.6929 - acc: 0.525 - ETA: 0s - loss: 0.6929 - acc: 0.525 - ETA: 0s - loss: 0.6929 - acc: 0.526 - ETA: 0s - loss: 0.6929 - acc: 0.527 - ETA: 0s - loss: 0.6929 - acc: 0.526 - ETA: 0s - loss: 0.6929 - acc: 0.524 - ETA: 0s - loss: 0.6929 - acc: 0.529 - ETA: 0s - loss: 0.6929 - acc: 0.525 - ETA: 0s - loss: 0.6929 - acc: 0.523 - ETA: 0s - loss: 0.6929 - acc: 0.522 - ETA: 0s - loss: 0.6929 - acc: 0.523 - 4s 3ms/sample - loss: 0.6929 - acc: 0.5228 - val_loss: 0.6923 - val_acc: 0.5606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ce4e9acef0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X/255.0\n",
    "\n",
    "NAME = \"Pokemon-model11-Zero-Initialization-{}\".format(int(time.time()))\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))\n",
    "\n",
    "model11 = Sequential()\n",
    "\n",
    "# Conv2D in Convelution for a 2D image\n",
    "#3x3 in the window we are using\n",
    "model11.add(Conv2D(64, (3, 3), input_shape=X.shape[1:]))\n",
    "model11.add(Activation('relu'))\n",
    "#We do the pooling as size 2x2\n",
    "model11.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#We do it again, but dont need shapping\n",
    "model11.add(Conv2D(64, (3, 3)))\n",
    "model11.add(Activation('relu'))\n",
    "model11.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#Convolution is 2D so we flatten the data to compress it to understandable\n",
    "model11.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "\n",
    "model11.add(Dense(64, kernel_initializer='zero'))\n",
    "model11.add(Activation('relu'))\n",
    "\n",
    "model11.add(Dense(1))\n",
    "model11.add(Activation('sigmoid'))\n",
    "\n",
    "model11.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model10.fit(X, y, batch_size=32, epochs=5, validation_split=0.3, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorBoard output for accuracy\n",
    "\n",
    "<img src=\"Images/TensorBoard/epoch_acc_model12.svg?sanitize=true\" width=\"500\">\n",
    "\n",
    "Seems, the accuracy increases after the 1st Epoch. Even though it starts from a bad accuracy of `0.48`\n",
    "\n",
    "Now, lets compare both Model10 and Model11 side by side to see significance difference in model performance of Xavier Uniform optimizer and Zero weight optimizer.\n",
    "\n",
    "<img src=\"Images/image5.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks, better in taking the Model 10th as it performs in a better way compared to Zero weight optimizer. Now, as we have generated many models, lets create a testing scenario for our model. THe best model that turned out to be in our case till now is Model1 which used ReLU. We can save the model for further use, and predict on our test data that has been taken fron an internet source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model and trying to predict on a realworld image of a Pokemone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-conv-64-nodes-0-dense-1554326408\n",
      "Train on 1230 samples, validate on 528 samples\n",
      "Epoch 1/10\n",
      "1230/1230 [==============================] - ETA: 19s - loss: 0.7185 - acc: 0.40 - ETA: 11s - loss: 0.7032 - acc: 0.40 - ETA: 8s - loss: 0.6906 - acc: 0.4479 - ETA: 6s - loss: 0.6861 - acc: 0.453 - ETA: 5s - loss: 0.6821 - acc: 0.443 - ETA: 5s - loss: 0.6726 - acc: 0.468 - ETA: 4s - loss: 0.6671 - acc: 0.508 - ETA: 4s - loss: 0.6596 - acc: 0.550 - ETA: 4s - loss: 0.6514 - acc: 0.586 - ETA: 3s - loss: 0.6405 - acc: 0.612 - ETA: 3s - loss: 0.6398 - acc: 0.590 - ETA: 3s - loss: 0.6314 - acc: 0.609 - ETA: 3s - loss: 0.6192 - acc: 0.632 - ETA: 2s - loss: 0.6067 - acc: 0.649 - ETA: 2s - loss: 0.5963 - acc: 0.664 - ETA: 2s - loss: 0.5820 - acc: 0.679 - ETA: 2s - loss: 0.5705 - acc: 0.691 - ETA: 2s - loss: 0.5610 - acc: 0.699 - ETA: 2s - loss: 0.5486 - acc: 0.710 - ETA: 2s - loss: 0.5417 - acc: 0.717 - ETA: 1s - loss: 0.5281 - acc: 0.726 - ETA: 1s - loss: 0.5188 - acc: 0.731 - ETA: 1s - loss: 0.5077 - acc: 0.737 - ETA: 1s - loss: 0.4936 - acc: 0.746 - ETA: 1s - loss: 0.4876 - acc: 0.751 - ETA: 1s - loss: 0.4782 - acc: 0.757 - ETA: 1s - loss: 0.4674 - acc: 0.765 - ETA: 1s - loss: 0.4599 - acc: 0.770 - ETA: 1s - loss: 0.4497 - acc: 0.775 - ETA: 0s - loss: 0.4449 - acc: 0.779 - ETA: 0s - loss: 0.4356 - acc: 0.785 - ETA: 0s - loss: 0.4249 - acc: 0.791 - ETA: 0s - loss: 0.4178 - acc: 0.794 - ETA: 0s - loss: 0.4096 - acc: 0.799 - ETA: 0s - loss: 0.4036 - acc: 0.802 - ETA: 0s - loss: 0.3957 - acc: 0.807 - ETA: 0s - loss: 0.3900 - acc: 0.809 - ETA: 0s - loss: 0.3857 - acc: 0.812 - 5s 4ms/sample - loss: 0.3818 - acc: 0.8146 - val_loss: 0.2532 - val_acc: 0.9091\n",
      "Epoch 2/10\n",
      "1230/1230 [==============================] - ETA: 3s - loss: 0.1266 - acc: 0.968 - ETA: 3s - loss: 0.2254 - acc: 0.921 - ETA: 3s - loss: 0.2049 - acc: 0.927 - ETA: 3s - loss: 0.2096 - acc: 0.929 - ETA: 3s - loss: 0.2146 - acc: 0.925 - ETA: 2s - loss: 0.2033 - acc: 0.921 - ETA: 2s - loss: 0.2222 - acc: 0.915 - ETA: 2s - loss: 0.2215 - acc: 0.906 - ETA: 2s - loss: 0.2323 - acc: 0.902 - ETA: 2s - loss: 0.2317 - acc: 0.903 - ETA: 2s - loss: 0.2181 - acc: 0.909 - ETA: 2s - loss: 0.2158 - acc: 0.908 - ETA: 2s - loss: 0.2177 - acc: 0.908 - ETA: 2s - loss: 0.2156 - acc: 0.910 - ETA: 2s - loss: 0.2138 - acc: 0.914 - ETA: 2s - loss: 0.2076 - acc: 0.916 - ETA: 2s - loss: 0.2018 - acc: 0.917 - ETA: 1s - loss: 0.2029 - acc: 0.918 - ETA: 1s - loss: 0.2075 - acc: 0.919 - ETA: 1s - loss: 0.2105 - acc: 0.918 - ETA: 1s - loss: 0.2077 - acc: 0.921 - ETA: 1s - loss: 0.2081 - acc: 0.919 - ETA: 1s - loss: 0.2075 - acc: 0.919 - ETA: 1s - loss: 0.2090 - acc: 0.919 - ETA: 1s - loss: 0.2033 - acc: 0.922 - ETA: 1s - loss: 0.2014 - acc: 0.923 - ETA: 1s - loss: 0.2001 - acc: 0.924 - ETA: 1s - loss: 0.2007 - acc: 0.924 - ETA: 0s - loss: 0.1982 - acc: 0.925 - ETA: 0s - loss: 0.1991 - acc: 0.926 - ETA: 0s - loss: 0.1982 - acc: 0.926 - ETA: 0s - loss: 0.1955 - acc: 0.927 - ETA: 0s - loss: 0.1939 - acc: 0.928 - ETA: 0s - loss: 0.1970 - acc: 0.926 - ETA: 0s - loss: 0.1952 - acc: 0.926 - ETA: 0s - loss: 0.1950 - acc: 0.925 - ETA: 0s - loss: 0.1960 - acc: 0.924 - ETA: 0s - loss: 0.1998 - acc: 0.922 - 4s 4ms/sample - loss: 0.2000 - acc: 0.9228 - val_loss: 0.2019 - val_acc: 0.9129\n",
      "Epoch 3/10\n",
      "1230/1230 [==============================] - ETA: 3s - loss: 0.3509 - acc: 0.843 - ETA: 3s - loss: 0.2749 - acc: 0.890 - ETA: 3s - loss: 0.2269 - acc: 0.906 - ETA: 3s - loss: 0.2444 - acc: 0.898 - ETA: 3s - loss: 0.2144 - acc: 0.918 - ETA: 3s - loss: 0.1883 - acc: 0.932 - ETA: 3s - loss: 0.1849 - acc: 0.933 - ETA: 3s - loss: 0.1805 - acc: 0.929 - ETA: 3s - loss: 0.1787 - acc: 0.930 - ETA: 3s - loss: 0.1688 - acc: 0.937 - ETA: 2s - loss: 0.1655 - acc: 0.937 - ETA: 2s - loss: 0.1718 - acc: 0.932 - ETA: 2s - loss: 0.1642 - acc: 0.935 - ETA: 2s - loss: 0.1836 - acc: 0.926 - ETA: 2s - loss: 0.1806 - acc: 0.927 - ETA: 2s - loss: 0.1798 - acc: 0.927 - ETA: 2s - loss: 0.1814 - acc: 0.926 - ETA: 2s - loss: 0.1749 - acc: 0.928 - ETA: 2s - loss: 0.1800 - acc: 0.924 - ETA: 1s - loss: 0.1793 - acc: 0.925 - ETA: 1s - loss: 0.1763 - acc: 0.925 - ETA: 1s - loss: 0.1702 - acc: 0.929 - ETA: 1s - loss: 0.1751 - acc: 0.925 - ETA: 1s - loss: 0.1767 - acc: 0.927 - ETA: 1s - loss: 0.1769 - acc: 0.928 - ETA: 1s - loss: 0.1792 - acc: 0.926 - ETA: 1s - loss: 0.1775 - acc: 0.928 - ETA: 1s - loss: 0.1770 - acc: 0.929 - ETA: 0s - loss: 0.1775 - acc: 0.930 - ETA: 0s - loss: 0.1785 - acc: 0.928 - ETA: 0s - loss: 0.1752 - acc: 0.929 - ETA: 0s - loss: 0.1717 - acc: 0.931 - ETA: 0s - loss: 0.1724 - acc: 0.931 - ETA: 0s - loss: 0.1718 - acc: 0.932 - ETA: 0s - loss: 0.1734 - acc: 0.931 - ETA: 0s - loss: 0.1751 - acc: 0.929 - ETA: 0s - loss: 0.1763 - acc: 0.928 - ETA: 0s - loss: 0.1803 - acc: 0.924 - 5s 4ms/sample - loss: 0.1828 - acc: 0.9228 - val_loss: 0.2029 - val_acc: 0.9205\n",
      "Epoch 4/10\n",
      "1230/1230 [==============================] - ETA: 3s - loss: 0.2605 - acc: 0.937 - ETA: 3s - loss: 0.2527 - acc: 0.890 - ETA: 3s - loss: 0.2231 - acc: 0.906 - ETA: 3s - loss: 0.3080 - acc: 0.882 - ETA: 3s - loss: 0.2921 - acc: 0.881 - ETA: 3s - loss: 0.2919 - acc: 0.885 - ETA: 3s - loss: 0.2687 - acc: 0.892 - ETA: 3s - loss: 0.2515 - acc: 0.902 - ETA: 2s - loss: 0.2481 - acc: 0.902 - ETA: 2s - loss: 0.2462 - acc: 0.906 - ETA: 2s - loss: 0.2417 - acc: 0.909 - ETA: 2s - loss: 0.2302 - acc: 0.914 - ETA: 2s - loss: 0.2195 - acc: 0.920 - ETA: 2s - loss: 0.2293 - acc: 0.915 - ETA: 2s - loss: 0.2323 - acc: 0.910 - ETA: 2s - loss: 0.2340 - acc: 0.908 - ETA: 2s - loss: 0.2337 - acc: 0.908 - ETA: 2s - loss: 0.2282 - acc: 0.908 - ETA: 1s - loss: 0.2279 - acc: 0.907 - ETA: 1s - loss: 0.2249 - acc: 0.909 - ETA: 1s - loss: 0.2221 - acc: 0.912 - ETA: 1s - loss: 0.2191 - acc: 0.911 - ETA: 1s - loss: 0.2176 - acc: 0.914 - ETA: 1s - loss: 0.2158 - acc: 0.915 - ETA: 1s - loss: 0.2150 - acc: 0.916 - ETA: 1s - loss: 0.2120 - acc: 0.917 - ETA: 1s - loss: 0.2131 - acc: 0.915 - ETA: 1s - loss: 0.2116 - acc: 0.916 - ETA: 0s - loss: 0.2061 - acc: 0.919 - ETA: 0s - loss: 0.2027 - acc: 0.919 - ETA: 0s - loss: 0.2028 - acc: 0.919 - ETA: 0s - loss: 0.1998 - acc: 0.920 - ETA: 0s - loss: 0.1985 - acc: 0.921 - ETA: 0s - loss: 0.1999 - acc: 0.920 - ETA: 0s - loss: 0.1962 - acc: 0.922 - ETA: 0s - loss: 0.1927 - acc: 0.923 - ETA: 0s - loss: 0.1922 - acc: 0.924 - ETA: 0s - loss: 0.1927 - acc: 0.924 - 5s 4ms/sample - loss: 0.1913 - acc: 0.9252 - val_loss: 0.2061 - val_acc: 0.9110\n",
      "Epoch 5/10\n",
      "1230/1230 [==============================] - ETA: 3s - loss: 0.2191 - acc: 0.906 - ETA: 3s - loss: 0.2257 - acc: 0.890 - ETA: 3s - loss: 0.2147 - acc: 0.895 - ETA: 3s - loss: 0.1943 - acc: 0.906 - ETA: 3s - loss: 0.1792 - acc: 0.912 - ETA: 3s - loss: 0.2001 - acc: 0.906 - ETA: 3s - loss: 0.1938 - acc: 0.910 - ETA: 3s - loss: 0.2021 - acc: 0.910 - ETA: 3s - loss: 0.1857 - acc: 0.920 - ETA: 3s - loss: 0.1811 - acc: 0.925 - ETA: 2s - loss: 0.1801 - acc: 0.926 - ETA: 2s - loss: 0.1717 - acc: 0.932 - ETA: 2s - loss: 0.1604 - acc: 0.937 - ETA: 2s - loss: 0.1522 - acc: 0.942 - ETA: 2s - loss: 0.1600 - acc: 0.937 - ETA: 2s - loss: 0.1650 - acc: 0.935 - ETA: 2s - loss: 0.1611 - acc: 0.937 - ETA: 2s - loss: 0.1600 - acc: 0.939 - ETA: 2s - loss: 0.1575 - acc: 0.940 - ETA: 1s - loss: 0.1574 - acc: 0.942 - ETA: 1s - loss: 0.1573 - acc: 0.943 - ETA: 1s - loss: 0.1592 - acc: 0.943 - ETA: 1s - loss: 0.1621 - acc: 0.940 - ETA: 1s - loss: 0.1597 - acc: 0.941 - ETA: 1s - loss: 0.1640 - acc: 0.940 - ETA: 1s - loss: 0.1605 - acc: 0.941 - ETA: 1s - loss: 0.1590 - acc: 0.942 - ETA: 1s - loss: 0.1590 - acc: 0.940 - ETA: 0s - loss: 0.1576 - acc: 0.941 - ETA: 0s - loss: 0.1550 - acc: 0.943 - ETA: 0s - loss: 0.1519 - acc: 0.944 - ETA: 0s - loss: 0.1481 - acc: 0.946 - ETA: 0s - loss: 0.1493 - acc: 0.946 - ETA: 0s - loss: 0.1551 - acc: 0.944 - ETA: 0s - loss: 0.1669 - acc: 0.942 - ETA: 0s - loss: 0.1657 - acc: 0.942 - ETA: 0s - loss: 0.1651 - acc: 0.942 - ETA: 0s - loss: 0.1613 - acc: 0.944 - 5s 4ms/sample - loss: 0.1622 - acc: 0.9439 - val_loss: 0.1937 - val_acc: 0.9261\n",
      "Epoch 6/10\n",
      "1230/1230 [==============================] - ETA: 3s - loss: 0.1523 - acc: 0.906 - ETA: 3s - loss: 0.1628 - acc: 0.906 - ETA: 3s - loss: 0.1687 - acc: 0.916 - ETA: 3s - loss: 0.1595 - acc: 0.921 - ETA: 3s - loss: 0.1602 - acc: 0.925 - ETA: 3s - loss: 0.1666 - acc: 0.921 - ETA: 3s - loss: 0.1539 - acc: 0.928 - ETA: 3s - loss: 0.1459 - acc: 0.937 - ETA: 3s - loss: 0.1382 - acc: 0.941 - ETA: 2s - loss: 0.1422 - acc: 0.940 - ETA: 2s - loss: 0.1470 - acc: 0.937 - ETA: 2s - loss: 0.1444 - acc: 0.940 - ETA: 2s - loss: 0.1431 - acc: 0.942 - ETA: 2s - loss: 0.1471 - acc: 0.939 - ETA: 2s - loss: 0.1436 - acc: 0.941 - ETA: 2s - loss: 0.1415 - acc: 0.943 - ETA: 2s - loss: 0.1466 - acc: 0.935 - ETA: 2s - loss: 0.1454 - acc: 0.935 - ETA: 2s - loss: 0.1416 - acc: 0.939 - ETA: 1s - loss: 0.1393 - acc: 0.940 - ETA: 1s - loss: 0.1394 - acc: 0.942 - ETA: 1s - loss: 0.1396 - acc: 0.943 - ETA: 1s - loss: 0.1368 - acc: 0.945 - ETA: 1s - loss: 0.1389 - acc: 0.945 - ETA: 1s - loss: 0.1346 - acc: 0.947 - ETA: 1s - loss: 0.1382 - acc: 0.945 - ETA: 1s - loss: 0.1364 - acc: 0.945 - ETA: 1s - loss: 0.1352 - acc: 0.946 - ETA: 0s - loss: 0.1398 - acc: 0.945 - ETA: 0s - loss: 0.1373 - acc: 0.946 - ETA: 0s - loss: 0.1438 - acc: 0.943 - ETA: 0s - loss: 0.1462 - acc: 0.941 - ETA: 0s - loss: 0.1432 - acc: 0.943 - ETA: 0s - loss: 0.1447 - acc: 0.941 - ETA: 0s - loss: 0.1464 - acc: 0.942 - ETA: 0s - loss: 0.1470 - acc: 0.941 - ETA: 0s - loss: 0.1462 - acc: 0.941 - ETA: 0s - loss: 0.1476 - acc: 0.940 - 5s 4ms/sample - loss: 0.1465 - acc: 0.9407 - val_loss: 0.1919 - val_acc: 0.9280\n",
      "Epoch 7/10\n",
      "1230/1230 [==============================] - ETA: 3s - loss: 0.0533 - acc: 1.000 - ETA: 3s - loss: 0.0640 - acc: 1.000 - ETA: 3s - loss: 0.0654 - acc: 0.989 - ETA: 3s - loss: 0.0709 - acc: 0.984 - ETA: 3s - loss: 0.0762 - acc: 0.981 - ETA: 3s - loss: 0.1041 - acc: 0.968 - ETA: 3s - loss: 0.1033 - acc: 0.964 - ETA: 3s - loss: 0.0966 - acc: 0.968 - ETA: 2s - loss: 0.1040 - acc: 0.958 - ETA: 2s - loss: 0.1062 - acc: 0.956 - ETA: 2s - loss: 0.1095 - acc: 0.957 - ETA: 2s - loss: 0.1163 - acc: 0.953 - ETA: 2s - loss: 0.1141 - acc: 0.956 - ETA: 2s - loss: 0.1274 - acc: 0.953 - ETA: 2s - loss: 0.1312 - acc: 0.950 - ETA: 2s - loss: 0.1316 - acc: 0.949 - ETA: 2s - loss: 0.1308 - acc: 0.948 - ETA: 2s - loss: 0.1329 - acc: 0.944 - ETA: 1s - loss: 0.1339 - acc: 0.944 - ETA: 1s - loss: 0.1357 - acc: 0.942 - ETA: 1s - loss: 0.1346 - acc: 0.943 - ETA: 1s - loss: 0.1299 - acc: 0.946 - ETA: 1s - loss: 0.1270 - acc: 0.947 - ETA: 1s - loss: 0.1264 - acc: 0.946 - ETA: 1s - loss: 0.1240 - acc: 0.948 - ETA: 1s - loss: 0.1303 - acc: 0.945 - ETA: 1s - loss: 0.1305 - acc: 0.945 - ETA: 1s - loss: 0.1313 - acc: 0.946 - ETA: 0s - loss: 0.1354 - acc: 0.945 - ETA: 0s - loss: 0.1373 - acc: 0.944 - ETA: 0s - loss: 0.1350 - acc: 0.946 - ETA: 0s - loss: 0.1319 - acc: 0.948 - ETA: 0s - loss: 0.1314 - acc: 0.947 - ETA: 0s - loss: 0.1302 - acc: 0.948 - ETA: 0s - loss: 0.1280 - acc: 0.949 - ETA: 0s - loss: 0.1303 - acc: 0.947 - ETA: 0s - loss: 0.1288 - acc: 0.948 - ETA: 0s - loss: 0.1292 - acc: 0.947 - 4s 4ms/sample - loss: 0.1300 - acc: 0.9463 - val_loss: 0.1827 - val_acc: 0.9261\n",
      "Epoch 8/10\n",
      "1230/1230 [==============================] - ETA: 3s - loss: 0.1318 - acc: 0.968 - ETA: 3s - loss: 0.0891 - acc: 0.984 - ETA: 3s - loss: 0.0788 - acc: 0.979 - ETA: 3s - loss: 0.0902 - acc: 0.968 - ETA: 3s - loss: 0.1108 - acc: 0.968 - ETA: 3s - loss: 0.1095 - acc: 0.968 - ETA: 3s - loss: 0.1156 - acc: 0.964 - ETA: 2s - loss: 0.1050 - acc: 0.968 - ETA: 2s - loss: 0.0972 - acc: 0.972 - ETA: 2s - loss: 0.0998 - acc: 0.971 - ETA: 2s - loss: 0.1087 - acc: 0.965 - ETA: 2s - loss: 0.1061 - acc: 0.966 - ETA: 2s - loss: 0.1105 - acc: 0.966 - ETA: 2s - loss: 0.1090 - acc: 0.966 - ETA: 2s - loss: 0.1106 - acc: 0.964 - ETA: 2s - loss: 0.1097 - acc: 0.964 - ETA: 2s - loss: 0.1160 - acc: 0.963 - ETA: 2s - loss: 0.1155 - acc: 0.961 - ETA: 1s - loss: 0.1144 - acc: 0.963 - ETA: 1s - loss: 0.1155 - acc: 0.962 - ETA: 1s - loss: 0.1205 - acc: 0.959 - ETA: 1s - loss: 0.1221 - acc: 0.958 - ETA: 1s - loss: 0.1256 - acc: 0.957 - ETA: 1s - loss: 0.1279 - acc: 0.955 - ETA: 1s - loss: 0.1282 - acc: 0.956 - ETA: 1s - loss: 0.1274 - acc: 0.956 - ETA: 1s - loss: 0.1288 - acc: 0.954 - ETA: 1s - loss: 0.1307 - acc: 0.953 - ETA: 0s - loss: 0.1332 - acc: 0.952 - ETA: 0s - loss: 0.1336 - acc: 0.953 - ETA: 0s - loss: 0.1318 - acc: 0.954 - ETA: 0s - loss: 0.1328 - acc: 0.954 - ETA: 0s - loss: 0.1316 - acc: 0.954 - ETA: 0s - loss: 0.1305 - acc: 0.955 - ETA: 0s - loss: 0.1296 - acc: 0.955 - ETA: 0s - loss: 0.1308 - acc: 0.954 - ETA: 0s - loss: 0.1316 - acc: 0.954 - ETA: 0s - loss: 0.1310 - acc: 0.954 - 4s 4ms/sample - loss: 0.1300 - acc: 0.9553 - val_loss: 0.1800 - val_acc: 0.9261\n",
      "Epoch 9/10\n",
      "1230/1230 [==============================] - ETA: 3s - loss: 0.0922 - acc: 0.968 - ETA: 3s - loss: 0.1015 - acc: 0.968 - ETA: 3s - loss: 0.1031 - acc: 0.979 - ETA: 3s - loss: 0.1289 - acc: 0.968 - ETA: 3s - loss: 0.1658 - acc: 0.956 - ETA: 3s - loss: 0.1534 - acc: 0.958 - ETA: 3s - loss: 0.1469 - acc: 0.959 - ETA: 3s - loss: 0.1584 - acc: 0.953 - ETA: 2s - loss: 0.1486 - acc: 0.958 - ETA: 2s - loss: 0.1458 - acc: 0.959 - ETA: 2s - loss: 0.1409 - acc: 0.960 - ETA: 2s - loss: 0.1356 - acc: 0.960 - ETA: 2s - loss: 0.1348 - acc: 0.961 - ETA: 2s - loss: 0.1321 - acc: 0.959 - ETA: 2s - loss: 0.1331 - acc: 0.958 - ETA: 2s - loss: 0.1257 - acc: 0.960 - ETA: 2s - loss: 0.1228 - acc: 0.961 - ETA: 2s - loss: 0.1206 - acc: 0.961 - ETA: 1s - loss: 0.1280 - acc: 0.957 - ETA: 1s - loss: 0.1241 - acc: 0.959 - ETA: 1s - loss: 0.1221 - acc: 0.958 - ETA: 1s - loss: 0.1187 - acc: 0.960 - ETA: 1s - loss: 0.1219 - acc: 0.959 - ETA: 1s - loss: 0.1220 - acc: 0.958 - ETA: 1s - loss: 0.1219 - acc: 0.957 - ETA: 1s - loss: 0.1209 - acc: 0.957 - ETA: 1s - loss: 0.1191 - acc: 0.959 - ETA: 1s - loss: 0.1204 - acc: 0.957 - ETA: 0s - loss: 0.1180 - acc: 0.959 - ETA: 0s - loss: 0.1230 - acc: 0.956 - ETA: 0s - loss: 0.1222 - acc: 0.955 - ETA: 0s - loss: 0.1215 - acc: 0.956 - ETA: 0s - loss: 0.1186 - acc: 0.957 - ETA: 0s - loss: 0.1201 - acc: 0.957 - ETA: 0s - loss: 0.1204 - acc: 0.957 - ETA: 0s - loss: 0.1183 - acc: 0.958 - ETA: 0s - loss: 0.1184 - acc: 0.957 - ETA: 0s - loss: 0.1196 - acc: 0.956 - 4s 4ms/sample - loss: 0.1201 - acc: 0.9561 - val_loss: 0.1817 - val_acc: 0.9242\n",
      "Epoch 10/10\n",
      "1230/1230 [==============================] - ETA: 3s - loss: 0.0978 - acc: 0.968 - ETA: 3s - loss: 0.0635 - acc: 0.984 - ETA: 3s - loss: 0.0817 - acc: 0.968 - ETA: 3s - loss: 0.1127 - acc: 0.953 - ETA: 3s - loss: 0.1014 - acc: 0.956 - ETA: 3s - loss: 0.1216 - acc: 0.947 - ETA: 3s - loss: 0.1259 - acc: 0.950 - ETA: 3s - loss: 0.1334 - acc: 0.949 - ETA: 2s - loss: 0.1318 - acc: 0.947 - ETA: 2s - loss: 0.1292 - acc: 0.950 - ETA: 2s - loss: 0.1305 - acc: 0.946 - ETA: 2s - loss: 0.1263 - acc: 0.947 - ETA: 2s - loss: 0.1345 - acc: 0.944 - ETA: 2s - loss: 0.1465 - acc: 0.939 - ETA: 2s - loss: 0.1483 - acc: 0.937 - ETA: 2s - loss: 0.1498 - acc: 0.935 - ETA: 2s - loss: 0.1502 - acc: 0.935 - ETA: 2s - loss: 0.1494 - acc: 0.935 - ETA: 1s - loss: 0.1482 - acc: 0.937 - ETA: 1s - loss: 0.1440 - acc: 0.940 - ETA: 1s - loss: 0.1404 - acc: 0.943 - ETA: 1s - loss: 0.1356 - acc: 0.946 - ETA: 1s - loss: 0.1328 - acc: 0.947 - ETA: 1s - loss: 0.1315 - acc: 0.946 - ETA: 1s - loss: 0.1273 - acc: 0.948 - ETA: 1s - loss: 0.1249 - acc: 0.950 - ETA: 1s - loss: 0.1220 - acc: 0.952 - ETA: 1s - loss: 0.1233 - acc: 0.952 - ETA: 0s - loss: 0.1210 - acc: 0.953 - ETA: 0s - loss: 0.1181 - acc: 0.955 - ETA: 0s - loss: 0.1165 - acc: 0.955 - ETA: 0s - loss: 0.1167 - acc: 0.955 - ETA: 0s - loss: 0.1149 - acc: 0.956 - ETA: 0s - loss: 0.1134 - acc: 0.957 - ETA: 0s - loss: 0.1116 - acc: 0.958 - ETA: 0s - loss: 0.1163 - acc: 0.957 - ETA: 0s - loss: 0.1146 - acc: 0.958 - ETA: 0s - loss: 0.1131 - acc: 0.958 - 4s 4ms/sample - loss: 0.1134 - acc: 0.9585 - val_loss: 0.1765 - val_acc: 0.9280\n"
     ]
    }
   ],
   "source": [
    "pickle_in = open(\"X.pickle\",\"rb\")\n",
    "X = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"y.pickle\",\"rb\")\n",
    "y = pickle.load(pickle_in)\n",
    "\n",
    "X = X/255.0\n",
    "\n",
    "dense_layers = [0]\n",
    "layer_sizes = [64]\n",
    "conv_layers = [3]\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            NAME = \"{}-conv-{}-nodes-{}-dense-{}\".format(conv_layer, layer_size, dense_layer, int(time.time()))\n",
    "            print(NAME)\n",
    "\n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Conv2D(layer_size, (3, 3), input_shape=X.shape[1:]))\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "            for l in range(conv_layer-1):\n",
    "                model.add(Conv2D(layer_size, (3, 3)))\n",
    "                model.add(Activation('relu'))\n",
    "                model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "            model.add(Flatten())\n",
    "\n",
    "            for _ in range(dense_layer):\n",
    "                model.add(Dense(layer_size))\n",
    "                model.add(Activation('relu'))\n",
    "\n",
    "            model.add(Dense(1))\n",
    "            model.add(Activation('sigmoid'))\n",
    "\n",
    "            tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "\n",
    "            model.compile(loss='binary_crossentropy',\n",
    "                          optimizer='adam',\n",
    "                          metrics=['accuracy'],\n",
    "                          )\n",
    "\n",
    "            model.fit(X, y,\n",
    "                      batch_size=32,\n",
    "                      epochs=10,\n",
    "                      validation_split=0.3,\n",
    "                      callbacks=[tensorboard])\n",
    "\n",
    "model.save('64x3-CNN_Pokemon_v1.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Pokemon class for the below is `Pokemon-a`\n",
    "\n",
    "<img src=\"kaggle-one-shot-pokemon/pokemon-tcg-images/65-pl2-103.png\">\n",
    "\n",
    "##### Lets see, if our model predicts it correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]]\n",
      "Pokemon-a\n"
     ]
    }
   ],
   "source": [
    "CATEGORIES = [\"Pokemon-a\", \"Pokemon-b\"]  # will use this to convert prediction num to string value\n",
    "\n",
    "\n",
    "def prepare(filepath):\n",
    "    IMG_SIZE = 50  # 50 in txt-based\n",
    "    img_array = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)  # read in the image, convert to grayscale\n",
    "    new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize image to match model's expected sizing\n",
    "    return new_array.reshape(-1, IMG_SIZE, IMG_SIZE, 1)  # return the image with shaping that TF wants.\n",
    "\n",
    "model_loaded = tf.keras.models.load_model(\"64x3-CNN_Pokemon_v1.model\")\n",
    "\n",
    "prediction0 = model.predict([prepare('kaggle-one-shot-pokemon/pokemon-tcg-images/65-pl2-103.png')])\n",
    "\n",
    "print(prediction0)\n",
    "prediction0[0][0]\n",
    "print(CATEGORIES[int(prediction0[0][0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It turned out to be right. Similarly lets try for another class - Pokemon-b\n",
    "which is as below:\n",
    "\n",
    "<img src=\"kaggle-one-shot-pokemon/pokemon-tcg-images/65-ex14-99.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]]\n",
      "Pokemon-b\n"
     ]
    }
   ],
   "source": [
    "prediction1 = model.predict([prepare('kaggle-one-shot-pokemon/pokemon-tcg-images/65-xy10-117.png')])\n",
    "print(prediction1)\n",
    "print(CATEGORIES[int(prediction1[0][0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Seems, luckily our model performed in the right way in predicting the class. We can check it with other classes as well, where it can fail. The model cannot always give the true prediction, as its not perfect. We can always, improve it by training it more. For now, we have trained it to predict images superficially enough. Lets try to conclude our assumptions and activities till now in this notebook as a final conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We by flowing the Deep Learning method and by implementing CNN(Convolution Neural Network) in it, have come to the below conclusion to summarize.\n",
    "\n",
    "\n",
    "|Sr.No|Model No.|Activation Function|Epoch|Loss        |Optimization|Initializers|No. of Layers|Final Accuracy|\n",
    "|-----|---------|-------------------|-----|------------|------------|------------|-------------|--------------|\n",
    "|  1  |Model1   |ReLU               |5    |CrossEntropy|adam        |      None  |1            |0.9553\n",
    "|2|Model2|tanh|5|CrossEntropy|adam|None|1|0.6\n",
    "|3|Model3|softsign|5|CrossEntropy|adam|None|1|0.918\n",
    "|4|Model4|softsign|10|CrossEntropy|adam|None|1|0.9553\n",
    "|5|Model5|softsign|10|QuadraticCost|adam|None|1|0.9358\n",
    "|6|Model6|softsign|10|QuadraticCost|adadelta|None|1|0.9366\n",
    "|7|Model7|softsign|10|QuadraticCost|adagrad|None|1|0.8984\n",
    "|8|Model8|ReLU|5|QuadraticCost|adagrad|None|2|0.5228\n",
    "|9|Model9|softsign|5|QuadraticCost|adagrad|None|3|0.8862\n",
    "|10|Model10|ReLU|5|QuadraticCost|adagrad|Xavier Uniform|2|0.9008\n",
    "|11|Model11|ReLU|5|QuadraticCost|adagrad|Zero Weight|2|0.5228\n",
    "\n",
    "\n",
    "Thus we can see, from the table above, `Model1` is the best performer on our over all Convolution neurons and Models that has been created. We have understood and trained our model to predict an Pokemon from the Pokemon family.\n",
    "The TensorBoard for all the models with their Accuracy in self and Cross Validation is as shown below.\n",
    "\n",
    "### epoch_acc\n",
    "<img src=\"Images/TensorBoard/epoch_acc_final.svg?sanitize=true\" width=\"500\">\n",
    "\n",
    "### epoch_loss\n",
    "<img src=\"Images/TensorBoard/epoch_loss.svg?sanitize=true\" width=\"500\">\n",
    "\n",
    "### epoch_val_acc\n",
    "<img src=\"Images/TensorBoard/epoch_val_acc.svg?sanitize=true\" width=\"500\">\n",
    "\n",
    "### epoch_val_loss\n",
    "<img src=\"Images/TensorBoard/epoch_val_loss.svg?sanitize=true\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/image6.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contribution\n",
    "\n",
    "In the above analysis:\n",
    "\n",
    "`95% Self` of the work is done by us which comprises below:\n",
    "\n",
    "All the models are generated by full understanding of the Convolutional concepts\n",
    "\n",
    "`5% Reference` though reference materials cited below\n",
    "The models have been visualized in TesonfBoard completely by self.\n",
    "The TesorFLow and TensorBoard are understood very well with Prof. and TAs help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation\n",
    "Below were the links that were refered\n",
    "* Videos:\n",
    "    * https://www.youtube.com/watch?v=A4K6D_gx2Iw\n",
    "    * https://www.youtube.com/watch?time_continue=116&v=BqgTU7_cBnk\n",
    "* Web Sites:\n",
    "    * https://medium.com/intro-to-artificial-intelligence/simple-image-classification-using-deep-learning-deep-learning-series-2-5e5b89e97926\n",
    "    * https://www.mathworks.com/solutions/deep-learning/convolutional-neural-network.html\n",
    "    * https://medium.com/intro-to-artificial-intelligence/deep-learning-series-1-intro-to-deep-learning-abb1780ee20\n",
    "    * https://pythonprogramming.net/using-trained-model-deep-learning-python-tensorflow-keras/?completed=/tensorboard-optimizing-models-deep-learning-python-tensorflow-keras/\n",
    "    \n",
    "The above sources were really helpful. Additional to which Prof.'s git also came in handy\n",
    "\n",
    "* https://github.com/nikbearbrown/\n",
    "* https://github.com/nikbearbrown/CSYE_7245/tree/master/Deep_Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n",
    "\n",
    "MIT License\n",
    "    \n",
    "<img src=\"Images/OSI_Approved_License.png\" width=\"100\" align=\"right\"/>\n",
    "\n",
    "    \n",
    "Copyright (c) 2019 Prabhu Subramanian\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
